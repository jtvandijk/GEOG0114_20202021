# Exploratory spatial data analysis

## Introduction {#introduction_w05}
This week, we focus on the second of the two key properties of spatial data: spatial heterogeneity. With the underlying process (or processes) that govern a spatial variable likely to vary across space, a single global relationship for an entire region of study may not adequately model the process that governs outcomes in any given location of the study region. As a result, multiple methods have been developed to incorporate ‘space’ into traditional regression models, including spatial lag models, spatial error models, and Geographical Weighted Regression. 

This week provides the building blocks to conducting a statistical and spatial investigation into the relationships between spatial variables, looking at the concept of **Exploratory Spatial Data Analysis (ESDA)**. We then look at the three types of spatial regression models in turn to understand their methodology and potential advantages and limitations in accounting for space when modelling relationships. However, before we move on to ESDA, we will look into some aspects of data prepration, data cleaning, and creating a **tidy dataset**.

In the first part of this week's session we will start by creating a **tidy dataset** using some data from the [UK 2011 Census of Population](https://www.ons.gov.uk/census/2011census). In the second part of this week's session we will explore potential factors that may contribute to the crime rate in London. We conduct an ESDA of these variables, followed by statistical and spatial regression analysis to determine whether our variables contribute to crime in our area of study. 

This week is structured by **three** short lecture videos, **two** [assignments](#assignment_w05) that you need to do in preparation for Friday's seminar, and the practical material. As always, this [week's reading list](https://rl.talis.com/3/ucl/lists/3FB0C66A-F603-A402-A31C-28F2DA091AB6.html?lang=en-gb&login=1) is availble on the UCL library reading list page for the course.

### Video: Overview {#overview_w05}
```{r 05-short-lecture-welcome, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()


## Tidy data
Over the past weeks a lot of information has come your way, diving deep into the world of Spatial Data Science. However, whilst you are slowly becoming proficient in using R and Python to solve complex (spatial) problems, it is now a good moment to start thinking about data themselves and how they are organised. This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be **dirty data**. With dirty data we mean data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Exploratory Spatial Data Analysis very much starts with your data preparation. 

In the following, you will learn a consistent way to structure your data in R: **tidy data**. Tidy data, as formalised by [R Wizard Hadley Wickham](http://hadley.nz/) in his contribution to the [Journal of Statistical Software](http://www.jstatsoft.org/v59/i10/paper) is not only very much at the core of the `tidyverse` R package, but also of general importance when organising your data. In the words, of [the Wizard](https://r4ds.had.co.nz/tidy-data.html):

> Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand.

### Video: Tidy data
```{r 05-short-lecture-tidy-data, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()

### What do tidy data look like?

You can represent the same underlying data in multiple ways. The example below, taken from the the `tidyverse` package and described in the [R for Data Science](https://r4ds.had.co.nz/tidy-data.html) book, shows that the same data can organised in four different ways. 

```{r 05-loading-tidyverse, warnings=FALSE, message=FALSE}
# load the tidyverse 
library(tidyverse)
```

Table **1**:
```{r 05-tidyverse-table1, warnings=FALSE, message=FALSE}
table1
```

Table **2**:
```{r 05-tidyverse-table2, warnings=FALSE, message=FALSE}
table2
```

Table **3**:
```{r 05-tidyverse-table3, warnings=FALSE, message=FALSE}
table3
```

Table **4a**:
```{r 05-tidyverse-table4a, warnings=FALSE, message=FALSE}
table4a
```

Table **4b**:
```{r 05-tidyverse-table4b, warnings=FALSE, message=FALSE}
table4b
```

None of these representations are wrong per se, however, not are equally easy to use. Only Table **1** can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy:

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

```{r 05-figure-hadley-wickham, echo=FALSE, fig.align='center', fig.cap='A visual representation of tidy data by [Hadley Wickham](https://r4ds.had.co.nz/tidy-data.html).'}
knitr::include_graphics('images/week05/05_a_tidy_data.png')
```

Fortunately, there are some functions in the `tidyr` and `dplyr` packages, both part of the `tidyverse` that will help us cleaning and preparing our datasets to create a **tidy dataset**. The most important and useful functions are:

| Package   | Function          | Use to |
| :------   | :------           | :------ |
| dplyr	    | select            | select columns |
| dplyr	    | filter            | select rows |
| dplyr	    | mutate            | transform or recode variables |
| dplyr	    | summarise         | summarise data |
| dplyr	    | group by          | group data into subgropus for further processing |
| tidyr	    | pivot_longer      | convert data from wide format to long format |
| tidyr	    | pivot_wider       | convert long format dataset to wide format |

Keep in mind that if you encounter a function in a piece of R code that you have not seen before and you are wondering what it does that you can get access the documentation through `?name_of_function`, e.g. `?pivot_longer`. For any mature R package, the documentation contains a list of arguments that the function takes as well as a set of usage examples.

### Example: Creating tidy data
Now we know what consitute **tidy data**, we can put this into practice with an example using some data from the [Office for National Statistics](https://www.ons.gov.uk/). Let's say we are asked by our boss to analyse some data on the ethnic background of the UK population, for instance, because we want to get some insights into the relationship between [COVID-19 and ethnic background](https://www.theguardian.com/world/2020/oct/09/bame-groups-hit-hard-again-covid-second-wave-grips-uk-nations). Our assignment is to calculate the relative proportions of each ethnic group within the administrative geography of the [Middle layer Super Output Area (MSOA)](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/middlesuperoutputareamidyearpopulationestimates). In order to do this, we have been given a file that contains data on ethnicity by age group at the MSOA-level of every person in the 2011 UK Census who is 16 year or older. Download the file to your own computer and [set up your data directory](spatial-autocorrelation.html#setting-up-the-environment) in the same fashion as you did last week. You can also decide on setting up your own folder structure, no problem, just make sure that you update the file paths in the practical to match the file paths on your own computers. Also, make sure that you unzip the data.

#### File download {-}
| File                                                 | Type           | Link |
| :------                                              | :------        | :------ |
| Etnicity by age group 2011 Census of Population      | `zip`          | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/population/eth_pop_2011/msoa_eth2011_ew_16plus.zip) |

We start by making sure our `tidyverse` is loaded into R and using the `read_csv()` function to read our `.csv` file.

```{r 05-reading-data, warnings=FALSE, message=FALSE}
# load the tidyverse 
library(tidyverse)

# read data into dataframe
df <- read_csv('raw/population/eth_pop_2011/msoa_eth2011_ew_16plus.csv')
  
# inspect the dataframe: number of columns
ncol(df)

# inspect the dataframe: number of rows
nrow(df)

# inspect the dataframe: sneak peak
print(df, n_extra=2)
```

It is clear that the data are not directly suitable to establish the proportion of each ethnic group within the population of each MSOA because the data are split out over multiple columns. Let's inspect the names of the columns to get a better idea of the structure of our data set.

```{r 05-setting-max-print, warnings=FALSE, message=FALSE, echo=FALSE}
# settings
options(max.print = 30)
```

```{r 05-inspecting-column-names, warnings=FALSE, message=FALSE}
# inspect the dataframe: column names
names(df)
```

The column names are all awfully long and it looks like that the data have been split out into age groups. Further to this, the data contain within group total counts: *all categories*, *white total*, *mixed/multiple ethnic group total*, and so on. You can also try using `View(df)` or use any other form of spreadsheet software (e.g. Microsoft Excel) to **browse** through the dataset to get a better idea of what is happening and get a better idea of the structure of the data.

Although the data is messy and we will need to reorganise our data set, it does look there is some form of structure present that we can exploit: the various columns with population counts for each ethnic group are repeated for each of the different age groups. This means that we can go through the data frame in steps of equal size to select the data we want: starting from column 2 (column 1 only contains the reference to the adminsitrative geography) we want to select all 24 columns of data for that particular age group. We can create a `for` loop that does exactly that:

```{r 05-creating-loop, warnings=FALSE, message=FALSE}
# loop through the columns of our data set
for (column in seq(2,ncol(df),24)) {
    
  # index number of start column of age group
  start <- column
  
  # index number of end column of age group
  stop <- column + 23 
  
  # print results
  print(c(start,stop))
}
```
For each age group in our data, the printed values should **(!)** correspond with the index number of the start column of the age group and the index number of the end column of the age group, respectively. Let's do a sanity check.

```{r 05-sanity-check-columns, warnings=FALSE, message=FALSE}
# sanity check: age group 16-17 (start column)
df[,2]

# sanity check: age group 16-17 (end column)
df[,25]

# sanity check: age group 18-19 (start column)
df[,26]

# sanity check: age group 18-19 (end column)
df[,49]
```

All seems to be correct and we have successfully identified our columns. This is great, however, we still cannot work with our data as everything is spread out over different columns. Let's fix this by manipulating the shape of our data by turning columns into rows.

```{r 05-columns-to-rows-function, warnings=FALSE, message=FALSE}
# create function
columns_to_rows <- function(df, start, stop) {
  
  # columns we are interested in
  col_sub <- c(1,start:stop)
  
  # subset the dataframe 
  df_sub <- select(df,col_sub)
  
  # pivot the columns in the dataframe, exclude the MSOA code column 
  df_piv <- pivot_longer(df_sub,-msoa11cd)
  
  # rename columns
  names(df_piv) <- c('msoa11cd','age_group','count')
  return(df_piv)
}

# test
columns_to_rows(df,2,25)
```

This looks much better. Now let's combine our loop with our newly created function to apply this to all of our data.

```{r 05-reformat-dataframe, warnings=FALSE, message=FALSE}
# create an empty list to store our result from the loop
df_lst <- list()

# loop through the columns of our data set
for (column in seq(2,ncol(df),24)) {
    
  # index number of start column of age group
  start <- column
  
  # index number of end column of age group
  stop <- column + 23 
  
  # call our function and assign it to the list
  df_lst[[length(df_lst)+1]] <- columns_to_rows(df,start=start,stop=stop)
}

# paste all elements from the list underneath one another
# do.call executes the function rbind for all elements in our list
df_reformatted <- as_tibble(do.call(rbind,df_lst))

# and the result
df_reformatted
```

Now the data is in a much more managable format, we can move on with preparing the data further. We will start by filtering out the columns (now rows!) that contain *all categories* and the *within group totals*. We will do this by cleverly filtering our data set on only a part of the text string that is contained in the *age_group* column of our dataframe using a [regular expression](https://en.wikipedia.org/wiki/Regular_expression). We further truncate the information that is contained in the *age_group* column to make all a little more readable.

```{r 05-further-clean-dataframe, warnings=FALSE, message=FALSE}
# filter rows
df_reformatted <- filter(df_reformatted,!grepl('*All categories*',age_group))
df_reformatted <- filter(df_reformatted,!grepl('*Total*',age_group))

# create variable that flags the 85 and over category
df_reformatted$g <- ifelse(grepl('85',as.character(df_reformatted$age_group)),1,0)

# select information from character 41 (85 and over category) or from character 38
df_reformatted <- mutate(df_reformatted,group = ifelse(g==0,substr(as.character(age_group),38,500),
                                                            substr(as.character(age_group),41,500))) 

# remove unnecessary columns
df_reformatted <- select(df_reformatted, -age_group, -g)
```

We are now really getting somewhere, although in order for our data to be tidy each variable must have its own column. We also want, within each ethnic group, to have the sum of the values that used to belong to individual age groups.

```{r 05-pivot-table-aggregate, warnings=FALSE, message=FALSE}
# pivot table and aggregate values
df_clean <- pivot_wider(df_reformatted,names_from=group,values_from=count,values_fn=sum)

# rename columns
names(df_clean) <- c('msoa11cd','white_british','white_irish','white_traveller','white_other','mixed_white_black_caribbean',
                     'mixed_white_black_african','mixed_white_asian','mixed_other','asian_indian','asian_pakistani',
                     'asian_bangladeshi','asian_chinese','asian_other','black_african','black_caribbean','black_other',
                     'other_arab','other_other')

# tidy date
df_clean
```

Finally. We now have a **tidy dataset** that we can work with!

### Assignments {#assignment_w05}

#### Assignment 1 {-}
Since we went through all the trouble of cleaning and creating this file, the first task for Friday's seminar is to finalise the analysis: use the cleaned data set to create a table that, for each MSOA, contains the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population. It could look something like this:

| msoa11cd          | white_british          | white_irish       | etc.        |
| :------           | :------:               | :------:          | :------:    |
| E02002562         | 0.74                   | 0.03              | ...         |
| E02002560         | 0.32                   | 0.32              | ...         |

#### Tips {-} 
1. First think what you what steps you would need to take in order to get the group proportions. Write them down on a piece of paper if you like. Once you have identified the steps, then start coding.
2. Conduct sanity checks. Every time you have written a piece of code, check the results to see if the code did indeed give the result that you expected to get.
3. Google is your friend. Do not be afraid to search for specific solutions and suggestions, chances are that there have been other people who have faces similar problems and posted their questions on [stackoverflow](https://stackoverflow.com/).

#### Assignment 2 {-}
Further to calculating the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population, we also want to make a choropleth map at district level of the UK population that is older than 60 as a proportion of the total population. For this analysis we have available one dataset with the administrative boundaries of the UK 2020 Local Authorithy Districts administrative geography and we have a `.csv` file that holds population estimates for the UK in 2019. Use everything you have learned over the past weeks to produces this map. Some tips:

#### Tips {-}
1. Inspect both the shapefile and the `.csv` file to get an idea of how your data look like. Use any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.).
2. The `.csv` file does contain a mix of administrative geographies, and you will need to do some data cleaning by filtering out *Country*, *County*, *Metropolitan County*, and *Region* before you link the data to the shapefile.
3. You are in charge of deciding which tools you want to use to visualise the data (ArcGIS, R, QGIS, etc.).
4. You now have to make your own decisions on how to go about this problem. Although this practical has so far covered some of the fucntions and strategies you might need, the data cleaning and data preparation process is not the same.

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Local Authorithy District boundaries 2020   | `zip`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/administrative_boundaries/uk_lad_2020/LADs_uk_2020.zip) |
| Mid-Year Population Estimates 2019          | `zip`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/population/mye_pop_2019/mye_pop_2019.zip) |

## Exploratory spatial data analysis in R
Exploratory Data Analysis got introduced in the the late 1970s by the American mathematician [John Tukey](https://en.wikipedia.org/wiki/John_Tukey). Tukey thought that in much data analysis there was much emphasis on statistical hypothesis testing and very little development of new hypotheses. He therefore made a distinction between **confirmatory data analysis** and **exploratory data analysis (EDA)**. EDA is a collection of descriptive techniques used to detect patterns, identify outliers, and form hypotheses from the data. An EDA typically involves descriptive statistics and data visualisation. Exploratory techniques generally stay 'close' to the original data. **Exploratory spatial data analysis (ESDA)** is the extension of exploratory data analysis (EDA) to the spatial realm.

### Video: Exploratory spatial data analysis
```{r echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Exploring our dataset
After successfully creating the maps that we were requested to make, our boss got so happy that she directly gave us a new assignment. This time she wants us to look at crime data in London again, specifically she wants to know whether relative deprivation and factors from the built environment can explain the occurence of anti-social behaviour in London in March 2019. We have access to two **open** data sets. The first one contains the English [Index of Multiple Deprivation (IMD)](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019) for 2019. The English indices of deprivation measure relative deprivation in small areas in England called Lower-layer Super Output Areas (LSOAs) The index of multiple deprivation is the most widely used of these indices. The second one contains some of the input data of the [Access to Healthy Assets and Hazards (AHAH)](https://data.cdrc.ac.uk/dataset/access-healthy-assets-hazards-ahah). AHAH is a multi-dimensional index developed by the [Consumer Data Research Centre (CDRC)](https://www.cdrc.ac.uk/) for Great Britain measuring how ‘healthy’ neighbourhoods are. We also have access to the LSOAs for London.

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Lower-layer Super Output Areas London 2011  | `zip`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/administrative_boundaries/london_lsoa_2011/lsoa_london_2011.zip) |
| Anti-social behaviour crime data London 2019| `zip`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/crime/march2019_london_anti_social.zip) |
| Index of Multiple Deprivation London 2019   | `zip`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/index/imd_ew_2019.zip) |
| Access to Healthy Assets and Hazards (AHAH) London | `zip` | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/index/ahah_input_gb_2016.zip) |

Download and the file to your own computer and again make sure your [your data directory](spatial-autocorrelation.html#setting-up-the-environment) is set up correctly and the data are unzipped. Then load the libraries and the data into R.

```{r 05-load-libraries, warnings=FALSE, message=FALSE}
# load libraries
library(tidyverse)
library(sf)
library(tmap)

# load data
lsoa <- st_read('raw/administrative_boundaries/london_lsoa_2011/lsoa_london_2011.shp')
crime <- read_csv('raw/crime/march2019_london_anti_social.csv')
imd <- read_csv('raw/index/imd_ew_2019.csv')
ahah <- read_csv('raw/index/ahah_input_gb_2016.csv')

```

## Spatial heterogeneity
### Video: Spatial heterogeneity
```{r echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Accounting for spatial heterogeneity

## Attributions {#attributions_w05}
This week's content and practical uses content and inspiration from:

- Wickham, H., Grolemund, G. 2017. R for Data Science. https://r4ds.had.co.nz/index.html 

## Feedback {#feedback_w05}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).
