# Raster data and geostatistics

## Introduction {#introduction_w07}
During this week, we will continue to work with point data by providing you with a more in-depth and detailed explanation behind the geostatistics methods of interpolation of point data, looking at various deterministic and geostatistical techniques. Because some of these techniques yield a raster dataset as an output, we will start by focusing on raster data and its applications.

This week is structured around **two** short lecture videos, **one** [assignments](#assignment_w07) that you need to do in preparation for Friday's seminar, and the practical material. As always, this [week's reading list](https://rl.talis.com/3/ucl/lists/3FB0C66A-F603-A402-A31C-28F2DA091AB6.html?lang=en-gb&login=1) is available on the UCL library reading list page for the course.

### Video: Overview {#overview_w07}
```{r 07-short-lecture-welcome, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('0c605bb0-59f2-44ec-aaa1-d09c2d22bc2e') %>% use_align('left')
```
[Lecture slides] [[Watch on MS stream]]()

## Raster data and map algebra
In the previous weeks, we have predominantly worked with **vector data**. However, depending on the nature of your research problem, you may also encounter **raster data**. Each of these GIS models has its own advantages and disadvantages. 

```{r 07-raster-as-text, echo=FALSE, fig.align='center', fig.cap='A hypothehtical raster and a vector model of landuse ([ESRI 2019](https://desktop.arcgis.com/en/arcmap/10.3/manage-data/geodatabases/raster-basics.htm)).'}
knitr::include_graphics('images/week07/07_raster_and_vector.png')
```

The main difference between vector and raster model is that raster datasets are composed of pixels (or grid cells), while vector dataset are composed of geometric points and lines (paths). This means that a raster dataset represent a geographic phenomemon by dividing the world into discrete rectangular cells laid out in a grid. Each cell holds one value that represents some characteristic of that gric cell's location. Probably one of the most common or well-known types of raster data is land cover derived from satellite imagery, however, there are many situations in which raster datasets are used. 

Within R, we can use the functions from the `raster` package for reading, writing, manipulating, analysing, and modelling raster data; however, before we do that let's first dive a little deeper into raster data and **map algebra**.

### Video: Raster data and map algebra
```{r 07-raster-data-and-map-algebra, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('c2e8d774-2506-454a-9a92-d5c18d0c6064') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Raster data and map algebra
For the first part of this week practical material we will be using raster datasets from the [Population Change and Geographic Inequalities in the UK, 1971-2011 (PopChange)](https://www.liverpool.ac.uk/geography-and-planning/research/popchange/introduction/) project. In this [ESRC-funded project](https://esrc.ukri.org/research/our-research/secondary-data-analysis-initiative/), researchers from the University of Liverpool created raster population surfaces from publicly available Census data (1971, 1981, 1991, 2001, 2011). Population surfaces are estimates of counts of people for regular grids, for example, population in a certain subgroup over 1km by 1km grid cells. In the example below, we will be using the 2001 and 2011 Age 30-44 years population surfaces but feel free to choose and download two different datasets through to https://popchange-data.liverpool.ac.uk/.

#### Question {-}
1. Small-area counts of people in a variety of population subgroups are publicly relesead for each Census, so why was it necessary to create these raster population surfaces?

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Population surface GB 2001 - Age 30-44      | `asc`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/raster/5a_ascii_grid2001_Age_Age30_44.asc) |
| Population surface GB 2011 - Age 30-44      | `asc`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/raster/5a_ascii_grid2011_Age_Age30_44.asc) |
| Local Authorithy Districts London 2020      | `shp`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/zip/london_lad_2020.zip) |

If you did not download two dataset directly from the [PopChange](https://www.liverpool.ac.uk/geography-and-planning/research/popchange/introduction/) data resource, download the individual files above to your own computer and as usual make sure [your data directory](spatial-autocorrelation.html#setting-up-the-environment) is set up correctly and the data are unzipped where necessary. 

Before we open up the data in R, try to have a 'non-spatial sneak peak' at the `.asc` file by opening it in a normal texteditor, for instance, *TextEdit* on Mac OS or *NotePad* on Windows. What you will notice is that the `asc` file, which is an exchange format, is in very fact a flat plain text file!

```{r 07-raster-vector, echo=FALSE, fig.align='center', fig.cap='Raster or plain text?'}
knitr::include_graphics('images/week07/07_raster_as_text.png')
```
#### Question {-}
1. Any idea on what the first few lines of the `asc` file, when opened with a texteditor, mean?

```{r 07-settings-and-options, warnings=FALSE, message=FALSE, echo=FALSE}
# settings
options(max.print = 30)
```
```{r 07-load-libraries-and data, warnings=FALSE, message=FALSE}
# load libraries
library(raster)
library(RColorBrewer)
library(rgdal)

# load data
pop01_3044 <- raster('raw/raster/5a_ascii_grid2001_Age_Age30_44.asc')
pop11_3044 <- raster('raw/raster/5a_ascii_grid2011_Age_Age30_44.asc')

# inspect 2001 // this can be a little slow, especially for large raster
plot(pop01_3044)

# plot 2011 // this can be a little slow, especially for large rasters
plot(pop11_3044)

# plot pretty 2001 // this can be a little slow, especially for large rasters
plot(pop01_3044,col=brewer.pal(6,'Blues'),main='Popolation surface age 30 - 44 in 2011',axes=FALSE)

# plot pretty 2011 // this can be a little slow, especially for large rasters
plot(pop11_3044,col=brewer.pal(6,'Blues'),main='Popolation surface age 30 - 44 in 2011',axes=FALSE)
```

#### Question {-}
1. The `plot()` function automatically adds a legend to our raster dataset. What do these numbers represent?

Now all data are loaded, we have been able to quickly make a halfway decent map, we are ready to look a bit better at our raster dataset. In a way a raster dataset (or any raster image for that matter), can be thought of as a big matrix of *n_rows* by *n_columns*. Every cell in the matrix holds a value - because of this many standard descriptive techniques can be applied onto raster data. In very fact, we can do some algebra with these datasets! 

**Map algebra** is a set-based algebra for manipulating geographic data, coined by [Dana Tomlin](https://en.wikipedia.org/wiki/Dana_Tomlin) in the early 1980s. Map algebra operations and functions are typically broken down into four groups: local, focal, zonal and global. We will explore some of them in the remainder of this section. Let's start, however, by confining our area to Greater London rather than using the whole of Great Britain.

```{r 07-confine-that-area-yall, warnings=FALSE, message=FALSE}
# load data
lad <- readOGR(dsn='raw/boundaries/london_lad_2020/london_lad_2020.shp')

# clip raster to extent greater london
lon01_3044 <- crop(pop01_3044,extent(lad))
lon11_3044 <- crop(pop11_3044,extent(lad))

# inspect
lon01_3044

# inspect
lon11_3044

# transfer values raster values to shape of greater london
lon01_3044 <- rasterize(lad, lon01_3044, mask=TRUE)
lon11_3044 <- rasterize(lad, lon11_3044, mask=TRUE)

# plot
plot(lon01_3044)

# plot 
plot(lon11_3044)
```

## Interpolation and geostatistics
Now we know how to work with raster data, we can move on to point data interpolation and geostatistics. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data. We might also want to smooth the data across space so that we cannot interpret the results of individuals, but still identify the general trends from the data. This is particularly useful when the data corresponds to individual persons and disclosing their locations is unethical. 

### Video: Interpolation and geostatistics
```{r 07-interpolation-and-geostatistics, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('c2e8d774-2506-454a-9a92-d5c18d0c6064') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Interpolation and geostatistics
Spatial interpolation is the prediction of a given phenomenon in unmeasured locations. For that, we need a set of procedures to calculate predicted values of the variable of interest, given calibration data. We will explore several methods by looking at air pollution in London by getting data from the [Londonair](https://www.londonair.org.uk/LondonAir/General/about.aspx) website. Londonair is the website of the London Air Quality Network (LAQN), and shows air pollution in London and south east England that is provided by the [Environmental Research Group](https://www.imperial.ac.uk/school-public-health/environmental-research-group/) of Imperial College London. The data are captured by hundreds of sensors at various continuous monitoring sites in London and the south east of England. The best of it all? The data are publicly available for download!

```{r 07-monitoring-in-london, echo=FALSE, fig.align='center', fig.cap='Continuous monitoring sites in London ([Londonair 2020](https://www.londonair.org.uk/london/asp/publicdetails.asp)).'}
knitr::include_graphics('images/week07/07_london_air.png')
```

#### Question {-}
1. Although we will use the air pollution data captured at the continuous monitoring sites in London, why may using air pollution not be the perfect type of data to use here?

We could manually download `csv` files from the Londonair website for all the monitoring sites we are interested in, however, there is also a [R package](https://davidcarslaw.github.io/openair/) that makes things much easier. Have a look at the [documentation of the openair package](https://cloud.r-project.org/web/packages/openair/openair.pdf) so that you get an idea why we use the `importMeta()` and `importKCL()` functions below!

Because `openair` contains **C++** code, a compiler is needed. For Windows, for example, [Rtools](https://cran.r-project.org/bin/windows/Rtools/) is needed; depending on system and configuration this can sometimes be a hassle. Fortunately, evenhough packed with functionality, we will only use `openair` to download all the air pollution data we are interested in: in case you cannot get `openair` to work on your computer you can download a copy of the dataset instead.

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Air pollution London 2019 (NO~2~)           | `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/zip/no2_london_2019.zip) |

In what follows, we will be looking at Nitrogen Dioxide (NO~2~) in London for 2019. Let's get started by importing our data using the functionality provided by `openair` or, alternatively, reading in the data you downloaded above.

```{r 07-get-data-from-london-air, warnings=FALSE, message=FALSE, eval=FALSE}
# option 1: using openair package

# libraries
library(openair)
library(tidyverse)

# import all monitoring sites
sites <- importMeta(source='kcl',all=FALSE)

# import pollution data for 2019 for all monitoring sites // this will take a few minutes
pollution <- importKCL(site=c(sites$code),year=2019,pollutant='no2',meta=TRUE)

# you can ignore the 'XXX_2019 does not exist' messages: 
# we just forcing a download for all possible sites
# irrespective of whether the monitoring site was active in 2019

# filter NA values for nitrogen dioxide
pollution <- filter(pollution,!is.na(no2))

# inspect
head(pollution)
```

```{r 07-get-data-from-zip, warnings=FALSE, message=FALSE}
# libraries
library(tidyverse)

# option 2: read in downloaded data // the file is quite large so we read directly from zip
pollution <- read_csv('raw/zip/no2_london_2019.zip')

# inspect
head(pollution)
```

Oof. That is quite a large data set. Let's quickly make it more useable by aggregating the data and get the average NO~2~ value for each monitoring site. Let's also make sure that we retain the latitude and longitude of our monitoring sites.

```{r 07-that-is-mean, warnings=FALSE, message=FALSE}
# aggregate data
avg_pol <- pollution %>% group_by(code,latitude,longitude) %>% summarise(no2=mean(no2))

# inspect
avg_pol
```

## Take home message
Although in many instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will be working with raster data. You now know the basics of working with raster datasets as well as that you can create your own raster datasets by using interpolation and geostatistical methods to predict a given phenomenon in unmeasured locations. That it is for this week! 

## Attributions {#attributions_w07}
This week's content and practical uses content and inspiration from:

- Gimond, M. 2020. Geodesic geometry. https://mgimond.github.io/Spatial/index.html
- Lansley, G., Cheshire, J. 2020. An Introduction to Spatial Data Analysis and Visualisation in R. https://data.cdrc.ac.uk/dataset/introduction-spatial-data-analysis-and-visualisation-r

## Feedback {#feedback_w07}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).
