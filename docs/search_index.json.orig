[
["index.html", "GEOG0114: Principles of Spatial Analysis Welcome Get in touch Noticed a mistake in this resource?", " GEOG0114: Principles of Spatial Analysis Justin van Dijk1 and Joanna Wilkin2 2020-11-04 Welcome Welcome to Principles of Spatial Analysis, one of the four core modules for the MSc in Geographic and Social Data Science here at UCL Geography. This module provides an introduction into the theory, methods and tools of spatial analysis essential for your career as a Data Scientist. This module has been designed in conjunction with your module on Geographic Information Systems (GIS) and Science (GIScience) from CASA to provide you with an extensive introduction into GIScience, spatial analysis and their associated tools. It has a specific focus on the principles, properties and parameters that are part of spatial analysis and how to understand and apply these effectively within geographic and data science-oriented research. The first half of the module provides detailed introductions into spatial concepts such as scale and geography, spatial dependency and autocorrelation, and spatial heterogeneity and spatial regression models. The second half then focuses on the applications of spatial analysis within current data science research, including cluster analysis and Bayesian modelling. Get in touch Dr Jo Wilkin will be taking Week 1-4, 9, and 10 of the module. You can contact her at j.wilkin [at] ucl.ac.uk or, for online office hours, you can book a half hour slot with Jo using MS Bookings. Dr Justin van Dijk will be taking Week 5-8 of the module. You can contact him at j.t.vandijk [at] ucl.ac.uk or, for online office hours, you can book a half hour slot with Justin using MS Bookings. The module is further supported by two Postgraduate Teaching Assistants: Alfie Long and Jakub Wyszomierski. Noticed a mistake in this resource? Please let us know through the GitHub issues tab, send us a message over MS Teams, or contact us by e-mail. Department of Geography, https://www.mappingdutchman.com/↩︎ Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/joanna-wilkin↩︎ "],
["course-information.html", "Course information Module structure Weekly topics Learning objectives Reading list Module assessment details Useful additional resources", " Course information Module structure This module consists of ten self-led workshops, ten interactive seminar discussions and ten help sessions. Each week, we’ll provide an online workshop, provided as a worksheet with videos and instructions to complete the practical component of the workshop. All online classes will be held on the Principles of Spatial Analysis ‘team’. In addition, each week will have its own reading list or additional ‘recommended’ (optional, not required!) online tutorials we know of that you might want to also complete. Weekly topics Week Date Topic 1 05/10/2020 Spatial analysis for data science 2 12/10/2020 Representation, scale and geography in spatial analysis 3 19/10/2020 Spatial properties, relationships and operations 4 26/10/2020 Spatial dependence, spatial autocorrelation and defining neighbours 5 02/11/2020 Exploratory Spatial Data Analysis reading week reading week reading week 6 16/11/2020 Mapping clusters with point pattern analysis 7 23/11/2020 Geostatistics, interpolation and raster analysis 8 30/11/2020 DBScan and cluster analysis for urban applications 9 09/12/2020 To be confirmed 10 04/12/2020 Creating reproducible research and module recap Learning objectives By the end of the module, you should: have a good understanding of the principles underlying the analysis of spatial data in general and spatial statistics in particular; be able to use GIS software and tools for generating and visualising summary statistics; be able to examine, analyse and simulate a range of spatial patterns and processes; be able to use geostatistical tools to analyze and interpolate spatial patterns; appreciate the many different sources of uncertainty in spatial data and spatial processing and how to address such issues in analysis and research; be able to master the key concepts in network analysis with a focus on social and spatial networks (now in Intro to Data Science and Advanced Data Science modules); be able to explain several novel applications of spatial analysis techniques within geographic and social data science applications. Reading list We link to books and resources throughout each practical. The full reading list for the course is provided on the UCL library reading list page for the course. Alternatively, you can always easily find the link to the Reading List in the top right of any Moodle page for our module, under “Library Resources”. This Reading List will be updated on a weekly basis, in preparation for the week to come, so you may see some weeks without reading for now. But please check back at the start of each week as the lecture, seminar and/or workshop material is released for that week to check for new readings. All reading for that week will be provided by the time your learning materials are released - so you will not need to check the reading list for updates as the week progresses. Module assessment details The assessment for Principles of Spatial Analysis is set across two pieces of separate coursework, weighted at 50% each. The first piece of coursework will involve the completion of a spatial analysis project, based on the theory, concepts and application learnt during the module. A 1,500 word report will be submitted, alongside the code used within the project, which describes the analysis undertaken and the results of the analysis. Further guidance will be provided in Week 5 of the module. The second piece of coursework will be a written (1,500 word) review on a current data science application that uses spatial analysis as its core methodology. The application can be drawn from the lecture material, particularly during Weeks 8 and 9, or one of your own choice. Further guidance will be provided in Week 9 of the module. Guidance for both pieces will be uploaded to Moodle, once provided. Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 "],
["spatial-analysis-for-data-science.html", "1 Spatial analysis for data science", " 1 Spatial analysis for data science The first week of PSA will introduce how geography and Geographical Information Science (GIScience) fits within the wider data science discipline and why there is a need to specialise in spatial and social analysis for data science. To provide this understanding, you will first work your way through a short document written specifically for this week to provide an extensive overview to why a geographical understanding to data science is critical to accurate and valid data analysis. Next, through a recorded lecture, you’ll be given a short introduction to Geographical Information Systems (GIS) tools for spatial data science and an explanation to how these tools have changed over the last decade, including a shift from traditional GIS software towards programming-based analysis in research applications. We’ll then show you examples of different types of GIS software through a recorded tutorial. To gain a practical basic understanding of the differences across these software, including their ease of use, the recorded tutorial will show you the steps and processing to create a simple choropleth map of population and population density in London. This week plays a formative role in providing everyone with baseline from which to not only pursue this module, but the other technical modules on the MSc. This week’s content is available on Moodle. "],
["geographical-representation.html", "2 Geographical representation", " 2 Geographical representation The first part of this week will look at spatial representation data models, i.e. how we transform geographic features and phenomena into geographic data for use within GIS. We will then explore the role of scale and geography within spatial analysis and provide you with a critical understanding of how both can impact and effect the analysis of data, particularly when looking at ‘event’ type data, i.e. the occurrence of a specific phenomenon over space. We will then introduce you to the role and usage of administrative geographies and discuss how they are subject to the Modifiable Area Unit Problem as well as its the consequences, including ecological fallacies. We will then discuss methods to account for these issues, including population standardization, as well as highlight alternative methods for representing data beyond traditional choropleth maps. The interactive lecture will also introduce the role of projections and what considerations you should make when choosing the projection for your analysis; projections are further discussed in Week 3 of CASA0005. The practical component of the week puts these issues into practice with an analysis of crime data from the UK and its various administrative geographies, as well as voting patterns in the USA. The practical component also introduces the two types of data joining primarily used in spatial analysis: attribute and spatial joins. This week’s content is available on Moodle. "],
["spatial-properties-and-relationships.html", "3 Spatial properties and relationships", " 3 Spatial properties and relationships Understanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This week, we look to provide you with a thorough introduction into using spatial operations (and the properties and relationships associated with them) through an introductory lecture, a research-based analysis (with demo and practical) and then a research task which we will look at during this week’s seminar. Within the lecture, we will highlight the different ways of conceptualizing key spatial properties, such as distance, and the impact this may have on measurement. We then focus on their application within spatial operations, and how they can be used for the selection, subset and validation of data. We then look at the core terminology used to define spatial relationships and how they can be used to process datasets, using the operations previously mentioned. The practical utilises these concepts to investigate the accessibility of greenspace for schools across London. Recent research (Bijnens et al, 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. Here we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed. For the practical, we provide an introduction to the research problem and outline how we devise a research methodology to be able to investigate our research questions. We then look at the required processing steps to create the final dataset that can be used in our analysis. This is followed by a short demo in which Jo will demonstrate the analysis visually in QGIS. We then ask you to recreate the analysis by creating a script in R-Studio (code provided) - this will allow you to replicate the analysis for other cities within the U.K, or even further afield if you can extract the same data. Finally, in preparation for this week’s seminar, we ask you to watch a five minute video from a local news channel in Jo’s hometown - ready to discuss as a possible research task in Friday’s seminar. This week’s content is available on Moodle. "],
["spatial-autocorrelation.html", "4 Spatial autocorrelation 4.1 Neighbours 4.2 Quantifying difference over space", " 4 Spatial autocorrelation Figure 4.1: Spatial distribution of qualitative data taken from the Colorbrewer 2.0 website. This week, we focus on the first of two key properties of spatial data: spatial dependence. Spatial dependence is the idea, as introduced in the first week via Tobler’s Law (1970), that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space. A critical first step of spatial autocorrelation is to define the criteria under which a spatial unit (e.g. an areal or point unit) can be understood as a “neighbor” to another unit. As highlighted in the previous week, spatial properties can often take on several meanings, and as a result, have an impact on the validity and accuracy of spatial analysis. This multiplicity also can be applied to the concept of spatial neighbours which can be defined through adjacency, contiguity or distance-based measures. As the specification of these criteria can impact the results, the definition followed therefore need to be grounded in particular theory that aims to represent the process and variable investigated. This week looks at spatial dependence and autocorrelation in detail, focusing on the different methods of assessment. As part of this, we look at the multiple methods to defining spatial neighbours and their suitability of use across different spatial phenomena – and how this approach is used to generate spatial weights for use within these spatial autocorrelation methods as well as their potential to generate spatially-explicit variables. We put these learnings into practice through an analysis of spatial dependence of our areal crime data from Week 2, experimenting with the deployment of different neighbours and the impact of their analyses. For this practical we will look at the distribution of thefts from persons in the borough of Camden, home of the main UCL Campus, and birthplace of Alfie Long (statue forthcoming). 4.1 Neighbours If we want to come up with quantifiable descriptions of variables and how they vary over space, then we need to find ways of quantifying the distance from point to point. When you attach values to the polygons of wards in London, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like. There can appear to be clusters, or the distribution can be random. If you want to explain and discuss variables, the underlying causes, and the possible solutions to issues, it becomes useful to quantify how clustered, or at the opposite end, how random these distributions are. This issue is known as spatial autocorrelation. “Things are more like things that are closer to them.” - Tobler(ish) In raster data, variables are measured at regular spatial intervals (or interpolated to be represented as such). Each measurement is regularly spaced from its neighbours, like the pixels on the screen you are reading this from. With vector data, the distance of measurement to measurement, and the size and shape of the “pixel” of measurement becomes part of the captured information. Whilst this can allow for more nuanced representations of spatial phenomena, it also means that the quantity and type of distance between measurements needs to be acknowledged. Quantify the relationship of “neighbour” allows this to be done systematically. If you want to calculate the relative spatial relation of distributions, knowledge of what counts as a “neighbour” becomes useful. Neighbours can be neighbours due to euclidean distance (distance in space), or they can be due to shares relationships, like a shared boundary, or they can simply be the nearest neighbour, if there aren’t many other vectors around. Depending on the variable you are measuring the appropriateness of neighbourhood calculation techniques can change. 4.1.1 Setting up the environment To begin let’s load the libraries needed for this practical. # for spatial objects and operations library(sf) ## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0 # for finding nearest neighbours in sf objects library(nngeo) # for reading and quantifying data library(data.table) # for visualising spatial data library(tmap) Download the data and put it in a folder we can find. We will use the same structure as the data directory from the previous week. If you are using the same directory for this weeks work then you can put these files in the same directories. If not make new ones with names that work. Alternatively you can make your own structure and use that. To follow along with the code as it is and not edit the file paths there should be a folder called “raw/” in your project working directory, if not best to make one. Inside this create a folder called “crime”. You can download the crime data we will be working with from here and put it in the crime folder. Inside the “raw” directory there should be a directory called “administrative_boundaries”. In here make a folder called “camden_oas_2011/”. You can download the shape data we will be using from here. To check that you have all the files you need have a look at what’s in these directories. We can create variables that hold a text version of the filepath, you can edit these to contain the filepath of your own copy of the data. # make the filepath objects shapefile_dir_path &lt;- &quot;raw/boundaries/camden_oas_2011/&quot; crime_data_dir_path &lt;- &quot;raw/crime/&quot; # we can use list.files() with the direcotry names to find their contents # check for the files within these folders list.files(shapefile_dir_path) ## [1] &quot;OAs_camden_2011.dbf&quot; &quot;OAs_camden_2011.prj&quot; &quot;OAs_camden_2011.shp&quot; ## [4] &quot;OAs_camden_2011.shx&quot; list.files(crime_data_dir_path) ## [1] &quot;2019_camden_theft_from_person.csv&quot; &quot;2019_london_bicycle_theft.csv&quot; # then we can assign the filenames to variables as well shapefile_name &lt;- &quot;OAs_camden_2011.shp&quot; crimedata_filename &lt;- &quot;2019_camden_theft_from_person.csv&quot; # you can use paste0() to add these together as complete filepaths paste0(shapefile_dir_path, shapefile_name) ## [1] &quot;raw/boundaries/camden_oas_2011/OAs_camden_2011.shp&quot; paste0(crime_data_dir_path, crimedata_filename) ## [1] &quot;raw/crime/2019_camden_theft_from_person.csv&quot; Now we know the data is there we can load and plot the shape data. Plot the output areas of the borough Camden. # first read in the data camden_oas &lt;- st_read(paste0(shapefile_dir_path, shapefile_name), # the filename of the shapefile crs=27700 # the projection information of the shape # as we know it is in British National Grid ) ## Reading layer `OAs_camden_2011&#39; from data source `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/raw/boundaries/camden_oas_2011/OAs_camden_2011.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 749 features and 17 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 523954.5 ymin: 180965.4 xmax: 531554.9 ymax: 187603.6 ## CRS: EPSG:27700 # plot it to have a look plot(st_geometry(camden_oas)) You can see how one of these output areas could have many more neighbours than others, they vary a great deal in size and shape. The dimensions of these objects change over space, as a result the measurements within them must change too. Output areas are designed to convey and contain census information, so they are created in a way that maintains a similar number of residents in each one. The more sparsely populated an OA the larger it is. Output Areas are designed to cover the entirety of the land of England and Wales so they stretch over places where there are no people. In the north of Camden the largest Ouput Areas span over Hampstead Heath, a large park. Let’s explore how to find different kinds of neighbours using the example of one randomly selected output area. # we have randomly selected one OA &quot;E00004174&quot;, the OA containing the UCL main campus # and coincidentally also the birthplace of Alfie Long (statue forthcoming) # highlight it in a map using tmap tm_shape(camden_oas) + tm_borders(col=&quot;black&quot;) + tm_shape(camden_oas[camden_oas$OA11CD==&quot;E00004174&quot;,]) + tm_fill(col = &quot;red&quot;) 4.1.2 Euclidean neighbours The first way we are going to call something a neighbour is by using Euclidean distance. As our OA shapefile is projected in BNG (British National Grid), the coordinates are planar, going up 1 is the same distance as going sideways 1. Even better the coordinates are in metric measurements so it’s easy to make up heuristic distances. Let’s call every output area with a centroid 500m or less away from the centroid of our chosen OA a neighbour. # make a variable for our chosen output area so that we could change # it and all the other code could remain the same chosen_oa &lt;- &quot;E00004174&quot; # find only nearest neighbours to our chosen output area centroid, find all within 500 meters # select only the the centroid of our chosen output area and all other areas (with st_centroid) # we set the maximum number of neighbours we want to find to &quot;50&quot; (with parameter k) # we set the maximum distance of calling an OA centroid a neigbour to &quot;500&quot; (with parameter maxdist) # we return a sparse matrix that tells us whether each OA is a neighbour or not (with parameter sparse) chosen_oa_neighbours &lt;- st_nn(st_geometry(st_centroid(camden_oas[camden_oas$OA11CD==chosen_oa,])), st_geometry(st_centroid(camden_oas)), sparse = TRUE, k = 50, maxdist = 500) ## projected points # see the output class(chosen_oa_neighbours) ## [1] &quot;list&quot; # this is a list object that contains the row number of all the found numbers # you can access the pure vector of these numbers by using [[1]] notation # this can then be used to subset the OA output areas object # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # plot the borough OA outlines in black with # the chosen oa filled in red and # the neighbours in green tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &quot;green&quot;) + # highlight only the chosen OA tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + tm_fill(col = &quot;red&quot;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&quot;black&quot;) 4.1.3 Shared boundary neighbours The next way of calculating neighbours takes into account the actual shape and location of the polygons in our shapefile. This has only recently been added to the world of sf(), previously we would have reverted to using the sp() package and others that depend on it such as spdep(). I’ve actually taken this bit of code from an issue raised in the Git repository of the sf package. You can look at it here. We create functions that check whether any polygons share boundaries or overlap one another, and then also check by how much. These new functions are based on the st_relate() function. The different cases of these are known as queen, and rook. These describe the relations in a similar way to the possible chess board movements of these pieces. # for rook case st_rook = function(a, b = a) st_relate(a, b, pattern = &quot;F***1****&quot;) # for queen case st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) # these methods are now in the documetation for st_relate() # have a look in the examples at the bottom of the documentation Now that we’ve created the functions lets try them out. # let&#39;s use the same example oa as previously # you can check that it exists and if not assign the variable again if(exists(&quot;chosen_oa&quot;)==TRUE) { print(paste0(&quot;You have already selected &quot;,chosen_oa)) }else{ chosen_oa &lt;- &quot;E00004174&quot; print(&quot;You have not yet assigned the variable chosen_oa in this session. You have now been assigned E00004174, home of the main UCL campus and birthplace of Alfie Long (statue forthcoming).&quot;) } ## [1] &quot;You have already selected E00004174&quot; # the neighbourhood assigning works in a very similar way to the way st_nn() works # this way we will return a sparse list for the row numbers that are neighbours # and use that to subset the oa shapefile chosen_oa_neighbours &lt;- st_rook(st_geometry(camden_oas[camden_oas$OA11CD==chosen_oa,]), st_geometry(camden_oas)) # get the names of neighbours # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # plot the borough OA outlines in black with # the chosen oa filled in red and # the neighbours in green tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &quot;green&quot;) + tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + # highlight only the chosen OA tm_fill(col = &quot;red&quot;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&quot;black&quot;) Because the tolerance of the shared boundaries in the st_rook pattern and the st_queen pattern in the example I have chosen they both assign the same neighbours. This is true for many non-square polygons as the difference is often given as whether two shapes share one or more points. Therefore the difference can have more to do with the resolution and alignment of your polygons than the actual spatial properties they represent. They can and do find different neighbours in other situations. Follow the grid example in the st_relate() documentation if you want to see it working. 4.2 Quantifying difference over space Now that we have found the different ways of finding neighbours we can consider how they relate to one another. There are two ways of looking at spatial autocorrelation: Local: This is the difference between an area and its neighbours. You would expect neighbours to be similar, but you can find exceptional places and results by seeing if places are quantifiably mor like or dislike their neighbours than the average other place. Global: This is a way of creating a metric of how regularly or irregularly clustered the variables are over the entire area studied. But before we start that let’s get into the data we are going to use! 4.2.1 Our property is theft. We’ll be using personal theft data from around camden. Our neighbourhood analysis of spatial autocorrelation should allow us to quantify the pattern of distribution of reported theft from persons in Camden in 2019. # read in the data camden_theft &lt;- fread(&quot;raw/crime/2019_camden_theft_from_person.csv&quot;) # this is a csv, and the coordinates are in columns called &quot;X&quot; and &quot;Y&quot; # you can convert this into an sf object quite easily camden_theft &lt;- st_as_sf(camden_theft, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 27700) # have a look at these points plot(camden_theft) This is point data, but we are interested in the polygons and how this data relates to the administrative boundaries it is within. Let’s count the number of thefts in each OA. This is a spatial operation that is often called “point in polygon”. As we are just counting the number of occurences in each polygon it is quite easy. In the future you may often want to aggregate over points for an area, or in reverse assign values from the polygon to the points. # this code counts the lengths (number) of points that intersect each polygon # let&#39;s add a new column to our OA sf object called n_thefts to store this camden_oas$n_thefts &lt;- lengths(st_intersects(camden_oas, camden_theft)) # plot a quick map of it tm_shape(camden_oas) + tm_fill(col=&quot;n_thefts&quot;) # you can see our map is massively skewed by central london # meaning that the results in central london (the south of camden) # are so much larger than those in the north that it makes it harder # to see the smaller differences between other areas # try taking the square root of the number of thefts # (there are 0s in data so log would return some negatives) camden_oas$sqrt_n_thefts &lt;- sqrt(camden_oas$n_thefts) # plot a quick map of it tm_shape(camden_oas) + tm_fill(col=&quot;sqrt_n_thefts&quot;) # there: a much more nuanced picture 4.2.2 Global Moran’s I With a global Moran’s I we can test how “random” the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a “random” distribution, and 1 is a “non-random” distribution of clearly defined clusters. To calculate the global Moran’s I you need an adjacency matrix that contains the information of whether or not an OA is next to another, for an even more nuanced view you can include distance, or a distance weighting in the matrix rather than just the TRUE or FALSE, to take into account the strength of the nieghbourhoodness. I will revisit the neighbourhood calculation used earlier then develop it from a one to all to an all to all computation. # here is the previous way of calculating neighbourhoodness for our chosen OA # but now we will add an extra argument that states we want a non-sparse adjacency matrix returned # find only nearest neighbours to our chosen output area centroid, find all within 500 meters # select only the the centroid of our chosen output area and all other areas (with st_centroid) # we set the maximum number of neighbours we want to find to &quot;50&quot; (with parameter k) # we set the maximum distance of calling an OA centroid a neigbour to &quot;500&quot; (with parameter maxdist) # we return a non-sparse matrix that tells us whether each OA is a neighbour or not (with parameter sparse) chosen_oa_neighbours &lt;- st_nn(st_geometry(st_centroid(camden_oas[camden_oas$OA11CD==chosen_oa,])), st_geometry(st_centroid(camden_oas)), sparse = FALSE, k=50, maxdist = 500) ## projected points # check the object type class(chosen_oa_neighbours) ## [1] &quot;matrix&quot; # this single row matrix contains a true or false for each OA in the OA sf object indicating whether it is true or false # if you sum() it it counts the number of TRUEs in it sum(chosen_oa_neighbours) ## [1] 21 Because of the way Moran’s I functions in R it is necessary to use sp() and spdep(). So load them in. # for doing spatial operations (now mostly superseded by sf) library(sp) # for our neighbourhood and Moran&#39;s I analysis library(spdep) ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` As you will see these methods and functions have quite esoteric and complicated syntax. Some of the operations they will do will be similar to the examples shown earlier, but the way they assign and store variables makes it much quicker to run complex spatial operations. # check the current class class(camden_oas) ## [1] &quot;sf&quot; &quot;data.frame&quot; # convert it to sp camden_oas_sp &lt;- as_Spatial(camden_oas, IDs=camden_oas$OA11CD) # check the class of the new object class(camden_oas_sp) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Now we can make the esoteric and timesaving “nb” object! # make the nb camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # what kind of object is it? class(camden_oas_nb) ## [1] &quot;nb&quot; # have a look inside str(camden_oas_nb,list.len=10) ## List of 749 ## $ : int [1:7] 10 15 215 303 327 375 464 ## $ : int [1:5] 19 72 309 365 430 ## $ : int [1:3] 133 152 709 ## $ : int [1:7] 78 131 152 286 314 582 651 ## $ : int [1:5] 67 316 486 492 703 ## $ : int [1:8] 7 68 317 487 556 612 625 638 ## $ : int [1:3] 6 68 317 ## $ : int [1:7] 57 58 164 358 429 605 684 ## $ : int [1:5] 58 164 489 609 700 ## $ : int [1:7] 1 215 245 311 327 366 644 ## [list output truncated] ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= Factor w/ 749 levels &quot;E00004120&quot;,&quot;E00004121&quot;,..: 268 187 447 448 531 533 534 56 522 263 ... ## - attr(*, &quot;call&quot;)= language poly2nb(pl = camden_oas_sp, row.names = camden_oas_sp$OA11CD) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE It’s basically the same as the sparse list of neighbours that we created before using st_nn() and st_rook(). Now, however it will work with the spdep() ecosystem of functions. If you look at the output you can see that it automatically uses “queen” case. # further into the sp rabbit hole we can now create a matrix from this (like the non-sparse output of the st_nn/st_rook processes) camden_oas_nb_mat &lt;- nb2mat(camden_oas_nb, style=&#39;B&#39;) # check the sum of one columnd sum(camden_oas_nb_mat[1,]) ## [1] 7 # where nb2mat() extends this is in allowing weighting camden_oas_nb_mat &lt;- nb2mat(camden_oas_nb, style=&#39;W&#39;) # check the sum of one column sum(camden_oas_nb_mat[1,]) ## [1] 1 Now the sum of one row instead of being the total number of neighbours for that OA instead adds up to 1. This is important for the calculation of Moran’s I, as the metric is designed to return a values between -1 and 1. If the neghbour relations are not limited to add up to one the value for Moran’s I can exceed -1 and 1, and it loses meaning when compared against other cases. Now we need to calculate a separate weights list object to use for the Moran’s I calculation. # create the list weights object nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;B&#39;) # have a look at the class class(nb_weights_list) ## [1] &quot;listw&quot; &quot;nb&quot; # so it&#39;s nb AND weights # now use that to create a quick Moran&#39;s I moran(camden_oas_sp$n_thefts, nb_weights_list, n=length(nb_weights_list$neighbours), S0=Szero(nb_weights_list)) ## $I ## [1] 0.5543285 ## ## $K ## [1] 75.21583 On the scale of -1 to 1 where does it sit? This is just a quick way to check the score. To do so properly you need to check between a randomly distributed version of the variables, and the variables as they are. To do so you need to use the MonteCarlo method where you actually carry this out. You can specify how many times it does this to increase precision. # run it 599 times mc_model &lt;- moran.mc(camden_oas_sp$n_thefts, nb_weights_list, nsim=599) # what do we get? mc_model ## ## Monte-Carlo simulation of Moran I ## ## data: camden_oas_sp$n_thefts ## weights: nb_weights_list ## number of simulations + 1: 600 ## ## statistic = 0.55433, observed rank = 600, p-value = 0.001667 ## alternative hypothesis: greater This model shows that our distribution of thefts differs significantly from a random distribution. 4.2.3 Local Moran’s I With a measurement of local spatial autocorrelation we could find hotspots of theft that are surrounded by areas of much lower theft. According to the previous global statistic these are not randomly distributed pockets but would be outliers against the general trend of clusteredness! These could be areas that contain very specific locations, where interventions could be made that drastically reduce the rate of crime rather than other areas where there is a high level of ambient crime. # you need the nb object and the nb and weights list # make the nb camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # create the list weights object # but importantly with the row stadardisation this time nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;W&#39;) # use the localmoran() function local_moran_camden_oa_theft &lt;- localmoran(camden_oas_sp$n_thefts, nb_weights_list) To properly utilise these local statistics and make in intuitively useful map we need to combine them with our crime count variable. Because of the way the new variable will be calculated we need to rescale our variable so that the mean is 0. this means that the lowest values will be -ve and the highest +ve. The distribution and the relationship between the will be preserved though. # rescale that variable! camden_oas_sp$scale_n_thefts &lt;- scale(camden_oas_sp$n_thefts) To compare against its neighbours we need to create a new column that carries information about the neighbours. This is called a spatial lag function! What fun! The “lag” just refers to the fact you are comparing one observation against another, this can be used between timed observations. In this case, the “lag” we are looking at is between neighbours. # create a spatial lag variable and save it to a new column camden_oas_sp$lag_scale_n_thefts &lt;- lag.listw(nb_weights_list, camden_oas_sp$scale_n_thefts) Now we have used sp for all it is worth it’s time to head back to the safety of sf()! # convert to sf camden_oas_moran_stats &lt;- st_as_sf(camden_oas_sp) To make a human readable version of the map we will generate some labels for our findings from the Local Moran’s I stats. This process calculates what the value of each polygon is compared to its neighbours and works out if they are similar or dissimilar and in which way, then gives them a text label to describe the relationship. I the text label, the first term describes the area being focused on and the second represents that areas neighbours. # run all of these through to assign variables # for the statistical significance version assign a level # of statistical significance for the p value, column 5 of the local moran model sig_level &lt;- 0.1 # version with significance value camden_oas_moran_stats$quad_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &quot;high-high&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &quot;low-low&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &quot;high-low&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &quot;low-high&quot;, ifelse(local_moran_camden_oa_theft[,5] &gt; sig_level, &quot;not-significant&quot;, &quot;not-significant&quot;))))) # version without significance check camden_oas_moran_stats$quad_non_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &quot;high-high&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &quot;low-low&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &quot;high-low&quot;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &quot;low-high&quot;,NA)))) Now that the horrible looking conditional is finished we can see what we did. To understand how this is working we can look at the data non-spatially. As we recentred the data, our axes should split the data neatly into their different area vs spatial lag relationship categories. To do this plot we will need to use the plotting package ggplot2. library(ggplot2) Let’s make the scatterplot using the scaled number of thefts for the areas in the x axis and their spatially lagged results in the y axis. # plot the results without the satistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_non_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&quot;Scaled Thefts (n)&quot;) + ylab(&quot;Lagged Scaled Thefts (n)&quot;) + labs(colour=&quot;Relative to neighbours&quot;) + geom_point() # plot the results nnw with the satistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&quot;Scaled Thefts (n)&quot;) + ylab(&quot;Lagged Scaled Thefts (n)&quot;) + labs(colour=&quot;Relative to neighbours&quot;) + geom_point() Now let’s see how they are arranged spatially. # map all of the results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &quot;quad_non_sig&quot;) # map only the statistically significant results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &quot;quad_sig&quot;) As our data are so spatially clustered we can’t see any outlier places (once we have ignored the non-significant results. This suggests that the pattern of theft from persons is not highly concentrated in very small areas or particular Output Areas, and instead is spread on a larger scale than we have used here. To go further than we have today it would be possibe to run the exact same code but using a larger scale, perhaps LSOA, or Ward, and compare how this changes the Moran’s I statistics globally and locally. Or, to gain statistical significance in looking at the difference between areas getting more data perhaps over a longer timescale, where there are less areas with 0 thefts. "],
<<<<<<< HEAD
["exploratory-spatial-data-analysis.html", "5 Exploratory spatial data analysis 5.1 Introduction 5.2 Tidy data 5.3 Exploratory spatial data analysis in R 5.4 Spatial heterogeneity 5.5 Take home message 5.6 Attributions 5.7 Feedback", " 5 Exploratory spatial data analysis 5.1 Introduction This week, we focus on the second of the two key properties of spatial data: spatial heterogeneity. With the underlying process (or processes) that govern a spatial variable likely to vary across space, a single global relationship for an entire region of study may not adequately model the process that governs outcomes in any given location of the study region. As a result, multiple methods have been developed to incorporate ‘space’ into traditional regression models, including spatial lag models, spatial error models, and Geographical Weighted Regression. This week provides the building blocks to conducting a statistical and spatial investigation into the relationships between spatial variables, looking at the concept of Exploratory Spatial Data Analysis (ESDA). We then look at the two of the three types of spatial regression models in turn to understand their methodology and potential advantages and limitations in accounting for space when modelling relationships. However, before we move on to ESDA, we will look into some aspects of data preparation, data cleaning, and creating a tidy dataset. In the first part of this week’s session we will start by creating a tidy dataset using some data from the UK 2011 Census of Population. In the second part of this week’s session we will explore potential factors that may contribute to bicycle theft in London. We conduct an ESDA of these variables, followed by statistical and spatial regression analysis to determine whether our variables contribute to bicycle theft in our area of study. This week is structured by three short lecture videos, two assignments that you need to do in preparation for Friday’s seminar, and the practical material. As always, this week’s reading list is available on the UCL library reading list page for the course. Please be aware that you will need to submit the result of the second assignment before Friday morning 09h00&amp; UK time (GMT+0). 5.1.1 Video: Overview [Lecture slides] [Watch on MS stream] 5.2 Tidy data Over the past weeks a lot of information has come your way, diving deep into the world of Spatial Data Science. However, whilst you are slowly becoming proficient in using R and Python to solve complex (spatial) problems, it is now a good moment to start thinking about data themselves and how they are organised. This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be dirty data. With dirty data we mean data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Exploratory Spatial Data Analysis very much starts with your data preparation . In the following, you will learn a consistent way to structure your data in R: tidy data. Tidy data, as formalised by R Wizard Hadley Wickham in his contribution to the Journal of Statistical Software is not only very much at the core of the tidyverse R package, but also of general importance when organising your data. In the words, of the Wizard: Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. 5.2.1 Video: Tidy data [Lecture slides] [Watch on MS stream] 5.2.2 What do tidy data look like? You can represent the same underlying data in multiple ways. The example below, taken from the the tidyverse package and described in the R for Data Science book, shows that the same data can organised in four different ways. # load the tidyverse library(tidyverse) Table 1: table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Table 2: table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Table 3: table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Table 4a: table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 Table 4b: table4b ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 None of these representations are wrong per se, however, not are equally easy to use. Only Table 1 can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Figure 5.1: A visual representation of tidy data by Hadley Wickham. Fortunately, there are some functions in the tidyr and dplyr packages, both part of the tidyverse that will help us cleaning and preparing our datasets to create a tidy dataset. The most important and useful functions are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgropus for further processing tidyr pivot_longer convert data from wide format to long format tidyr pivot_wider convert long format dataset to wide format Remember that when you encounter a function in a piece of R code that you have not seen before and you are wondering what it does that you can get access the documentation through ?name_of_function, e.g. ?pivot_longer. For almost any R package, the documentation contains a list of arguments that the function takes, in which format the functions expects these arguments, as well as a set of usage examples. 5.2.3 Example: Creating tidy data Now we know what consitute tidy data, we can put this into practice with an example using some data from the Office for National Statistics. Let’s say we are asked by our bosses to analyse some data on the ethnic background of the UK population, for instance, because we want to get some insights into the relationship between COVID-19 and ethnic background. Our assignment is to calculate the relative proportions of each ethnic group within the administrative geography of the Middle layer Super Output Area (MSOA). In order to do this, we have been given a file that contains data on ethnicity by age group at the MSOA-level of every person in the 2011 UK Census who is 16 year or older. Download the file to your own computer and set up your data directory in the same fashion as you did last week. You can also decide on setting up your own folder structure, no problem, just make sure that you update the file paths in the practical to match the file paths on your own computer. Make sure that after downloading you first unzip the data, for instance, using 7-Zip on Windows or using The Unarchiver on Mac OS. File download File Type Link Etnicity by age group 2011 Census of Population csv Download We start by making sure our tidyverse is loaded into R and using the read_csv() function to read our csv file. # load the tidyverse library(tidyverse) # read data into dataframe df &lt;- read_csv(&#39;raw/population/msoa_eth2011_ew_16plus.csv&#39;) # inspect the dataframe: number of columns ncol(df) ## [1] 385 # inspect the dataframe: number of rows nrow(df) ## [1] 7201 # inspect the dataframe: sneak peak print(df, n_extra=2) ## # A tibble: 7,201 x 385 ## msoa11cd `Sex: All perso… `Sex: All perso… `Sex: All perso… `Sex: All perso… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E020025… 215 206 204 0 ## 2 E020025… 175 170 168 1 ## 3 E020025… 140 128 128 0 ## 4 E020025… 160 155 154 0 ## 5 E020025… 132 130 130 0 ## 6 E020025… 270 263 261 0 ## 7 E020025… 124 119 117 0 ## 8 E020025… 150 125 117 0 ## 9 E020025… 178 166 159 0 ## 10 E020025… 162 159 157 0 ## # … with 7,191 more rows, and 380 more variables: `Sex: All persons; Age: Age ## # 16 to 17; Ethnic Group: White: Gypsy or Irish Traveller; measures: ## # Value` &lt;dbl&gt;, `Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: ## # Other White; measures: Value` &lt;dbl&gt;, … Because the data are split out over multiple columns, it is clear that the data are not directly suitable to establish the proportion of each ethnic group within the population of each MSOA. Let’s inspect the names of the columns to get a better idea of the structure of our data set. # inspect the dataframe: column names names(df) ## [1] &quot;msoa11cd&quot; ## [2] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categories: Ethnic group; measures: Value&quot; ## [3] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Total; measures: Value&quot; ## [4] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: English/Welsh/Scottish/Northern Irish/British; measures: Value&quot; ## [5] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Irish; measures: Value&quot; ## [6] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gypsy or Irish Traveller; measures: Value&quot; ## [7] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Other White; measures: Value&quot; ## [8] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: Total; measures: Value&quot; ## [9] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Black Caribbean; measures: Value&quot; ## [10] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Black African; measures: Value&quot; ## [11] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Asian; measures: Value&quot; ## [12] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: Other Mixed; measures: Value&quot; ## [13] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Total; measures: Value&quot; ## [14] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Indian; measures: Value&quot; ## [15] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Pakistani; measures: Value&quot; ## [16] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Bangladeshi; measures: Value&quot; ## [17] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Chinese; measures: Value&quot; ## [18] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Other Asian; measures: Value&quot; ## [19] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Total; measures: Value&quot; ## [20] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: African; measures: Value&quot; ## [21] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Caribbean; measures: Value&quot; ## [22] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Other Black; measures: Value&quot; ## [23] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Total; measures: Value&quot; ## [24] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Arab; measures: Value&quot; ## [25] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Any other ethnic group; measures: Value&quot; ## [26] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: All categories: Ethnic group; measures: Value&quot; ## [27] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Total; measures: Value&quot; ## [28] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: English/Welsh/Scottish/Northern Irish/British; measures: Value&quot; ## [29] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Irish; measures: Value&quot; ## [30] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Gypsy or Irish Traveller; measures: Value&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 355 entries ] The column names are all awfully long and it looks like the data have been split out into age groups. Further to this, the data contain within group total counts: all categories, white total, mixed/multiple ethnic group total, and so on. You can also try using View(df) or use any other form of spreadsheet software (e.g. Microsoft Excel) to browse through the dataset to get a better idea of what is happening and get a better idea of the structure of the data. You first will need to understand the structure of your dataset before you can start reorganising your dataset. Although the data is messy and we will need to reorganise our data set, it does look there is some form of structure present that we can exploit: the various columns with population counts for each ethnic group are repeated for each of the different age groups. This means that we can go through the data frame in steps of equal size to select the data we want: starting from column 2 (column 1 only contains the reference to the adminsitrative geography) we want to select all 24 columns of data for that particular age group. We can create a for loop that does exactly that: # loop through the columns of our data set for (column in seq(2,ncol(df),24)) { # index number of start column of age group start &lt;- column # index number of end column of age group stop &lt;- column + 23 # print results print(c(start,stop)) } ## [1] 2 25 ## [1] 26 49 ## [1] 50 73 ## [1] 74 97 ## [1] 98 121 ## [1] 122 145 ## [1] 146 169 ## [1] 170 193 ## [1] 194 217 ## [1] 218 241 ## [1] 242 265 ## [1] 266 289 ## [1] 290 313 ## [1] 314 337 ## [1] 338 361 ## [1] 362 385 For each age group in our data, the printed values should (!) correspond with the index number of the start column of the age group and the index number of the end column of the age group, respectively. Let’s do a sanity check. # sanity check: age group 16-17 (start column) df[,2] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categories: Ethnic g… ## &lt;dbl&gt; ## 1 215 ## 2 175 ## 3 140 ## 4 160 ## 5 132 ## 6 270 ## 7 124 ## 8 150 ## 9 178 ## 10 162 ## # … with 7,191 more rows # sanity check: age group 16-17 (end column) df[,25] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Any … ## &lt;dbl&gt; ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## # … with 7,191 more rows # sanity check: age group 18-19 (start column) df[,26] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 18 to 19; Ethnic Group: All categories: Ethnic g… ## &lt;dbl&gt; ## 1 157 ## 2 129 ## 3 102 ## 4 162 ## 5 121 ## 6 217 ## 7 126 ## 8 231 ## 9 175 ## 10 95 ## # … with 7,191 more rows # sanity check: age group 18-19 (end column) df[,49] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 18 to 19; Ethnic Group: Other ethnic group: Any … ## &lt;dbl&gt; ## 1 1 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 1 ## 9 0 ## 10 0 ## # … with 7,191 more rows All seems to be correct and we have successfully identified our columns. This is great, however, we still cannot work with our data as everything is spread out over different columns. Let’s fix this by manipulating the shape of our data by turning columns into rows. # create function columns_to_rows &lt;- function(df, start, stop) { # columns we are interested in col_sub &lt;- c(1,start:stop) # subset the dataframe df_sub &lt;- select(df,col_sub) # pivot the columns in the dataframe, exclude the MSOA code column df_piv &lt;- pivot_longer(df_sub,-msoa11cd) # rename columns names(df_piv) &lt;- c(&#39;msoa11cd&#39;,&#39;age_group&#39;,&#39;count&#39;) return(df_piv) } # test columns_to_rows(df,2,25) ## # A tibble: 172,824 x 3 ## msoa11cd age_group count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categ… 215 ## 2 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: To… 206 ## 3 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: En… 204 ## 4 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ir… 0 ## 5 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gy… 2 ## 6 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ot… 0 ## 7 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 4 ## 8 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## 9 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 2 ## 10 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## # … with 172,814 more rows This looks much better. Now let’s combine our loop with our newly created function to apply this to all of our data. # create an empty list to store our result from the loop df_lst &lt;- list() # loop through the columns of our data set for (column in seq(2,ncol(df),24)) { # index number of start column of age group start &lt;- column # index number of end column of age group stop &lt;- column + 23 # call our function and assign it to the list df_lst[[length(df_lst)+1]] &lt;- columns_to_rows(df,start=start,stop=stop) } # paste all elements from the list underneath one another # do.call executes the function &#39;rbind&#39; for all elements in our list df_reformatted &lt;- as_tibble(do.call(rbind,df_lst)) # and the result df_reformatted ## # A tibble: 2,765,184 x 3 ## msoa11cd age_group count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categ… 215 ## 2 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: To… 206 ## 3 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: En… 204 ## 4 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ir… 0 ## 5 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gy… 2 ## 6 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ot… 0 ## 7 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 4 ## 8 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## 9 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 2 ## 10 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## # … with 2,765,174 more rows Now the data is in a much more manageable format, we can move on with preparing the data further. We will start by filtering out the columns (now rows!) that contain all categories and the within group totals. We will do this by cleverly filtering our data set on only a part of the text string that is contained in the age_group column of our dataframe using a regular expression. We further truncate the information that is contained in the age_group column to make all a little more readable. # filter rows # this can be a little slow because of the regular expression! df_reformatted &lt;- filter(df_reformatted,!grepl(&#39;*All categories*&#39;,age_group)) df_reformatted &lt;- filter(df_reformatted,!grepl(&#39;*Total*&#39;,age_group)) # create variable that flags the 85 and over category # this can be a little slow because of the regular expression! df_reformatted$g &lt;- ifelse(grepl(&#39;85&#39;,as.character(df_reformatted$age_group)),1,0) # select information from character 41 (85 and over category) or from character 38 df_reformatted &lt;- mutate(df_reformatted,group = ifelse(g==0,substr(as.character(age_group),38,500), substr(as.character(age_group),41,500))) # remove unnecessary columns df_reformatted &lt;- select(df_reformatted, -age_group, -g) We are now really getting somewhere, although in order for our data to be tidy each variable must have its own column. We also want, within each ethnic group, to aggregate the individual values within each age group. # pivot table and aggregate values df_clean &lt;- pivot_wider(df_reformatted,names_from=group,values_from=count,values_fn=sum) # rename columns # names are assigned based on index values, so make sure that the current columnnames match the # order of the new columnames otherwise our whole analysis will be wrong! names(df_clean) &lt;- c(&#39;msoa11cd&#39;,&#39;white_british&#39;,&#39;white_irish&#39;,&#39;white_traveller&#39;,&#39;white_other&#39;,&#39;mixed_white_black_caribbean&#39;, &#39;mixed_white_black_african&#39;,&#39;mixed_white_asian&#39;,&#39;mixed_other&#39;,&#39;asian_indian&#39;,&#39;asian_pakistani&#39;, &#39;asian_bangladeshi&#39;,&#39;asian_chinese&#39;,&#39;asian_other&#39;,&#39;black_african&#39;,&#39;black_caribbean&#39;,&#39;black_other&#39;, &#39;other_arab&#39;,&#39;other_other&#39;) # tidy data df_clean ## # A tibble: 7,201 x 19 ## msoa11cd white_british white_irish white_traveller white_other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E020025… 6775 17 27 51 ## 2 E020025… 4688 11 8 31 ## 3 E020025… 4609 20 3 55 ## 4 E020025… 4653 4 6 131 ## 5 E020025… 4369 13 2 38 ## 6 E020025… 7320 11 3 73 ## 7 E020025… 4274 16 2 96 ## 8 E020025… 4713 33 24 308 ## 9 E020025… 5344 17 42 111 ## 10 E020025… 4583 29 6 100 ## # … with 7,191 more rows, and 14 more variables: ## # mixed_white_black_caribbean &lt;dbl&gt;, mixed_white_black_african &lt;dbl&gt;, ## # mixed_white_asian &lt;dbl&gt;, mixed_other &lt;dbl&gt;, asian_indian &lt;dbl&gt;, ## # asian_pakistani &lt;dbl&gt;, asian_bangladeshi &lt;dbl&gt;, asian_chinese &lt;dbl&gt;, ## # asian_other &lt;dbl&gt;, black_african &lt;dbl&gt;, black_caribbean &lt;dbl&gt;, ## # black_other &lt;dbl&gt;, other_arab &lt;dbl&gt;, other_other &lt;dbl&gt; Finally. We now have a tidy dataset that we can work with! 5.2.4 Assignments Assignment 1 Since we went through all the trouble of cleaning and creating this file, the first task for Friday’s seminar is to finalise the analysis: use the cleaned data set to create a table that, for each MSOA, contains the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population. It could look something like this: msoa11cd white_british white_irish etc. E02002562 0.74 0.03 … E02002560 0.32 0.32 … Tips First think what you what steps you would need to take in order to get the group proportions. Write them down on a piece of paper if you like. Once you have identified the steps, then start coding. Conduct sanity checks. Every time you have written a line of code, check the results to see if the code did indeed give the result that you expected to get. Google is your friend. Do not be afraid to search for specific solutions and suggestions, chances are that there have been other people who have faces similar problems and posted their questions on stackoverflow. Assignment 2 Further to calculating the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population, we also want to make a choropleth map at district level of the UK population that is older than 60 as a proportion of the total population. For this analysis we have available one dataset with the administrative boundaries of the UK 2020 Local Authorithy Districts administrative geography and we have a csv file that holds population estimates for the UK in 2019. Use everything you have learned over the past weeks to produces this map. Some tips: Tips Inspect both the shapefile and the csv file to get an idea of how your data look like. Use any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.). The csv file does contain a mix of administrative geographies, and you will need to do some data cleaning by filtering out Country, County, Metropolitan County, and Region before you link the data to the shapefile. You are in charge of deciding what software you want to use to visualise the data (ArcGIS, R, QGIS, etc.). You now have to make your own decisions on how to go about this problem. Although this practical has so far covered some of the functions and strategies you might need, the data cleaning and data preparation process is not the same. Map submission E-mail your final map as a PDF or JPG to j.t.vandijk [at] ucl.ac.uk before Friday morning 09h00 UK time (GMT+0). Also, ‘bring’ your final map with you to Friday’s seminar. File download File Type Link Local Authorithy District boundaries 2020 shp Download Mid-Year Population Estimates 2019 csv Download 5.3 Exploratory spatial data analysis in R Exploratory Data Analysis got introduced in the the late 1970s by the American mathematician John Tukey. Tukey thought that in much data analysis there was much emphasis on statistical hypothesis testing and very little development of new hypotheses. He therefore made a distinction between confirmatory data analysis and exploratory data analysis (EDA). EDA is a collection of descriptive techniques used to detect patterns, identify outliers, and form hypotheses from the data. An EDA typically involves descriptive statistics and data visualisation. Exploratory techniques generally stay ‘close’ to the original data. Exploratory spatial data analysis (ESDA) is the extension of exploratory data analysis (EDA) to the spatial realm. 5.3.1 Video: Exploratory spatial data analysis [Lecture slides] [Watch on MS stream] 5.3.2 Example: Exploring our dataset After successfully creating the maps that we were requested to make, our bosses got so happy that they directly gave us a new assignment. This time they want us to look at bicycle theft in London, specifically they want to know which factors contribute to bicycle theft in London’s central boroughs of Southwark, Lambeth, Camden, Westminster, Tower Hamlets, City of London, and Hackney. For this assignment, we have access to two open datasets. The first one contains the English Index of Multiple Deprivation (IMD) for 2019. The English indices of deprivation measure relative deprivation in small areas in England called Lower-layer Super Output Areas (LSOAs) The index of multiple deprivation is the most widely used of these indices. The second dataset contains some of the input data that have been used to create the Access to Healthy Assets and Hazards (AHAH). AHAH is a multi-dimensional index developed by the Consumer Data Research Centre (CDRC) for Great Britain measuring how ‘healthy’ neighbourhoods are. We also have access to the LSOAs for London. Further to these two open datasets, we have a shapefile with all of London’s LSOAs as well as a csv file that contains the coordinates of all tube stations. File download File Type Link Selected Lower-layer Super Output Areas London 2011 shp Download Bicycle theft data London 2019 csv Download Index of Multiple Deprivation London 2019 csv Download Access to Healthy Assets and Hazards (AHAH) London csv Download Locations of tube stations in London csv Download Download the individual files to your own computer and again make sure your data directory is set up correctly and the data are unzipped. Then load the libraries and the data into R. # load libraries library(tidyverse) library(sf) library(tmap) library(Rfast) # load spatial data lsoa &lt;- st_read(&#39;raw/boundaries/london_lsoa_2011/lsoa_london_2011_sel.shp&#39;) ## Reading layer `lsoa_london_2011_sel&#39; from data source `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/raw/boundaries/london_lsoa_2011/lsoa_london_2011_sel.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1020 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 523847.7 ymin: 169650 xmax: 539533.3 ymax: 188327.4 ## CRS: 27700 names(lsoa) &lt;- c(&#39;lsoa11cd&#39;,&#39;geometry&#39;) # load data crime &lt;- read_csv(&#39;raw/crime/2019_london_bicycle_theft.csv&#39;) imd &lt;- read_csv(&#39;raw/index/imd_ew_2019.csv&#39;) ahah &lt;- read_csv(&#39;raw/index/ahah_input_gb_2016.csv&#39;) tube &lt;- read_csv(&#39;raw/locations/london_underground_stations.csv&#39;) # inspect crime data head(crime) ## # A tibble: 6 x 4 ## month long lat type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-03 -0.0981 51.5 Bicycle theft ## 2 2019-03 -0.0981 51.5 Bicycle theft ## 3 2019-03 -0.0976 51.5 Bicycle theft ## 4 2019-03 -0.0930 51.5 Bicycle theft ## 5 2019-03 -0.0941 51.5 Bicycle theft ## 6 2019-03 -0.0930 51.5 Bicycle theft # inspect imd data head(imd) ## # A tibble: 6 x 3 ## lsoa11cd imd_rank imd_dec ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E01000001 29 199 9 ## 2 E01000002 30 379 10 ## 3 E01000003 14 915 5 ## 4 E01000005 8 678 3 ## 5 E01000006 14 486 5 ## 6 E01000007 7 256 3 # inspect ahah data head(ahah) ## # A tibble: 6 x 4 ## lsoa11cd gamb_dist pubs_dist off_dist ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E01000001 0.390 0.181 0.381 ## 2 E01000002 0.417 0.221 0.680 ## 3 E01000003 0.398 0.192 0.422 ## 4 E01000005 0.280 0.228 0.617 ## 5 E01000006 0.599 0.611 0.648 ## 6 E01000007 0.377 0.621 0.734 # inspect tube location data head(tube) ## # A tibble: 6 x 3 ## station lat long ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abbey Road 51.5 0.00372 ## 2 Abbey Wood 51.5 0.120 ## 3 Acton Central 51.5 -0.263 ## 4 Acton Main Line 51.5 -0.268 ## 5 Acton Town 51.5 -0.280 ## 6 Addington Village 51.4 -0.0327 Looking at the data you will notice that both the bike theft data and the tube location data exist of longitudes and latitudes. The IMD dataset has an lsoa11cd column, an IMD rank column, and an IMD decile column. The AHAH dataset has an lsoa11cd column, a distance to the nearest gamling outlet column, a distance to the nearest pub column, and a distance to the nearest off license column. The technical report of the AHAH index suggests these distances represent the mean distance (kilometres) by car of all postcodes within each LSOA to the nearest outlet. Good to know! Let’s start by linking the IMD dataset and the AHAH dataset to the spatial dataset. Since we are using sf for our spatial data, we can very easily join them together using the common variable in these datasets: lsoa11cd. # join imd data lsoa_join &lt;- left_join(lsoa,imd,by=c(&#39;lsoa11cd&#39;)) # join ahah data lsoa_join &lt;- left_join(lsoa_join,ahah,by=c(&#39;lsoa11cd&#39;)) # inspect the result head(lsoa_join) ## Simple feature collection with 6 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 525360.9 ymin: 177857 xmax: 538921.5 ymax: 183191.6 ## CRS: 27700 ## lsoa11cd imd_rank imd_dec gamb_dist pubs_dist off_dist ## 1 E01004665 5 376 2 0.2504762 0.2019048 0.4619048 ## 2 E01003111 5 781 2 0.4455556 0.2648148 0.4603704 ## 3 E01004276 30 144 10 0.8789286 0.1492857 1.8007143 ## 4 E01004706 27 823 9 0.6737500 0.7354167 0.3750000 ## geometry ## 1 MULTIPOLYGON (((529040.9 17... ## 2 MULTIPOLYGON (((531023.1 17... ## 3 MULTIPOLYGON (((537563.7 17... ## 4 MULTIPOLYGON (((525406.7 18... ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2 rows ] This is all looking fine. Let’s now turn our turn our bike theft location dataset into a spatial dataset using the longitude and latitude values and get total bike theft counts within each of our available LSOAs. # filter out points without a longitude and/or latitude value crime &lt;- filter(crime,!is.na(long) | !is.na(lat)) crime_points &lt;- st_as_sf(crime, coords=c(&#39;long&#39;,&#39;lat&#39;),crs=4326) # project into british national grid (epsg 27700) crime_points &lt;- st_transform(crime_points,27700) # count the number of points intersecting within each of London&#39;s LSOAs lsoa_join$n_crime &lt;- lengths(st_intersects(lsoa_join,crime_points)) # inspect the result tm_shape(lsoa_join) + tm_fill(col=&#39;n_crime&#39;) This all looks good. We can see, however, that the the number of times bike theft was reported are highly skewed by Central London. It is now a good idea to explore the bike theft data a bit further to see how the data are distributed non-spatially by plotting them. # summary summary(lsoa_join$n_crime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 4.000 8.993 10.000 260.000 # histogram ggplot(lsoa_join, aes(x=n_crime)) + geom_histogram(binwidth=.5, colour=&#39;black&#39;, fill=&#39;white&#39;) # boxplot ggplot(lsoa_join, aes(x=n_crime))+ geom_boxplot() # as the data is very skewed, let&#39;s cube transform the bike theft data # and inspect the transformed data again lsoa_join$n_crime_cube &lt;- abs(lsoa_join$n_crime)^(1/3) # summary summary(lsoa_join$n_crime_cube) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.260 1.587 1.666 2.154 6.383 # histogram ggplot(lsoa_join, aes(x=n_crime_cube)) + geom_histogram(binwidth=.5, colour=&#39;black&#39;, fill=&#39;white&#39;) # boxplot ggplot(lsoa_join, aes(x=n_crime_cube))+ geom_boxplot() # inspect the result tm_shape(lsoa_join) + tm_fill(col=&#39;n_crime_cube&#39;) This does look much better, although there are clearly several outliers in our data set with some areas that have a higher number of cases of bike theft. Let’s now also inspect our input data before we start to see whether there is a relationship between our independent variables and our dependent variable (i.e. the number of cases of bike theft). # summary imd deciles summary(lsoa_join[,4]) ## gamb_dist geometry ## Min. :0.1230 MULTIPOLYGON :1020 ## 1st Qu.:0.3362 epsg:27700 : 0 ## Median :0.4603 +proj=tmer...: 0 ## Mean :0.5189 ## 3rd Qu.:0.6268 ## Max. :3.8188 # boxplot imd deciles ggplot(lsoa_join, aes(x=imd_dec))+ geom_boxplot() # summary ahah summary(lsoa_join[,5:7]) ## pubs_dist off_dist geometry ## Min. :0.06364 Min. :0.1574 MULTIPOLYGON :1020 ## 1st Qu.:0.22088 1st Qu.:0.4110 epsg:27700 : 0 ## Median :0.32001 Median :0.5723 +proj=tmer...: 0 ## Mean :0.37230 Mean :0.6586 ## 3rd Qu.:0.47544 3rd Qu.:0.8262 ## Max. :1.59219 Max. :2.0869 # boplot ahah pub distance ggplot(lsoa_join, aes(x=gamb_dist))+ geom_boxplot() # boplot ahah off license distance ggplot(lsoa_join, aes(x=off_dist))+ geom_boxplot() # gambling distance vs bike theft ggplot(lsoa_join,aes(x=gamb_dist, y=n_crime_cube)) + geom_point() The input data look good. There are no real strange and unexpected outliers or missing values. At the same time there is enough heterogeneity in our data. This is important because if the input data have very similar values for al LSOAs they would not be very useful in trying to explain the differences in bike theft. However, we have not touched our tube locations dataset just yet. Perhaps bicycle theft happens more around tube stations? Let’s do some quick spatial analysis and add for each LSOA the distance to the nearest tube station to our dataset. Please note: the following operation requires some computing power as for all of our selected LSOAs in London we are going to calculate the euclidean distance from its centroid to all tube stations and create a distance matrix. In case the calculation takes too much time on your computer, you can download the distance matrix instead and use dist_matrix &lt;- data.matrix(read_csv('path_to_file')) to continue with the practical. # create a point dataset from the tube csv file tube_points &lt;- st_as_sf(tube, coords=c(&#39;long&#39;,&#39;lat&#39;),crs=4326) # project into british national grid (epsg 27700) tube_points &lt;- st_transform(tube_points,27700) # create a distance matrix dist_matrix &lt;- st_distance(st_centroid(lsoa_join),tube_points) ## Warning in st_centroid.sf(lsoa_join): st_centroid assumes attributes are ## constant over geometries of x # assign the distance of the closes tube station lsoa_join$tube_dist &lt;- rowMins(dist_matrix,value=TRUE) Great. So, now we can continue. Let’s start by seeing if there is any visual relationship between our input (independent) variables and our output (dependent) variable by plotting them against each other. # imd decile vs bike theft ggplot(lsoa_join,aes(x=imd_dec, y=n_crime_cube)) + geom_point() # pub distance vs bike theft ggplot(lsoa_join,aes(x=pubs_dist, y=n_crime_cube)) + geom_point() # gambling outlets distance vs bike theft ggplot(lsoa_join,aes(x=gamb_dist, y=n_crime_cube)) + geom_point() # off license distance vs bike theft ggplot(lsoa_join,aes(x=off_dist, y=n_crime_cube)) + geom_point() # tube station distance vs bike theft ggplot(lsoa_join,aes(x=tube_dist, y=n_crime_cube)) + geom_point() The IMD deciles, which are in fact categorical data, do not show any obvious pattern, however, the distance to the various outlets do show some pattern. Let’s correlate. # imd decile vs bike theft cor(lsoa_join$imd_dec,lsoa_join$n_crime_cube) ## [1] 0.05715025 # gambling outlet distance vs bike theft cor(lsoa_join$gamb_dist,lsoa_join$n_crime_cube) ## [1] -0.2768482 # pub distance vs bike theft cor(lsoa_join$pubs_dist,lsoa_join$n_crime_cube) ## [1] -0.4175234 # off license distance vs bike theft cor(lsoa_join$off_dist,lsoa_join$n_crime_cube) ## [1] -0.1750449 # tube location distance vs bike theft cor(lsoa_join$off_dist,lsoa_join$n_crime_cube) ## [1] -0.1750449 So, looking at these correlations there seems to be some relationship between our explanatory (independent) variables and our dependent variables. It is time to form a hypothesis: LSOAs that are in a less deprived IMD decile, LSOAs with a lower mean distance to one of our outlets of interest, and LSOAs that are closer to a tube station, experience more bike theft. The null hypothesis then becomes: this relationship does not exist. Let’s see if we can test this hypothesis using an Ordinary Least Squares (OLS) regression model. In R, models are typically fitted by calling a model-fitting function, in our case lm() (linear model). The lm() function returns a fitted model which we can inspect by calling summary() on the object. # linear model crime_model &lt;- lm(n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + off_dist + tube_dist, data=lsoa_join) # get the results summary(crime_model) ## ## Call: ## lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1400 -0.4691 -0.0322 0.4464 4.0813 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4663553 0.0894498 27.573 &lt; 2e-16 *** ## imd_dec 0.0257697 0.0126189 2.042 0.0414 * ## gamb_dist -0.2611631 0.1194108 -2.187 0.0290 * ## pubs_dist -1.3719555 0.1489382 -9.212 &lt; 2e-16 *** ## off_dist -0.0100393 0.0831764 -0.121 0.9040 ## tube_dist -0.0005161 0.0001010 -5.109 3.86e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8039 on 1014 degrees of freedom ## Multiple R-squared: 0.2034, Adjusted R-squared: 0.1995 ## F-statistic: 51.79 on 5 and 1014 DF, p-value: &lt; 2.2e-16 In running a regression model, we are effectively trying to test our null hypothesis. If our null hypothesis is true then we expect our coefficients to equal to 0. Right now our null hypothesis is that there is no relationship between bike theft and our input variables. Without going in much further detail, as this will be in-depth covered in your CASA005 module, we can see that the null hypothesis is disproven and, in fact, there seems to be a negative relationship between our distance-related input variables and our dependent variable. Four out of our five variables are significant at a 0.05 level of significance. The R-squared, which is a measure of the goodness-of-fit of the model, suggest that 19.95 per cent of the variation in our dependent variable can be explained by our independent variables. This means that a relationship is present, but it is not very strong! Nonetheless, the model suggest that LSOAs with a higher IMD score, a higher accessibility to pubs, a higher accessibility to gambling outlets, and a higher accessibility to a tube location, experience more bike theft. Consider that when parking your bike somewhere overnight. Important Normally, you would also check the underlying assumptions of the linear model (linearity, homoscedasticity, independence, and normality), however, this will be covered more in-depth in your CASA005 module and for our current purposes we are going to be a little naughty and accept the results as is and assume that all our underlying assumptions are satisfied (even though we are actually violating some of these underlying assumptions!). If you do want to get into some more detail on regression analysis right here and now, the following video explains the process of fitting a line to your data very clearly. While you at it, do have a look at some of the other videos on the StatsQuestwith Josh Stammer Youtube Channel. 5.4 Spatial heterogeneity We now have established a relationship between IMD deciles, access to gambling outlets, access to pubs, access to off licenses, access to the nearest tube stations, and the occurrence of bike theft. However, our data are very spatial and we did not incorporate this spatial distribution into our model. So if we want to develop a regression model for bike theft in London we may have to recognise this spatial component. On top of this, a regression model assumes independence of observations: what happens in LSOA001 is not related to what happens in LSOA002 or any other LSOA. However, we know from last weeks content that this is not always the case because of spatial autocorrelation. 5.4.1 Video: Spatial heterogeneity [Lecture slides] [Watch on MS stream] 5.4.2 Example: Accounting for spatial heterogeneity Let’s start by exploring wether autocorrelation is an issue in our current analysis. Make sure the following libraries are loaded: # load libraries library(sf) library(tmap) library(sp) library(spdep) library(spatialreg) Now we are ready to go again, we are going to start by having a closer look at the residuals of our model. In other words, the residuals represent the distance between the observed values of the dependent variable and the predicted values of our dependent variables in our linear model. Residuals should be randomly distributed over space, otherwise we may be dealing with spatial autocorrelation. Strange words coming from a geographer, but right now we do not want to see any spatial patterning. # extract the residuals from the model and assign to our spatial dataset lsoa_join$residuals &lt;- residuals(crime_model) # extract the predicted values from the model and assign to our spatial dataset lsoa_join$predicted &lt;- fitted(crime_model) # example observed, residual, predicted of first LSOA in our data lsoa_join[1,c(1,9,11,12)] ## Simple feature collection with 1 feature and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 528983.6 ymin: 177857 xmax: 529243.5 ymax: 178173.5 ## CRS: 27700 ## lsoa11cd n_crime_cube residuals predicted geometry ## 1 E01004665 1.587401 -0.2261528 1.813554 MULTIPOLYGON (((529040.9 17... Now we have our residuals assigned to their spatial units, we can plot the residuals in number of standard deviations from the mean. # standardise lsoa_join$sd_mean &lt;- (lsoa_join$predicted - mean(lsoa_join$predicted)) / sd(lsoa_join$predicted) # inspect the result breaks &lt;- c(-14,-3,-2,-1,1,2,3,14) tm_shape(lsoa_join) + tm_fill(&#39;sd_mean&#39;, style=&#39;fixed&#39;, breaks=breaks, palette=&#39;-RdBu&#39;) + tm_borders(alpha = 0.1) You can see the spatial patterning of areas of over-prediction in the centre of Greater London with areas of under-prediction concentrating on the edges of the study area. We could be dealing with some spatial autocorrelation, so we need to test for it again. We will be using a Global Moran’s I for this. Remember with a global Moran’s I we can test how ‘random’ the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a ‘random’ distribution, and 1 is a ‘non-random’ distribution of clearly defined clusters. # force sf to sp lsoa_join_sp &lt;- as_Spatial(lsoa_join, IDs=lsoa_join$lsoa11cd) # create a list of neighbours using the &#39;queen&#39; criteria lsoa_nb &lt;- poly2nb(lsoa_join_sp, row.names=lsoa_join_sp$lsoa11cd, queen=TRUE) # check neighbours summary(lsoa_nb) ## Neighbour list object: ## Number of regions: 1020 ## Number of nonzero links: 5800 ## Percentage nonzero weights: 0.5574779 ## Average number of links: 5.686275 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 11 12 13 ## 14 64 152 259 244 166 66 39 10 2 2 2 ## 14 least connected regions: ## E01001823 E01004720 E01004677 E01003143 E01033596 E01033710 E01004692 E01004218 E01003030 E01003142 E01001813 E01032775 E01004739 E01004732 with 2 links ## 2 most connected regions: ## E01000946 E01000953 with 13 links # generate the row standardised spatial weight matrix wm &lt;- nb2mat(lsoa_nb, style=&#39;B&#39;) rwm &lt;- nb2listw(lsoa_nb, style=&#39;W&#39;) Now we can execute a Moran’s test for the regression residuals. We use lm.morantest() for this. This is important because this function takes into account that the input data are residuals, something which the ‘normal’ Moran’s I test does not do. # Moran&#39;s I lm.morantest(crime_model, rwm, alternative=&#39;two.sided&#39;) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## Moran I statistic standard deviate = 18.427, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## Observed Moran I Expectation Variance ## 0.3439158982 -0.0035656704 0.0003556032 Not surprisingly, the test is significant: we are indeed dealing with spatial autocorrelation. This means that our OLS regression may not be the best way to respresent our data as currently our regression line will per definition under-predict or over-predict in areas that are close to one another. There are two ways of taking this spatial dependence into account: by means of a spatial error model or by means of a spatially lagged model. These methods are quite different as they treat spatial autocorrelation in different ways. In the first case, the observed spatial dependence is not considered as an actual spatial process but is an effect of spatial clustering. In the second case, spatial dependence is considered as an actual spatial process and ‘space’ should be incorporated as an explanation in the model. How do we now decide which model to use? Well, first, as with any model, the relationship between independent and dependent variables must make sense. So, for instance, if you think that the spatial dependence is just an artefact of the data distribution rather than a truly spatial process, you should go with a spatial error model. Vice versa, if you think that the spatial dependence is the result of a spatial process, you should consider the spatially lagged model. In less clear cut cases, we can also get a little help by making use of the Lagrange Multiplier test statistics. We can these run these tests as follows: # lagrange mulitplier lm.LMtests(crime_model, rwm, test = c(&#39;LMerr&#39;,&#39;LMlag&#39;,&#39;RLMerr&#39;,&#39;RLMlag&#39;)) ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## LMerr = 327.63, df = 1, p-value &lt; 2.2e-16 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## LMlag = 355.85, df = 1, p-value &lt; 2.2e-16 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## RLMerr = 1.1286, df = 1, p-value = 0.2881 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## RLMlag = 29.348, df = 1, p-value = 6.049e-08 How dow we use this information? The output of the test is split into two groups: the standard tests (LMerr and LMlag) and the robust versions of these tests (RLMerr and RMlag). The robust version should be used only if both of the standard tests are significant (i.e. below the .05 level). If only one of the standard tests comes back signficant, but the other one is not, then it becomes very easy to decide between the spatial error and the spatially lagged model: choose the one that is significant! In case both the standard tests are significant we would need to look at the outcomes of the robust tests and make a decision. In such a situation, the best suggestion is to compare the p-values of both tests and select the model that is ‘more significant’ than the other. In our case the situation is very clear and we should go with a spatially lagged model rather than a spatial error model. Let’s do that using the lagsarlm() function. Also, let’s remove the distance to the neareast off license as that variable was not significant to begin with. # execute the spatially lagged model # this can take a little bit of time, be patient! crime_model_lag &lt;- lagsarlm(n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + tube_dist, data=lsoa_join, listw=rwm) # let&#39;s inspet the results summary(crime_model_lag) ## ## Call:lagsarlm(formula = n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + ## tube_dist, data = lsoa_join, listw = rwm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.202012 -0.393879 0.016251 0.403836 3.178825 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.12440760 0.09842553 11.4239 &lt; 2.2e-16 ## imd_dec 0.03119665 0.01078435 2.8928 0.0038186 ## pubs_dist -0.77701415 0.12727377 -6.1051 1.028e-09 ## gamb_dist -0.16495609 0.09942310 -1.6591 0.0970891 ## tube_dist -0.00031330 0.00008769 -3.5729 0.0003531 ## ## Rho: 0.54502, LR test value: 244.87, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.033565 ## z-value: 16.238, p-value: &lt; 2.22e-16 ## Wald statistic: 263.67, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1099.293 for lag model ## ML residual variance (sigma squared): 0.47432, (sigma: 0.68871) ## Number of observations: 1020 ## Number of parameters estimated: 7 ## AIC: 2212.6, (AIC for lm: 2455.5) ## LM test for residual autocorrelation ## test value: 8.1292, p-value: 0.0043559 To interpret these results, we have to look at the Akaike Information Criterion (AIC). The AIC of our spatially lagged model is 2212.6, whereas the model without taking spatial dependence into account has an AIC of 2455.5 This means that the spatially lagged model is a better fit to the data then the standard OLS model. We also have a new term Rho, which is the spatial autoregressive parameter. In our case it is both positive and statistically significant, suggesting that when bike theft in the neighbours of area Xi increase, bike theft will also be higher in area Xi. At the same time, the IMD decile, the distance to the nearest pub, and the distance to the nearest tube stations are all still significant. Next time you know where not to park your bike. Be careful not to compare the regression coefficients of the spatially lagged model to the regression coefficients of the original OLS model because their meaning now has changed. Whilst in the OLS model the coefficient of any of the independent variables measures the overall impact of these variables, this is not the case in the spatially lagged model: by including the spatial lag, the effect that X has on Y depends also (indirectly) on the effect that X has on Y in its neighbours. Although we probably should not, let’s also run a spatial error model. Just for the sake of running it and see how it looks. # execute the spatially lagged model # this can take a little bit of time, be patient! crime_model_err &lt;- errorsarlm(n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + tube_dist, data=lsoa_join, listw=rwm) # let&#39;s inspect the results summary(crime_model_err) ## ## Call:errorsarlm(formula = n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + ## tube_dist, data = lsoa_join, listw = rwm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1086e+00 -3.8717e-01 5.2999e-05 4.0082e-01 3.1615e+00 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.01465810 0.10346058 19.4727 &lt; 2.2e-16 ## imd_dec 0.06067673 0.01345733 4.5088 6.519e-06 ## pubs_dist -0.96803178 0.15645501 -6.1873 6.121e-10 ## gamb_dist -0.14677894 0.11902951 -1.2331 0.2175 ## tube_dist -0.00049008 0.00012571 -3.8986 9.675e-05 ## ## Lambda: 0.58216, LR test value: 241.97, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.033829 ## z-value: 17.209, p-value: &lt; 2.22e-16 ## Wald statistic: 296.16, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1100.746 for error model ## ML residual variance (sigma squared): 0.47069, (sigma: 0.68607) ## Number of observations: 1020 ## Number of parameters estimated: 7 ## AIC: 2215.5, (AIC for lm: 2455.5) If we have good reasons to believe that we need to run a spatial error model rather than an OLS regression model, we actually notice that the model fit improves: the AIC of the spatial error model is 2215.5, whilst the AIC of the OLS model is still 2455.5 Keep in mind: whereas the spatially lagged model takes spatial dependence into account, the spatial error model simply tries to remove the influence of space and we in fact estimate a model that relaxes one of the standard regression model assumptions that the errors need to be independent. This time you could actually compare the regression coefficients from the spatial error model with those from the OLS model! 5.4.3 Geographically Weighted Regression As mentioned in the short lecture video, another option to take ‘space’ into account is by executing a Geographically Weighted Regression (GWR). However, as this week’s content is already getting very long as well as that GWR will be covered in your CASA005 module, we will not cover this now. Do have a look at the article by Shabrian et al. 2020 that is part of this week’s essential reading to see GWR in action. 5.5 Take home message Exploratory (Spatial) Data Analysis is at the core of any analysis. After you cleaned and prepared your data, you first want to inspect your data through simple and visual methods to get acquainted with the data. Then you can start looking for relationships, and, if any relationships are found you are going to want to ask the question to what extent these relationships are real or to what extent other (spatial) processes, such as spatial dependence, play a role in shaping these relationships. That is it for this week! Next week we will move to point pattern analysis, but that is next week so for now we can celebrate the successful completion of this week’s material. 5.6 Attributions This week’s content and practical uses content and inspiration from: Wickham, H., Grolemund, G. 2017. R for Data Science. https://r4ds.had.co.nz/index.html Medina, J, Solymosi, R. 2019. Crime Mapping in R. https://maczokni.github.io/crimemapping_textbook_bookdown/ 5.7 Feedback Please take a moment to give us some feedback on this week’s content. "],
["point-pattern-analysis.html", "6 Point pattern analysis 6.1 Introduction 6.2 Basic point pattern analysis 6.3 Advanced Point Pattern Analysis 6.4 Take home message 6.5 Attributions 6.6 Feedback", " 6 Point pattern analysis 6.1 Introduction In our previous practicals, we have aggregated our event data into areal units, primarily using administrative geographies, to enable its easy comparison with other datasets provided at the same spatial scale, such as the census data used in the previous week, as well as to conduct spatial autocorrelation tests. However, when locations are precisely known, spatial point data can be used with a variety of spatial analytic techniques that go beyond the methods typically applied to areal data. The set of methods unique to point data are often referred to as point pattern analysis and geostatistics. This week, we focus on point pattern analysis, whilst next week we will look at geostatistics. Within point pattern analysis, we look to detect patterns across a set of locations, including measuring density, dispersion and homogeneity in our point structures. We will look first at basic forms of point pattern analysis, including mean centers and the standard deviational ellipse, and then more powerful analysis methods, including kernel density estimation and Ripley’s K function. These latter functions help determine and/or show whether points have a random, dispersed or clustered distribution pattern at a certain scale. This week is structured by two short lecture videos, one assignment that you need to do in preparation for Friday’s seminar, and the practical material. As always, this week’s reading list is available on the UCL library reading list page for the course. Please be aware that you will need to submit the result of the assignment before Friday morning 09h00 UK time (GMT+0). 6.1.1 Video: Overview [Lecture slides] [Watch on MS stream] 6.2 Basic point pattern analysis In the previous weeks, we have aggregated our event data into areal units. In R we could do this very easily by identifying all points that fall within a polygon using the st_intersects() function from the sf package to and we used this method to aggregate ‘theft from persons’ in Camden in 2019 as well as to aggregate ‘bicycle theft’ in four of London’s boroughs. However, depending on what you are trying to find out, points do not necessarily have to be aggregateda and there are many applications in which you want to work with the point locations directly. In fact, the R package spatstat for spatial statistics is predominantly designed for analysing spatial point patterns. The mere fact that the spatstat documentation has almost 1,800 pages should give you a good idea about the genearl importance of point pattern analysis. 6.2.1 Video: Basic point pattern analysis [Lecture slides] [Watch on MS stream] 6.2.2 Example: Basic point pattern analysis After last week’s success, we got a follow-up assignment to study bicycle theft. This time our assignment is to analyse the pattern of bicycle theft for the whole of Greater London. We also have access to London’s Middle Layer super Output Areas (MSOAs) as well as the boundaries of the London boroughs. Even though we are going to analyses the points, we still need some administrative boundaries even if it was only to give some context to our maps! File download File Type Link Middle-layer Super Output Areas London 2011 shp Download Local Authorithy Districts London 2020 csv Download Bicycle theft data London 2019 csv Download Download the individual files to your own computer and as usal make sure your data directory is set up correctly and the data are unzipped. 6.3 Advanced Point Pattern Analysis 6.3.1 Video: Advanced point pattern analaysis data 6.3.2 Example: Advanced Point Pattern Analysis 6.4 Take home message 6.5 Attributions This week’s content and practical uses content and inspiration from: Gimond, M. 2020. Geodesic geometry. https://mgimond.github.io/Spatial/index.html 6.6 Feedback Please take a moment to give us some feedback on this week’s content. "],
["geostatistics.html", "7 Geostatistics 7.1 Feedback", " 7 Geostatistics Following on from Week 3 and in conjunction with this week in your CASA005 module, we will also be focusing on rasters, exploring further the creation and application of rasters from vector datasets using geostatistics. During the lecture, we will provide a more in-depth and detailed explanation behind the geostatistics methods of interpolation, looking at various deterministic and geostatistical techniques. We then introduce methods to using vector datasets with rasters, utilizing a GIS method known as zonal statistics. Following on from Week 3’s practical, we look to study further the role of greenspace quality in relation to schools and how this varies across different geographies. This week, we look at greenspace quality from a personal health perspective, analyzing air quality across our greenspaces. By analyzing the air quality of greenspace, we can provide a more detailed understanding to how “accessible” the greenspaces are to the surrounding schools. This week’s content will be made available on 23/11. 7.1 Feedback Please take a moment to give us some feedback on this week’s content. "],
=======
["exploratory-spatial-data-analysis.html", "5 Exploratory spatial data analysis 5.1 Introduction 5.2 Tidy data 5.3 Exploratory spatial data analysis in R 5.4 Spatial heterogeneity 5.5 Take home message 5.6 Attributions 5.7 Feedback", " 5 Exploratory spatial data analysis 5.1 Introduction This week, we focus on the second of the two key properties of spatial data: spatial heterogeneity. With the underlying process (or processes) that govern a spatial variable likely to vary across space, a single global relationship for an entire region of study may not adequately model the process that governs outcomes in any given location of the study region. As a result, multiple methods have been developed to incorporate ‘space’ into traditional regression models, including spatial lag models, spatial error models, and Geographical Weighted Regression. This week provides the building blocks to conducting a statistical and spatial investigation into the relationships between spatial variables, looking at the concept of Exploratory Spatial Data Analysis (ESDA). We then look at the two of the three types of spatial regression models in turn to understand their methodology and potential advantages and limitations in accounting for space when modelling relationships. However, before we move on to ESDA, we will look into some aspects of data preparation, data cleaning, and creating a tidy dataset. In the first part of this week’s session we will start by creating a tidy dataset using some data from the UK 2011 Census of Population. In the second part of this week’s session we will explore potential factors that may contribute to bicycle theft in London. We conduct an ESDA of these variables, followed by statistical and spatial regression analysis to determine whether our variables contribute to bicycle theft in our area of study. This week is structured by three short lecture videos, two assignments that you need to do in preparation for Friday’s seminar, and the practical material. As always, this week’s reading list is available on the UCL library reading list page for the course. Please be aware that you will need to submit the result of the second assignment before Friday morning 09h00 UK time (GMT+0). 5.1.1 Video: Overview [Lecture slides] [Watch on MS stream] 5.2 Tidy data Over the past weeks a lot of information has come your way, diving deep into the world of Spatial Data Science. However, whilst you are slowly becoming proficient in using R and Python to solve complex (spatial) problems, it is now a good moment to start thinking about data themselves and how they are organised. This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be dirty data. With dirty data we mean data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Exploratory Spatial Data Analysis very much starts with your data preparation . In the following, you will learn a consistent way to structure your data in R: tidy data. Tidy data, as formalised by R Wizard Hadley Wickham in his contribution to the Journal of Statistical Software is not only very much at the core of the tidyverse R package, but also of general importance when organising your data. In the words, of the Wizard: Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. 5.2.1 Video: Tidy data [Lecture slides] [Watch on MS stream] 5.2.2 What do tidy data look like? You can represent the same underlying data in multiple ways. The example below, taken from the the tidyverse package and described in the R for Data Science book, shows that the same data can organised in four different ways. # load the tidyverse library(tidyverse) Table 1: table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Table 2: table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Table 3: table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Table 4a: table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 Table 4b: table4b ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 None of these representations are wrong per se, however, not are equally easy to use. Only Table 1 can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Figure 5.1: A visual representation of tidy data by Hadley Wickham. Fortunately, there are some functions in the tidyr and dplyr packages, both part of the tidyverse that will help us cleaning and preparing our datasets to create a tidy dataset. The most important and useful functions are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgropus for further processing tidyr pivot_longer convert data from wide format to long format tidyr pivot_wider convert long format dataset to wide format Remember that when you encounter a function in a piece of R code that you have not seen before and you are wondering what it does that you can get access the documentation through ?name_of_function, e.g. ?pivot_longer. For almost any R package, the documentation contains a list of arguments that the function takes, in which format the functions expects these arguments, as well as a set of usage examples. 5.2.3 Example: Creating tidy data Now we know what consitute tidy data, we can put this into practice with an example using some data from the Office for National Statistics. Let’s say we are asked by our bosses to analyse some data on the ethnic background of the UK population, for instance, because we want to get some insights into the relationship between COVID-19 and ethnic background. Our assignment is to calculate the relative proportions of each ethnic group within the administrative geography of the Middle layer Super Output Area (MSOA). In order to do this, we have been given a file that contains data on ethnicity by age group at the MSOA-level of every person in the 2011 UK Census who is 16 year or older. Download the file to your own computer and set up your data directory in the same fashion as you did last week. You can also decide on setting up your own folder structure, no problem, just make sure that you update the file paths in the practical to match the file paths on your own computer. Make sure that after downloading you first unzip the data, for instance, using 7-Zip on Windows or using The Unarchiver on Mac OS. File download File Type Link Etnicity by age group 2011 Census of Population csv Download We start by making sure our tidyverse is loaded into R and using the read_csv() function to read our csv file. # load the tidyverse library(tidyverse) # read data into dataframe df &lt;- read_csv(&#39;raw/population/msoa_eth2011_ew_16plus.csv&#39;) # inspect the dataframe: number of columns ncol(df) ## [1] 385 # inspect the dataframe: number of rows nrow(df) ## [1] 7201 # inspect the dataframe: sneak peak print(df, n_extra=2) ## # A tibble: 7,201 x 385 ## msoa11cd `Sex: All perso… `Sex: All perso… `Sex: All perso… `Sex: All perso… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E020025… 215 206 204 0 ## 2 E020025… 175 170 168 1 ## 3 E020025… 140 128 128 0 ## 4 E020025… 160 155 154 0 ## 5 E020025… 132 130 130 0 ## 6 E020025… 270 263 261 0 ## 7 E020025… 124 119 117 0 ## 8 E020025… 150 125 117 0 ## 9 E020025… 178 166 159 0 ## 10 E020025… 162 159 157 0 ## # … with 7,191 more rows, and 380 more variables: `Sex: All persons; Age: Age ## # 16 to 17; Ethnic Group: White: Gypsy or Irish Traveller; measures: ## # Value` &lt;dbl&gt;, `Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: ## # Other White; measures: Value` &lt;dbl&gt;, … Because the data are split out over multiple columns, it is clear that the data are not directly suitable to establish the proportion of each ethnic group within the population of each MSOA. Let’s inspect the names of the columns to get a better idea of the structure of our data set. # inspect the dataframe: column names names(df) ## [1] &quot;msoa11cd&quot; ## [2] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categories: Ethnic group; measures: Value&quot; ## [3] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Total; measures: Value&quot; ## [4] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: English/Welsh/Scottish/Northern Irish/British; measures: Value&quot; ## [5] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Irish; measures: Value&quot; ## [6] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gypsy or Irish Traveller; measures: Value&quot; ## [7] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Other White; measures: Value&quot; ## [8] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: Total; measures: Value&quot; ## [9] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Black Caribbean; measures: Value&quot; ## [10] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Black African; measures: Value&quot; ## [11] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: White and Asian; measures: Value&quot; ## [12] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/multiple ethnic group: Other Mixed; measures: Value&quot; ## [13] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Total; measures: Value&quot; ## [14] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Indian; measures: Value&quot; ## [15] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Pakistani; measures: Value&quot; ## [16] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Bangladeshi; measures: Value&quot; ## [17] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Chinese; measures: Value&quot; ## [18] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Asian/Asian British: Other Asian; measures: Value&quot; ## [19] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Total; measures: Value&quot; ## [20] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: African; measures: Value&quot; ## [21] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Caribbean; measures: Value&quot; ## [22] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Black/African/Caribbean/Black British: Other Black; measures: Value&quot; ## [23] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Total; measures: Value&quot; ## [24] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Arab; measures: Value&quot; ## [25] &quot;Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Any other ethnic group; measures: Value&quot; ## [26] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: All categories: Ethnic group; measures: Value&quot; ## [27] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Total; measures: Value&quot; ## [28] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: English/Welsh/Scottish/Northern Irish/British; measures: Value&quot; ## [29] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Irish; measures: Value&quot; ## [30] &quot;Sex: All persons; Age: Age 18 to 19; Ethnic Group: White: Gypsy or Irish Traveller; measures: Value&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 355 entries ] The column names are all awfully long and it looks like the data have been split out into age groups. Further to this, the data contain within group total counts: all categories, white total, mixed/multiple ethnic group total, and so on. You can also try using View(df) or use any other form of spreadsheet software (e.g. Microsoft Excel) to browse through the dataset to get a better idea of what is happening and get a better idea of the structure of the data. You first will need to understand the structure of your dataset before you can start reorganising your dataset. Although the data is messy and we will need to reorganise our data set, it does look there is some form of structure present that we can exploit: the various columns with population counts for each ethnic group are repeated for each of the different age groups. This means that we can go through the data frame in steps of equal size to select the data we want: starting from column 2 (column 1 only contains the reference to the adminsitrative geography) we want to select all 24 columns of data for that particular age group. We can create a for loop that does exactly that: # loop through the columns of our data set for (column in seq(2,ncol(df),24)) { # index number of start column of age group start &lt;- column # index number of end column of age group stop &lt;- column + 23 # print results print(c(start,stop)) } ## [1] 2 25 ## [1] 26 49 ## [1] 50 73 ## [1] 74 97 ## [1] 98 121 ## [1] 122 145 ## [1] 146 169 ## [1] 170 193 ## [1] 194 217 ## [1] 218 241 ## [1] 242 265 ## [1] 266 289 ## [1] 290 313 ## [1] 314 337 ## [1] 338 361 ## [1] 362 385 For each age group in our data, the printed values should (!) correspond with the index number of the start column of the age group and the index number of the end column of the age group, respectively. Let’s do a sanity check. # sanity check: age group 16-17 (start column) df[,2] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categories: Ethnic g… ## &lt;dbl&gt; ## 1 215 ## 2 175 ## 3 140 ## 4 160 ## 5 132 ## 6 270 ## 7 124 ## 8 150 ## 9 178 ## 10 162 ## # … with 7,191 more rows # sanity check: age group 16-17 (end column) df[,25] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 16 to 17; Ethnic Group: Other ethnic group: Any … ## &lt;dbl&gt; ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## # … with 7,191 more rows # sanity check: age group 18-19 (start column) df[,26] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 18 to 19; Ethnic Group: All categories: Ethnic g… ## &lt;dbl&gt; ## 1 157 ## 2 129 ## 3 102 ## 4 162 ## 5 121 ## 6 217 ## 7 126 ## 8 231 ## 9 175 ## 10 95 ## # … with 7,191 more rows # sanity check: age group 18-19 (end column) df[,49] ## # A tibble: 7,201 x 1 ## `Sex: All persons; Age: Age 18 to 19; Ethnic Group: Other ethnic group: Any … ## &lt;dbl&gt; ## 1 1 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 1 ## 9 0 ## 10 0 ## # … with 7,191 more rows All seems to be correct and we have successfully identified our columns. This is great, however, we still cannot work with our data as everything is spread out over different columns. Let’s fix this by manipulating the shape of our data by turning columns into rows. # create function columns_to_rows &lt;- function(df, start, stop) { # columns we are interested in col_sub &lt;- c(1,start:stop) # subset the dataframe df_sub &lt;- select(df,col_sub) # pivot the columns in the dataframe, exclude the MSOA code column df_piv &lt;- pivot_longer(df_sub,-msoa11cd) # rename columns names(df_piv) &lt;- c(&#39;msoa11cd&#39;,&#39;age_group&#39;,&#39;count&#39;) return(df_piv) } # test columns_to_rows(df,2,25) ## # A tibble: 172,824 x 3 ## msoa11cd age_group count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categ… 215 ## 2 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: To… 206 ## 3 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: En… 204 ## 4 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ir… 0 ## 5 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gy… 2 ## 6 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ot… 0 ## 7 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 4 ## 8 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## 9 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 2 ## 10 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## # … with 172,814 more rows This looks much better. Now let’s combine our loop with our newly created function to apply this to all of our data. # create an empty list to store our result from the loop df_lst &lt;- list() # loop through the columns of our data set for (column in seq(2,ncol(df),24)) { # index number of start column of age group start &lt;- column # index number of end column of age group stop &lt;- column + 23 # call our function and assign it to the list df_lst[[length(df_lst)+1]] &lt;- columns_to_rows(df,start=start,stop=stop) } # paste all elements from the list underneath one another # do.call executes the function &#39;rbind&#39; for all elements in our list df_reformatted &lt;- as_tibble(do.call(rbind,df_lst)) # and the result df_reformatted ## # A tibble: 2,765,184 x 3 ## msoa11cd age_group count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: All categ… 215 ## 2 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: To… 206 ## 3 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: En… 204 ## 4 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ir… 0 ## 5 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Gy… 2 ## 6 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: White: Ot… 0 ## 7 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 4 ## 8 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## 9 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 2 ## 10 E02002559 Sex: All persons; Age: Age 16 to 17; Ethnic Group: Mixed/mul… 1 ## # … with 2,765,174 more rows Now the data is in a much more manageable format, we can move on with preparing the data further. We will start by filtering out the columns (now rows!) that contain all categories and the within group totals. We will do this by cleverly filtering our data set on only a part of the text string that is contained in the age_group column of our dataframe using a regular expression. We further truncate the information that is contained in the age_group column to make all a little more readable. # filter rows # this can be a little slow because of the regular expression! df_reformatted &lt;- filter(df_reformatted,!grepl(&#39;*All categories*&#39;,age_group)) df_reformatted &lt;- filter(df_reformatted,!grepl(&#39;*Total*&#39;,age_group)) # create variable that flags the 85 and over category # this can be a little slow because of the regular expression! df_reformatted$g &lt;- ifelse(grepl(&#39;85&#39;,as.character(df_reformatted$age_group)),1,0) # select information from character 41 (85 and over category) or from character 38 df_reformatted &lt;- mutate(df_reformatted,group = ifelse(g==0,substr(as.character(age_group),38,500), substr(as.character(age_group),41,500))) # remove unnecessary columns df_reformatted &lt;- select(df_reformatted, -age_group, -g) We are now really getting somewhere, although in order for our data to be tidy each variable must have its own column. We also want, within each ethnic group, to aggregate the individual values within each age group. # pivot table and aggregate values df_clean &lt;- pivot_wider(df_reformatted,names_from=group,values_from=count,values_fn=sum) # rename columns # names are assigned based on index values, so make sure that the current columnnames match the # order of the new columnames otherwise our whole analysis will be wrong! names(df_clean) &lt;- c(&#39;msoa11cd&#39;,&#39;white_british&#39;,&#39;white_irish&#39;,&#39;white_traveller&#39;,&#39;white_other&#39;,&#39;mixed_white_black_caribbean&#39;, &#39;mixed_white_black_african&#39;,&#39;mixed_white_asian&#39;,&#39;mixed_other&#39;,&#39;asian_indian&#39;,&#39;asian_pakistani&#39;, &#39;asian_bangladeshi&#39;,&#39;asian_chinese&#39;,&#39;asian_other&#39;,&#39;black_african&#39;,&#39;black_caribbean&#39;,&#39;black_other&#39;, &#39;other_arab&#39;,&#39;other_other&#39;) # tidy data df_clean ## # A tibble: 7,201 x 19 ## msoa11cd white_british white_irish white_traveller white_other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E020025… 6775 17 27 51 ## 2 E020025… 4688 11 8 31 ## 3 E020025… 4609 20 3 55 ## 4 E020025… 4653 4 6 131 ## 5 E020025… 4369 13 2 38 ## 6 E020025… 7320 11 3 73 ## 7 E020025… 4274 16 2 96 ## 8 E020025… 4713 33 24 308 ## 9 E020025… 5344 17 42 111 ## 10 E020025… 4583 29 6 100 ## # … with 7,191 more rows, and 14 more variables: ## # mixed_white_black_caribbean &lt;dbl&gt;, mixed_white_black_african &lt;dbl&gt;, ## # mixed_white_asian &lt;dbl&gt;, mixed_other &lt;dbl&gt;, asian_indian &lt;dbl&gt;, ## # asian_pakistani &lt;dbl&gt;, asian_bangladeshi &lt;dbl&gt;, asian_chinese &lt;dbl&gt;, ## # asian_other &lt;dbl&gt;, black_african &lt;dbl&gt;, black_caribbean &lt;dbl&gt;, ## # black_other &lt;dbl&gt;, other_arab &lt;dbl&gt;, other_other &lt;dbl&gt; Finally. We now have a tidy dataset that we can work with! 5.2.4 Assignments Assignment 1 Since we went through all the trouble of cleaning and creating this file, the first task for Friday’s seminar is to finalise the analysis: use the cleaned data set to create a table that, for each MSOA, contains the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population. It could look something like this: msoa11cd white_british white_irish etc. E02002562 0.74 0.03 … E02002560 0.32 0.32 … Tips First think what you what steps you would need to take in order to get the group proportions. Write them down on a piece of paper if you like. Once you have identified the steps, then start coding. Conduct sanity checks. Every time you have written a line of code, check the results to see if the code did indeed give the result that you expected to get. Google is your friend. Do not be afraid to search for specific solutions and suggestions, chances are that there have been other people who have faces similar problems and posted their questions on stackoverflow. Assignment 2 Further to calculating the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population, we also want to make a choropleth map at district level of the UK population that is older than 60 as a proportion of the total population. For this analysis we have available one dataset with the administrative boundaries of the UK 2020 Local Authorithy Districts administrative geography and we have a csv file that holds population estimates for the UK in 2019. Use everything you have learned over the past weeks to produces this map. Some tips: Tips Inspect both the shapefile and the csv file to get an idea of how your data look like. Use any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.). The csv file does contain a mix of administrative geographies, and you will need to do some data cleaning by filtering out Country, County, Metropolitan County, and Region before you link the data to the shapefile. You are in charge of deciding what software you want to use to visualise the data (ArcGIS, R, QGIS, etc.). You now have to make your own decisions on how to go about this problem. Although this practical has so far covered some of the functions and strategies you might need, the data cleaning and data preparation process is not the same. Map submission E-mail your final map as a PDF or JPG to j.t.vandijk [at] ucl.ac.uk before Friday morning 09h00 UK time (GMT+0). Also, ‘bring’ your final map with you to Friday’s seminar. File download File Type Link Local Authorithy District boundaries 2020 shp Download Mid-Year Population Estimates 2019 csv Download 5.3 Exploratory spatial data analysis in R Exploratory Data Analysis got introduced in the the late 1970s by the American mathematician John Tukey. Tukey thought that in much data analysis there was much emphasis on statistical hypothesis testing and very little development of new hypotheses. He therefore made a distinction between confirmatory data analysis and exploratory data analysis (EDA). EDA is a collection of descriptive techniques used to detect patterns, identify outliers, and form hypotheses from the data. An EDA typically involves descriptive statistics and data visualisation. Exploratory techniques generally stay ‘close’ to the original data. Exploratory spatial data analysis (ESDA) is the extension of exploratory data analysis (EDA) to the spatial realm. 5.3.1 Video: Exploratory spatial data analysis [Lecture slides] [Watch on MS stream] 5.3.2 Example: Exploring our dataset After successfully creating the maps that we were requested to make, our bosses got so happy that they directly gave us a new assignment. This time they want us to look at bicycle theft in London, specifically they want to know which factors contribute to bicycle theft in London’s central boroughs of Southwark, Lambeth, Camden, Westminster, Tower Hamlets, City of London, and Hackney. For this assignment, we have access to two open datasets. The first one contains the English Index of Multiple Deprivation (IMD) for 2019. The English indices of deprivation measure relative deprivation in small areas in England called Lower-layer Super Output Areas (LSOAs) The index of multiple deprivation is the most widely used of these indices. The second dataset contains some of the input data that have been used to create the Access to Healthy Assets and Hazards (AHAH). AHAH is a multi-dimensional index developed by the Consumer Data Research Centre (CDRC) for Great Britain measuring how ‘healthy’ neighbourhoods are. We also have access to the LSOAs for London. Further to these two open datasets, we have a shapefile with all of London’s LSOAs as well as a csv file that contains the coordinates of all tube stations. File download File Type Link Lower-layer Super Output Areas London 2011 shp Download Bicycle theft data London 2019 csv Download Index of Multiple Deprivation London 2019 csv Download Access to Healthy Assets and Hazards (AHAH) London csv Download Locations of tube stations in London csv Download Download the individual files to your own computer and again make sure your data directory is set up correctly and the data are unzipped. Then load the libraries and the data into R. # load libraries library(tidyverse) library(sf) library(tmap) library(Rfast) # load spatial data lsoa &lt;- st_read(&#39;raw/boundaries/london_lsoa_2011/lsoa_london_2011_sel.shp&#39;) ## Reading layer `lsoa_london_2011_sel&#39; from data source `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/raw/boundaries/london_lsoa_2011/lsoa_london_2011_sel.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1020 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 523847.7 ymin: 169650 xmax: 539533.3 ymax: 188327.4 ## CRS: 27700 names(lsoa) &lt;- c(&#39;lsoa11cd&#39;,&#39;geometry&#39;) # load data crime &lt;- read_csv(&#39;raw/crime/2019_london_bicycle_theft.csv&#39;) imd &lt;- read_csv(&#39;raw/index/imd_ew_2019.csv&#39;) ahah &lt;- read_csv(&#39;raw/index/ahah_input_gb_2016.csv&#39;) tube &lt;- read_csv(&#39;raw/locations/london_underground_stations.csv&#39;) # inspect crime data head(crime) ## # A tibble: 6 x 4 ## month long lat type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-03 -0.0981 51.5 Bicycle theft ## 2 2019-03 -0.0981 51.5 Bicycle theft ## 3 2019-03 -0.0976 51.5 Bicycle theft ## 4 2019-03 -0.0930 51.5 Bicycle theft ## 5 2019-03 -0.0941 51.5 Bicycle theft ## 6 2019-03 -0.0930 51.5 Bicycle theft # inspect imd data head(imd) ## # A tibble: 6 x 3 ## lsoa11cd imd_rank imd_dec ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E01000001 29 199 9 ## 2 E01000002 30 379 10 ## 3 E01000003 14 915 5 ## 4 E01000005 8 678 3 ## 5 E01000006 14 486 5 ## 6 E01000007 7 256 3 # inspect ahah data head(ahah) ## # A tibble: 6 x 4 ## lsoa11cd gamb_dist pubs_dist off_dist ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E01000001 0.390 0.181 0.381 ## 2 E01000002 0.417 0.221 0.680 ## 3 E01000003 0.398 0.192 0.422 ## 4 E01000005 0.280 0.228 0.617 ## 5 E01000006 0.599 0.611 0.648 ## 6 E01000007 0.377 0.621 0.734 # inspect tube location data head(tube) ## # A tibble: 6 x 3 ## station lat long ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abbey Road 51.5 0.00372 ## 2 Abbey Wood 51.5 0.120 ## 3 Acton Central 51.5 -0.263 ## 4 Acton Main Line 51.5 -0.268 ## 5 Acton Town 51.5 -0.280 ## 6 Addington Village 51.4 -0.0327 Looking at the data you will notice that both the bike theft data and the tube location data exist of longitudes and latitudes. The IMD dataset has an lsoa11cd column, an IMD rank column, and an IMD decile column. The AHAH dataset has an lsoa11cd column, a distance to the nearest gamling outlet column, a distance to the nearest pub column, and a distance to the nearest off license column. The technical report of the AHAH index suggests these distances represent the mean distance (kilometres) by car of all postcodes within each LSOA to the nearest outlet. Good to know! Let’s start by linking the IMD dataset and the AHAH dataset to the spatial dataset. Since we are using sf for our spatial data, we can very easily join them together using the common variable in these datasets: lsoa11cd. # join imd data lsoa_join &lt;- left_join(lsoa,imd,by=c(&#39;lsoa11cd&#39;)) # join ahah data lsoa_join &lt;- left_join(lsoa_join,ahah,by=c(&#39;lsoa11cd&#39;)) # inspect the result head(lsoa_join) ## Simple feature collection with 6 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 525360.9 ymin: 177857 xmax: 538921.5 ymax: 183191.6 ## CRS: 27700 ## lsoa11cd imd_rank imd_dec gamb_dist pubs_dist off_dist ## 1 E01004665 5 376 2 0.2504762 0.2019048 0.4619048 ## 2 E01003111 5 781 2 0.4455556 0.2648148 0.4603704 ## 3 E01004276 30 144 10 0.8789286 0.1492857 1.8007143 ## 4 E01004706 27 823 9 0.6737500 0.7354167 0.3750000 ## geometry ## 1 MULTIPOLYGON (((529040.9 17... ## 2 MULTIPOLYGON (((531023.1 17... ## 3 MULTIPOLYGON (((537563.7 17... ## 4 MULTIPOLYGON (((525406.7 18... ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2 rows ] This is all looking fine. Let’s now turn our turn our bike theft location dataset into a spatial dataset using the longitude and latitude values and get total bike theft counts within each of our available LSOAs. # filter out points without a longitude and/or latitude value crime &lt;- filter(crime,!is.na(long) | !is.na(lat)) crime_points &lt;- st_as_sf(crime, coords=c(&#39;long&#39;,&#39;lat&#39;),crs=4326) # project into british national grid (epsg 27700) crime_points &lt;- st_transform(crime_points,27700) # count the number of points intersecting within each of London&#39;s LSOAs lsoa_join$n_crime &lt;- lengths(st_intersects(lsoa_join,crime_points)) # inspect the result tm_shape(lsoa_join) + tm_fill(col=&#39;n_crime&#39;) This all looks good. We can see, however, that the the number of times bike theft was reported are highly skewed by Central London. It is now a good idea to explore the bike theft data a bit further to see how the data are distributed non-spatially by plotting them. # summary summary(lsoa_join$n_crime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 4.000 8.993 10.000 260.000 # histogram ggplot(lsoa_join, aes(x=n_crime)) + geom_histogram(binwidth=.5, colour=&#39;black&#39;, fill=&#39;white&#39;) # boxplot ggplot(lsoa_join, aes(x=n_crime))+ geom_boxplot() # as the data is very skewed, let&#39;s cube transform the bike theft data # and inspect the transformed data again lsoa_join$n_crime_cube &lt;- abs(lsoa_join$n_crime)^(1/3) # summary summary(lsoa_join$n_crime_cube) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.260 1.587 1.666 2.154 6.383 # histogram ggplot(lsoa_join, aes(x=n_crime_cube)) + geom_histogram(binwidth=.5, colour=&#39;black&#39;, fill=&#39;white&#39;) # boxplot ggplot(lsoa_join, aes(x=n_crime_cube))+ geom_boxplot() # inspect the result tm_shape(lsoa_join) + tm_fill(col=&#39;n_crime_cube&#39;) This does look much better, although there are clearly several outliers in our data set with some areas that have a higher number of cases of bike theft. Let’s now also inspect our input data before we start to see whether there is a relationship between our independent variables and our dependent variable (i.e. the number of cases of bike theft). # summary imd deciles summary(lsoa_join[,4]) ## gamb_dist geometry ## Min. :0.1230 MULTIPOLYGON :1020 ## 1st Qu.:0.3362 epsg:27700 : 0 ## Median :0.4603 +proj=tmer...: 0 ## Mean :0.5189 ## 3rd Qu.:0.6268 ## Max. :3.8188 # boxplot imd deciles ggplot(lsoa_join, aes(x=imd_dec))+ geom_boxplot() # summary ahah summary(lsoa_join[,5:7]) ## pubs_dist off_dist geometry ## Min. :0.06364 Min. :0.1574 MULTIPOLYGON :1020 ## 1st Qu.:0.22088 1st Qu.:0.4110 epsg:27700 : 0 ## Median :0.32001 Median :0.5723 +proj=tmer...: 0 ## Mean :0.37230 Mean :0.6586 ## 3rd Qu.:0.47544 3rd Qu.:0.8262 ## Max. :1.59219 Max. :2.0869 # boplot ahah pub distance ggplot(lsoa_join, aes(x=gamb_dist))+ geom_boxplot() # boplot ahah off license distance ggplot(lsoa_join, aes(x=off_dist))+ geom_boxplot() # gambling distance vs bike theft ggplot(lsoa_join,aes(x=gamb_dist, y=n_crime_cube)) + geom_point() The input data look good. There are no real strange and unexpected outliers or missing values. At the same time there is enough heterogeneity in our data. This is important because if the input data have very similar values for al LSOAs they would not be very useful in trying to explain the differences in bike theft. However, we have not touched our tube locations dataset just yet. Perhaps bicycle theft happens more around tube stations? Let’s do some quick spatial analysis and add for each LSOA the distance to the nearest tube station to our dataset. Please note: the following operation requires some computing power as for all of our selected LSOAs in London we are going to calculate the euclidean distance from its centroid to all tube stations and create a distance matrix. In case the calculation takes too much time on your computer, you can download the distance matrix instead and use dist_matrix &lt;- data.matrix(read_csv('path_to_file')) to continue with the practical. # create a point dataset from the tube csv file tube_points &lt;- st_as_sf(tube, coords=c(&#39;long&#39;,&#39;lat&#39;),crs=4326) # project into british national grid (epsg 27700) tube_points &lt;- st_transform(tube_points,27700) # create a distance matrix dist_matrix &lt;- st_distance(st_centroid(lsoa_join),tube_points) ## Warning in st_centroid.sf(lsoa_join): st_centroid assumes attributes are ## constant over geometries of x # assign the distance of the closes tube station lsoa_join$tube_dist &lt;- rowMins(dist_matrix,value=TRUE) Great. So, now we can continue. Let’s start by seeing if there is any visual relationship between our input (independent) variables and our output (dependent) variable by plotting them against each other. # imd decile vs bike theft ggplot(lsoa_join,aes(x=imd_dec, y=n_crime_cube)) + geom_point() # pub distance vs bike theft ggplot(lsoa_join,aes(x=pubs_dist, y=n_crime_cube)) + geom_point() # gambling outlets distance vs bike theft ggplot(lsoa_join,aes(x=gamb_dist, y=n_crime_cube)) + geom_point() # off license distance vs bike theft ggplot(lsoa_join,aes(x=off_dist, y=n_crime_cube)) + geom_point() # tube station distance vs bike theft ggplot(lsoa_join,aes(x=tube_dist, y=n_crime_cube)) + geom_point() The IMD deciles, which are in fact categorical data, do not show any obvious pattern, however, the distance to the various outlets do show some pattern. Let’s correlate. # imd decile vs bike theft cor(lsoa_join$imd_dec,lsoa_join$n_crime_cube) ## [1] 0.05715025 # gambling outlet distance vs bike theft cor(lsoa_join$gamb_dist,lsoa_join$n_crime_cube) ## [1] -0.2768482 # pub distance vs bike theft cor(lsoa_join$pubs_dist,lsoa_join$n_crime_cube) ## [1] -0.4175234 # off license distance vs bike theft cor(lsoa_join$off_dist,lsoa_join$n_crime_cube) ## [1] -0.1750449 # tube location distance vs bike theft cor(lsoa_join$off_dist,lsoa_join$n_crime_cube) ## [1] -0.1750449 So, looking at these correlations there seems to be some relationship between our explanatory (independent) variables and our dependent variables. It is time to form a hypothesis: LSOAs that are in a less deprived IMD decile, LSOAs with a lower mean distance to one of our outlets of interest, and LSOAs that are closer to a tube station, experience more bike theft. The null hypothesis then becomes: this relationship does not exist. Let’s see if we can test this hypothesis using an Ordinary Least Squares (OLS) regression model. In R, models are typically fitted by calling a model-fitting function, in our case lm() (linear model). The lm() function returns a fitted model which we can inspect by calling summary() on the object. # linear model crime_model &lt;- lm(n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + off_dist + tube_dist, data=lsoa_join) # get the results summary(crime_model) ## ## Call: ## lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1400 -0.4691 -0.0322 0.4464 4.0813 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4663553 0.0894498 27.573 &lt; 2e-16 *** ## imd_dec 0.0257697 0.0126189 2.042 0.0414 * ## gamb_dist -0.2611631 0.1194108 -2.187 0.0290 * ## pubs_dist -1.3719555 0.1489382 -9.212 &lt; 2e-16 *** ## off_dist -0.0100393 0.0831764 -0.121 0.9040 ## tube_dist -0.0005161 0.0001010 -5.109 3.86e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8039 on 1014 degrees of freedom ## Multiple R-squared: 0.2034, Adjusted R-squared: 0.1995 ## F-statistic: 51.79 on 5 and 1014 DF, p-value: &lt; 2.2e-16 In running a regression model, we are effectively trying to test our null hypothesis. If our null hypothesis is true then we expect our coefficients to equal to 0. Right now our null hypothesis is that there is no relationship between bike theft and our input variables. Without going in much further detail, as this will be in-depth covered in your CASA005 module, we can see that the null hypothesis is disproven and, in fact, there seems to be a negative relationship between our distance-related input variables and our dependent variable. Four out of our five variables are significant at a 0.05 level of significance. The R-squared, which is a measure of the goodness-of-fit of the model, suggest that 19.95 per cent of the variation in our dependent variable can be explained by our independent variables. This means that a relationship is present, but it is not very strong! Nonetheless, the model suggest that LSOAs with a higher IMD score, a higher accessibility to pubs, a higher accessibility to gambling outlets, and a higher accessibility to a tube location, experience more bike theft. Consider that when parking your bike somewhere overnight. Important Normally, you would also check the underlying assumptions of the linear model (linearity, homoscedasticity, independence, and normality), however, this will be covered more in-depth in your CASA005 module and for our current purposes we are going to be a little naughty and accept the results as is and assume that all our underlying assumptions are satisfied (even though we are actually violating some of these underlying assumptions!). If you do want to get into some more detail on regression analysis right here and now, the following video explains the process of fitting a line to your data very clearly. While you at it, do have a look at some of the other videos on the StatsQuestwith Josh Stammer Youtube Channel. 5.4 Spatial heterogeneity We now have established a relationship between IMD deciles, access to gambling outlets, access to pubs, access to off licenses, access to the nearest tube stations, and the occurrence of bike theft. However, our data are very spatial and we did not incorporate this spatial distribution into our model. So if we want to develop a regression model for bike theft in London we may have to recognise this spatial component. On top of this, a regression model assumes independence of observations: what happens in LSOA001 is not related to what happens in LSOA002 or any other LSOA. However, we know from last weeks content that this is not always the case because of spatial autocorrelation. 5.4.1 Video: Spatial heterogeneity [Lecture slides] [Watch on MS stream] 5.4.2 Example: Accounting for spatial heterogeneity Let’s start by exploring wether autocorrelation is an issue in our current analysis. Make sure the following libraries are loaded: # load libraries library(sf) library(tmap) library(sp) library(spdep) library(spatialreg) Now we are ready to go again, we are going to start by having a closer look at the residuals of our model. In other words, the residuals represent the distance between the observed values of the dependent variable and the predicted values of our dependent variables in our linear model. Residuals should be randomly distributed over space, otherwise we may be dealing with spatial autocorrelation. Strange words coming from a geographer, but right now we do not want to see any spatial patterning. # extract the residuals from the model and assign to our spatial dataset lsoa_join$residuals &lt;- residuals(crime_model) # extract the predicted values from the model and assign to our spatial dataset lsoa_join$predicted &lt;- fitted(crime_model) # example observed, residual, predicted of first LSOA in our data lsoa_join[1,c(1,9,11,12)] ## Simple feature collection with 1 feature and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 528983.6 ymin: 177857 xmax: 529243.5 ymax: 178173.5 ## CRS: 27700 ## lsoa11cd n_crime_cube residuals predicted geometry ## 1 E01004665 1.587401 -0.2261528 1.813554 MULTIPOLYGON (((529040.9 17... Now we have our residuals assigned to their spatial units, we can plot the residuals in number of standard deviations from the mean. # standardise lsoa_join$sd_mean &lt;- (lsoa_join$predicted - mean(lsoa_join$predicted)) / sd(lsoa_join$predicted) # inspect the result breaks &lt;- c(-14,-3,-2,-1,1,2,3,14) tm_shape(lsoa_join) + tm_fill(&#39;sd_mean&#39;, style=&#39;fixed&#39;, breaks=breaks, palette=&#39;-RdBu&#39;) + tm_borders(alpha = 0.1) You can see the spatial patterning of areas of over-prediction in the centre of Greater London with areas of under-prediction concentrating on the edges of the study area. We could be dealing with some spatial autocorrelation, so we need to test for it again. We will be using a Global Moran’s I for this. Remember with a global Moran’s I we can test how ‘random’ the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a ‘random’ distribution, and 1 is a ‘non-random’ distribution of clearly defined clusters. # force sf to sp lsoa_join_sp &lt;- as_Spatial(lsoa_join, IDs=lsoa_join$lsoa11cd) # create a list of neighbours using the &#39;queen&#39; criteria lsoa_nb &lt;- poly2nb(lsoa_join_sp, row.names=lsoa_join_sp$lsoa11cd, queen=TRUE) # check neighbours summary(lsoa_nb) ## Neighbour list object: ## Number of regions: 1020 ## Number of nonzero links: 5800 ## Percentage nonzero weights: 0.5574779 ## Average number of links: 5.686275 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 11 12 13 ## 14 64 152 259 244 166 66 39 10 2 2 2 ## 14 least connected regions: ## E01001823 E01004720 E01004677 E01003143 E01033596 E01033710 E01004692 E01004218 E01003030 E01003142 E01001813 E01032775 E01004739 E01004732 with 2 links ## 2 most connected regions: ## E01000946 E01000953 with 13 links # generate the row standardised spatial weight matrix wm &lt;- nb2mat(lsoa_nb, style=&#39;B&#39;) rwm &lt;- nb2listw(lsoa_nb, style=&#39;W&#39;) Now we can execute a Moran’s test for the regression residuals. We use lm.morantest() for this. This is important because this function takes into account that the input data are residuals, something which the ‘normal’ Moran’s I test does not do. # Moran&#39;s I lm.morantest(crime_model, rwm, alternative=&#39;two.sided&#39;) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## Moran I statistic standard deviate = 18.427, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## Observed Moran I Expectation Variance ## 0.3439158982 -0.0035656704 0.0003556032 Not surprisingly, the test is significant: we are indeed dealing with spatial autocorrelation. This means that our OLS regression may not be the best way to respresent our data as currently our regression line will per definition under-predict or over-predict in areas that are close to one another. There are two ways of taking this spatial dependence into account: by means of a spatial error model or by means of a spatially lagged model. These methods are quite different as they treat spatial autocorrelation in different ways. In the first case, the observed spatial dependence is not considered as an actual spatial process but is an effect of spatial clustering. In the second case, spatial dependence is considered as an actual spatial process and ‘space’ should be incorporated as an explanation in the model. How do we now decide which model to use? Well, first, as with any model, the relationship between independent and dependent variables must make sense. So, for instance, if you think that the spatial dependence is just an artefact of the data distribution rather than a truly spatial process, you should go with a spatial error model. Vice versa, if you think that the spatial dependence is the result of a spatial process, you should consider the spatially lagged model. In less clear cut cases, we can also get a little help by making use of the Lagrange Multiplier test statistics. We can these run these tests as follows: # lagrange mulitplier lm.LMtests(crime_model, rwm, test = c(&#39;LMerr&#39;,&#39;LMlag&#39;,&#39;RLMerr&#39;,&#39;RLMlag&#39;)) ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## LMerr = 327.63, df = 1, p-value &lt; 2.2e-16 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## LMlag = 355.85, df = 1, p-value &lt; 2.2e-16 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## RLMerr = 1.1286, df = 1, p-value = 0.2881 ## ## ## Lagrange multiplier diagnostics for spatial dependence ## ## data: ## model: lm(formula = n_crime_cube ~ imd_dec + gamb_dist + pubs_dist + ## off_dist + tube_dist, data = lsoa_join) ## weights: rwm ## ## RLMlag = 29.348, df = 1, p-value = 6.049e-08 How dow we use this information? The output of the test is split into two groups: the standard tests (LMerr and LMlag) and the robust versions of these tests (RLMerr and RMlag). The robust version should be used only if both of the standard tests are significant (i.e. below the .05 level). If only one of the standard tests comes back signficant, but the other one is not, then it becomes very easy to decide between the spatial error and the spatially lagged model: choose the one that is significant! In case both the standard tests are significant we would need to look at the outcomes of the robust tests and make a decision. In such a situation, the best suggestion is to compare the p-values of both tests and select the model that is ‘more significant’ than the other. In our case the situation is very clear and we should go with a spatially lagged model rather than a spatial error model. Let’s do that using the lagsarlm() function. Also, let’s remove the distance to the neareast off license as that variable was not significant to begin with. # execute the spatially lagged model # this can take a little bit of time, be patient! crime_model_lag &lt;- lagsarlm(n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + tube_dist, data=lsoa_join, listw=rwm) # let&#39;s inspet the results summary(crime_model_lag) ## ## Call:lagsarlm(formula = n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + ## tube_dist, data = lsoa_join, listw = rwm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.202012 -0.393879 0.016251 0.403836 3.178825 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.12440760 0.09842553 11.4239 &lt; 2.2e-16 ## imd_dec 0.03119665 0.01078435 2.8928 0.0038186 ## pubs_dist -0.77701415 0.12727377 -6.1051 1.028e-09 ## gamb_dist -0.16495609 0.09942310 -1.6591 0.0970891 ## tube_dist -0.00031330 0.00008769 -3.5729 0.0003531 ## ## Rho: 0.54502, LR test value: 244.87, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.033565 ## z-value: 16.238, p-value: &lt; 2.22e-16 ## Wald statistic: 263.67, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1099.293 for lag model ## ML residual variance (sigma squared): 0.47432, (sigma: 0.68871) ## Number of observations: 1020 ## Number of parameters estimated: 7 ## AIC: 2212.6, (AIC for lm: 2455.5) ## LM test for residual autocorrelation ## test value: 8.1292, p-value: 0.0043559 To interpret these results, we have to look at the Akaike Information Criterion (AIC). The AIC of our spatially lagged model is 2212.6, whereas the model without taking spatial dependence into account has an AIC of 2455.5 This means that the spatially lagged model is a better fit to the data then the standard OLS model. We also have a new term Rho, which is the spatial autoregressive parameter. In our case it is both positive and statistically significant, suggesting that when bike theft in the neighbours of area Xi increase, bike theft will also be higher in area Xi. At the same time, the IMD decile, the distance to the nearest pub, and the distance to the nearest tube stations are all still significant. Next time you know where not to park your bike. Be careful not to compare the regression coefficients of the spatially lagged model to the regression coefficients of the original OLS model because their meaning now has changed. Whilst in the OLS model the coefficient of any of the independent variables measures the overall impact of these variables, this is not the case in the spatially lagged model: by including the spatial lag, the effect that X has on Y depends also (indirectly) on the effect that X has on Y in its neighbours. Although we probably should not, let’s also run a spatial error model. Just for the sake of running it and see how it looks. # execute the spatially lagged model # this can take a little bit of time, be patient! crime_model_err &lt;- errorsarlm(n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + tube_dist, data=lsoa_join, listw=rwm) # let&#39;s inspect the results summary(crime_model_err) ## ## Call:errorsarlm(formula = n_crime_cube ~ imd_dec + pubs_dist + gamb_dist + ## tube_dist, data = lsoa_join, listw = rwm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1086e+00 -3.8717e-01 5.2999e-05 4.0082e-01 3.1615e+00 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.01465810 0.10346058 19.4727 &lt; 2.2e-16 ## imd_dec 0.06067673 0.01345733 4.5088 6.519e-06 ## pubs_dist -0.96803178 0.15645501 -6.1873 6.121e-10 ## gamb_dist -0.14677894 0.11902951 -1.2331 0.2175 ## tube_dist -0.00049008 0.00012571 -3.8986 9.675e-05 ## ## Lambda: 0.58216, LR test value: 241.97, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.033829 ## z-value: 17.209, p-value: &lt; 2.22e-16 ## Wald statistic: 296.16, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1100.746 for error model ## ML residual variance (sigma squared): 0.47069, (sigma: 0.68607) ## Number of observations: 1020 ## Number of parameters estimated: 7 ## AIC: 2215.5, (AIC for lm: 2455.5) If we have good reasons to believe that we need to run a spatial error model rather than an OLS regression model, we actually notice that the model fit improves: the AIC of the spatial error model is 2215.5, whilst the AIC of the OLS model is still 2455.5 Keep in mind: whereas the spatially lagged model takes spatial dependence into account, the spatial error model simply tries to remove the influence of space and we in fact estimate a model that relaxes one of the standard regression model assumptions that the errors need to be independent. This time you could actually compare the regression coefficients from the spatial error model with those from the OLS model! Geographically Weighted Regression As mentioned in the short lecture video, another option to take ‘space’ into account is by executing a Geographically Weighted Regression (GWR). However, as this week’s content is already getting very long as well as that GWR will be covered in your CASA005 module, we will not cover this now. Do have a look at the article by Shabrian et al. 2020 that is part of this week’s essential reading to see GWR in action. 5.5 Take home message Exploratory (Spatial) Data Analysis is at the core of any analysis. After you cleaned and prepared your data, you first want to inspect your data through simple and visual methods to get acquainted with the data. Then you can start looking for relationships, and, if any relationships are found you are going to want to ask the question to what extent these relationships are real or to what extent other (spatial) processes, such as spatial dependence, play a role in shaping these relationships. That is it for this week! Next week we will move to point pattern analysis, but that is next week so for now we can celebrate the successful completion of this week’s material. 5.6 Attributions This week’s content and practical uses content and inspiration from: Wickham, H., Grolemund, G. 2017. R for Data Science. https://r4ds.had.co.nz/index.html Medina, J, Solymosi, R. 2019. Crime Mapping in R. https://maczokni.github.io/crimemapping_textbook_bookdown/ 5.7 Feedback Please take a moment to give us some feedback on this week’s content. "],
["point-pattern-analysis.html", "6 Point pattern analysis 6.1 Feedback", " 6 Point pattern analysis In our previous practicals, we have aggregated our event data into areal units, primarily using administrative geographies, to enable its easy comparison with other datasets provided at the same spatial scale, such as the census data used in the previous week, as well as to conduct spatial autocorrelation tests. However, when locations are precisely known, spatial point data can be used with a variety of spatial analytic techniques that go beyond the methods typically applied to areal data. The set of methods unique to point data are often referred to as point pattern analysis and geostatistics. This week, we focus on point pattern analysis, whilst next week we will look at geostatistics. Within point pattern analysis, we look to detect patterns across a set of locations, including measuring density, dispersion and homogeneity in our point structures. We will look first at basic forms of point pattern analysis, including mean centers and the standard deviational ellipse, and then more powerful analysis methods, including kernel density estimation and Ripley’s K function. These latter functions help determine and/or show whether points have a random, dispersed or clustered distribution pattern at a certain scale. This week’s content will be made available on 16/10. 6.1 Feedback Please take a moment to give us some feedback on this week’s content. "],
["geostatistics.html", "7 Geostatistics 7.1 Feedback", " 7 Geostatistics Following on from Week 3 and in conjunction with this week in your CASA005 module, we will also be focusing on rasters, exploring further the creation and application of rasters from vector datasets using geostatistics. During the lecture, we will provide a more in-depth and detailed explanation behind the geostatistics methods of interpolation, looking at various deterministic and geostatistical techniques. We then introduce methods to using vector datasets with rasters, utilizing a GIS method known as zonal statistics. This week’s content will be made available on 23/11. 7.1 Feedback Please take a moment to give us some feedback on this week’s content. "],
>>>>>>> master
["cluster-analysis.html", "8 Cluster analysis 8.1 Feedback", " 8 Cluster analysis DBScan is a density-based clustering algorithm that is commonly used in data mining and machine learning. Across a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. Whilst the algorithm is nearly 25 years old, it still is incredibly relevant to many applications within data science today, including within spatial analysis. Novel use of the algorithm has seen researchers detect and delineate urban areas from building footprint data. This use presents significant opportunities for those working to provide spatial data within countries where until recently, significant reference data did not exist. To understand this potential application, we look at the issues of data scarcity and sparsity within LICs and MICs, which do not currently have extensive mapping data or national mapping agencies to provide this data. We look at current initiatives to address this data dearth, with a specific focus on those working on the automated extraction of building footprints from satellite imagery using machine learning. Within the practical component, we look to deploy a simple version of DBScan on building data subset from the Tanzanian building dataset created by Microsoft. We attempt to detect and delineate our own urban areas within the dataset and discuss the advantages of novel changes made by those working with the algorithm for this specific purpose. This week’s content will be made available on 30/10. 8.1 Feedback Please take a moment to give us some feedback on this week’s content. "],
["bayesian-modelling.html", "9 Bayesian modelling", " 9 Bayesian modelling Bayesian methodology is an approach to statistical inference that has existed for a long time. However, its applications have been limited until recent advancements in computation and simulation methods has made its deployment and use within big datasets viable. The concept underlying Bayesian spatial modeling is Bayes’ theorem, which considers both the distributions of the data and the unknown coefficient estimates (LeSage and Pace, 2009). Bayesian spatial modeling embraces most, if not all, spatial models in the literature, such as the spatial lag model, the spatial error model, and geographically weighted regression discussed in Week 5. As a result, recent decades have experienced a rapid growth in the application of Bayesian spatial modeling to epidemiology, demography, and environmental health research, in addition to other geography-related disciplines. Bayesian modelling has been used to estimate population distributions, disease spread and air pollution. This week provides you with a general introduction to Bayesian modelling for geographical applications, with a focus on understanding the overall methodology as well as selection of variables for use within the model. The practical component looks at R-INLA package and how it can be used within health modelling. This week’s content will be made available on Moodle on 09/12. "],
["reproducible-research.html", "10 Reproducible research", " 10 Reproducible research In our final week of PSA, we will recap the main principles of spatial analysis that you have learnt over the last nine weeks. We will then look to tidy one of our projects from the previous week to use it within CodeOcean, an online platform that hosts code and data to create “reproducible runs” and “capsules” of research projects. The platform (alongside others!) enables you to create an anonymized version of your project, ready for its submission to journals as part of their increasing requirement to pass reproducibility and openness tests, e.g. the International Journal of Geographic Information Science. The live session of this week will be extended to present the two pieces of coursework you will assessed on as part of this module, as well as to answer any questions you may have on these assessments. This week’s content will be made available on Moodle on 09/12. "]
]
