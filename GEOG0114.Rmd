--- 
title: "GEOG0114: Principles of Spatial Analysis"
author: Justin van Dijk^[Department of Geography, https://www.mappingdutchman.com/] and Joanna Wilkin^[Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/joanna-wilkin]
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
link-citations: yes
github-repo: "jtvandijk/GEOG0114"
description: "GEOG114: Principles of Spatial Analysis handbook."
url: 'https\://jtvandijk.github.io/GEOG0114/'
---

# Welcome {-}

Placeholder


## Get in touch {-}
## Noticed a mistake in this resource? {-}

<!--chapter:end:index.Rmd-->


# Course information {-}

Placeholder


## Module structure {-}
## Weekly topics {-}
## Learning objectives {-}
## Reading list {-}
## Module assessment details {-}
## Useful additional resources {-}

<!--chapter:end:00-course-info.Rmd-->

# Spatial analysis for data science

The first week of PSA will introduce how geography and Geographical Information Science (GIScience) fits within the wider data science discipline and why there is a need to specialise in spatial and social analysis for data science. 

To provide this understanding, you will first work your way through a short document written specifically for this week to provide an extensive overview to why a geographical understanding to data science is critical to accurate and valid data analysis. 
Next, through a recorded lecture, you'll be given a short introduction to Geographical Information Systems (GIS) tools for spatial data science and an explanation to how these tools have changed over the last decade, including a shift from traditional GIS software towards programming-based analysis in research applications. 

We’ll then show you examples of different types of GIS software through a recorded tutorial. To gain a practical basic understanding of the differences across these software, including their ease of use, the recorded tutorial will show you the steps and processing to create a simple choropleth map of population and population density in London. This week plays a formative role in providing everyone with baseline from which to not only pursue this module, but the other technical modules on the MSc. 

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>

<!--chapter:end:01-week01.Rmd-->

# Geographical representation

The first part of this week will look at spatial representation data models, i.e. how we transform geographic features and phenomena into geographic data for use within GIS. We will then explore the role of scale and geography within spatial analysis and provide you with a critical understanding of how both can impact and effect the analysis of data, particularly when looking at ‘event’ type data, i.e. the occurrence of a specific phenomenon over space. 

We will then introduce you to the role and usage of administrative geographies and discuss how they are subject to the Modifiable Area Unit Problem as well as its the consequences, including ecological fallacies. We will then discuss methods to account for these issues, including population standardization, as well as highlight alternative methods for representing data beyond traditional choropleth maps. The interactive lecture will also introduce the role of projections and what considerations you should make when choosing the projection for your analysis; projections are further discussed in Week 3 of CASA0005. 

The practical component of the week puts these issues into practice with an analysis of crime data from the UK and its various administrative geographies, as well as voting patterns in the USA. The practical component also introduces the two types of data joining primarily used in spatial analysis: attribute and spatial joins.

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>


<!--chapter:end:02-week02.Rmd-->

# Spatial properties and relationships

Understanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This week, we look to provide you with a thorough introduction into using spatial operations (and the properties and relationships associated with them) through an introductory lecture, a research-based analysis (with demo and practical) and then a research task which we will look at during this week's seminar.

Within the lecture, we will highlight the different ways of conceptualizing key spatial properties, such as distance, and the impact this may have on measurement.  We then focus on their application within spatial operations, and how they can be used for the selection, subset and validation of data. We then look at the core terminology used to define spatial relationships and how they can be used to process datasets, using the operations previously mentioned. 

The practical utilises these concepts to investigate the accessibility of greenspace for schools across London. Recent research (Bijnens et al, 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. Here we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed.

For the practical, we provide an introduction to the research problem and outline how we devise a research methodology to be able to investigate our research questions. We then look at the required processing steps to create the final dataset that can be used in our analysis. This is followed by a short demo in which Jo will demonstrate the analysis visually in QGIS. We then ask you to recreate the analysis by creating a script in R-Studio (code provided) - this will allow you to replicate the analysis for other cities within the U.K, or even further afield if you can extract the same data. Finally, in preparation for this week's seminar, we ask you to watch a five minute video from a local news channel in Jo's hometown - ready to discuss as a possible research task in Friday's seminar.

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>


<!--chapter:end:03-week03.Rmd-->

# Geodemographics

## Introduction {#introduction_w09}
After our focus on issues of data scarcity and sparsity within low- and middle-income countries and automated extraction of building footprints from satellite imagery using machine learning, this week we looking at data rich environments and how we can use socio-demographic and socio-economic data to characterise neighbourhoods using geodemographics. We will do this by analysing and subsequently recreating a geodemographic classification known as the [Internet User Classification](https://data.cdrc.ac.uk/dataset/internet-user-classification).

> Geodemographics is the ‘analysis of people by where they live’ (Harris et al. 2005) and as such entails representing the individual and collective identities that are manifest in observable neighbourhood structure ([Longley 2012](https://www.tandfonline.com/doi/pdf/10.1080/13658816.2012.719623?needAccess=true))

This week is structured around **two** short lecture videos, **one** [assignments](#assignment_w09) that you need to do in preparation for Friday's seminar, and the practical material. As always, this [week's reading list](https://rl.talis.com/3/ucl/lists/3FB0C66A-F603-A402-A31C-28F2DA091AB6.html?lang=en-gb&login=1) is available on the UCL library reading list page for the course.

### Video: Overview {#overview_w09}
```{r 09-short-lecture-welcome, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('64e6a9e8-4a05-42ad-bfc2-a67008dd9c5f') %>% use_align('left')
```
[Lecture slides] [[Watch on MS stream]]()

## Geodemographics and the Internet User Classification
The [CDRC Internet User Classification](https://data.cdrc.ac.uk/dataset/internet-user-classification) (IUC) is a bespoke geodemographic classification that describes how people residing in different parts of Great Britain interact with the Internet. For every Lower Super Output Area (LSOA) in England and Wales and Data Zone (DZ) ([2011 Census Geographies](https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography)), the IUC provides aggregate population estimates of Internet use ([Singleton *et al.* 2020](https://www.sciencedirect.com/science/article/pii/S0198971519307963)).

### Video: Geodemographics and the Internet User Classification
```{r 09-geodemographics-iuc, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('7854a8dc-f24f-4a3d-908c-9a10adb4b085') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Geodemographics and the Internet User Classification
For the first part of this week's practical material, we will be looking at the Internet User Classification for Great Britain in more detail. Our first step is to download the IUC dataset:

- Open a web browser and go to the [data portal of the CDRC](https://data.cdrc.ac.uk).
- Register if you need to, or if you are already registered, make sure you are logged in.
- Search for **Internet User Classification**.
- Scroll down and choose the download option for the *IUC 2018 (CSV)*.
- Save the `iuc_gb_2018.csv` file in an appropriate folder.

```{r 09-iuc-download, echo=FALSE, fig.align='center', fig.cap='Download the GB IUC 2018.'}
knitr::include_graphics('images/week09/09_iuc_download.png')
```

We will start by inspecting the dataset in MS Excel, or any other spreadsheet software such as [Apache OpenOffice Calc ](https://www.openoffice.org/product/calc.html) or [Numbers](https://www.apple.com/uk/numbers/). Also, have a look at the [IUC 2018 User Guide](https://data.cdrc.ac.uk/system/files/iuc2018userguide.pdf) that provides the **pen portraits** of every cluster, including plots of cluster centres and a brief summary of the methodology.

<div class="note">
It is always a good idea to inspect your data prior to analysis to find out how your data look like. Of course, depending on the type of data, you can choose any tool you like to do this inspection ([ArcGIS](https://www.arcgis.com/index.html), [R](https://www.r-project.org/), [QGIS](https://qgis.org/en/site/), [Microsoft Excel](https://www.office.com/), etc.).
</div>

```{r 09-iuc-in-excel, echo=FALSE, fig.align='center', fig.cap='GB IUC 2018 in Excel.'}
knitr::include_graphics('images/week09/09_iuc_excel.png')
```

```{r 09-load-libraries-and data, warnings=FALSE, message=FALSE}
# load libraries
library(tidyverse)
library(tmap)

# load data
iuc <- read_csv('raw/index/iuc_gb_2018.csv')

# inspect
iuc

# inspect data types
str(iuc)
```

#### Questions {-}
1. Create a histogram to inspect the distribution of the data?
2. Looking at the histogram: is this what you expected? Why (not)?

Now are data are loaded we can move to acquiring our spatial data. As the IUC is at created at the level of the Lower super Layer Output Area Census geography, we need to download borders for this.

1. Go to [UK Data Service Census support portal](https://borders.ukdataservice.ac.uk/) and select **Boundary Data Selector**.
2. Set Country to *England*, Geography to *Statistical Building Block*, dates to *2011 and later*, and click **Find**.
3. Select *English Lower Layer Super Output Areas, 2011* and click **List Areas**.
4. Select *Liverpool* from the list and click **Extract Boundary Data**.
5. Wait until loaded and download the `BoundaryData.zip` file.
6. Unzip and save the file in the usual fashion.

<div class="note">
You could also have downloaded the shapefile with the data already joined to the LSOA boundaries directly from the CDRC data portal, but this is the national data set and is quite large (75MB) and as a consequence it may be a little slow to work with. Also, we will be looking at [Liverpool](https://en.wikipedia.org/wiki/Liverpool) today so we do not need all LSOAs in Great Britain.
</div>

Now we can prepare the IUC map by joining our `csv` file with the IUC classification to the shapefile.

```{r 09-load-those-spatial-data, warnings=FALSE, message=FALSE}
# load libraries
library(sf)
library(tmap)

# load spatial data
liverpool <- st_read('raw/boundaries/liverpool_lsoa_2011/england_lsoa_2011.shp')

# inspect
plot(liverpool$geometry)

# join data
liv_iuc <- merge(liverpool, iuc, by.x='code', by.y='LSOA11_CD')

# inspect
liv_iuc

# inspect
tmap_mode('view')
tm_shape(liv_iuc) +
  tm_polygons(col='GRP_LABEL') +
  tm_layout(legend.outside=TRUE)
```

Let's use the same colours as used on [CDRC mapmaker](https://mapmaker.cdrc.ac.uk/#/internet-user-classification?lon=-2.81187&lat=53.31045&zoom=9.58) by specifying the **hex** colour codes for each of our groups. Note the order of the colours is important. Colour for group 1 is first, group 2 second and so on. 

```{r 09-pretty-colours, warnings=FALSE, message=FALSE}
# define palette
iuc_colours <- c('#dd7cdc','#ea4d78','#d2d1ab','#f36d5a','#a5cfbc','#e4a5d0','#8470ff','#79cdcd','#808fee','#ffd39b')

# plot pretty
tm_shape(liv_iuc) +
  tm_polygons(col='GRP_LABEL', palette=iuc_colours) +
  tm_layout(legend.outside=TRUE)
```

#### Questions {-}
1. Looking at the interactive map of the 2018 IUC in Liverpool: are their any obvious patterns?
2. How does the 2018 IUC pattern in Liverpool relate to a 'typical' organisation of a city? Why? 

### Assignment {#assignment_w09}
Now we have these cluster classifications, how can we link them to people? Try using the **Mid-Year Population Estimates 2019** that you can download below to:

- calculate the total number of people in each cluster group **for Great Britain as a whole** (not just Liverpool!); and
- create a **pretty** data visualisation of the results.

No further instructions are provided and you will have to figure out on your own how to do this! We will discuss your choices and results in **Friday's seminar**, so make sure to 'bring' your results along.

#### File download {-}
| File                                                 | Type           | Link |
| :------                                              | :------        | :------ |
| Mid-Year Population Estimates 2019                   | `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/zip/mye_pop_2019.zip) |

## K-means clustering
Often, not always, a technique called **k-means clustering** is used in the creation of a geodemographic classification. K-means clustering aims to partition your input observations into a number of clusters (*k*) in which each observation will be assigned to the cluster with the neareast mean. A cluster refers to a collection of data points aggregated together because of certain similarities (i.e. standardised scores of your input data). You first define a target number *k* of clusters that you want and then the k-means algorithmn parititions the data by minimising the within-cluster variance.

### Video: K-means clustering
```{r 09-k-means-clustering-iuc, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
#embed_msstream('7854a8dc-f24f-4a3d-908c-9a10adb4b085') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: K-means clustering
For the second part of this week's practical material, we will be replicating the Internet User Classification for Great Britain. For this we will be using an MSOA-level input dataset containing various socio-demographic and socio-economic variables that you can download below. 

The dataset contains the following variables:

| Variable  | Definition |
|---|---|
| `msoa11cd` | MSOA Code |
| `age_total`, `age0to4pc`, `age5to14pc`, `age16to24pc`, `age25to44pc`, `age45to64pc`, `age75pluspc`| Percentage of people in various age groups |
| `nssec_total`, `1_higher_managerial`, `2_lower_managerial`, `3_intermediate_occupations`, `4_employers_small_org`, `5_lower_supervisory`, `6_semi_routine`, `7_routine`, `8_unemployed` | Percentage of people in selected operational categories and sub-categories classes drawn from the National Statistics Socio-economic Classification ([NS-SEC](https://www.ons.gov.uk/methodology/classificationsandstandards/otherclassifications/thenationalstatisticssocioeconomicclassificationnssecrebasedonsoc2010)) |
| `avg_dwn_speed`, `avb_superfast`, `no_decent_bband`, `bband_speed_under2mbs`, `bband_speed_under10mbs`, `bband_speed_over30mbs` | Measures of broadband use and availability |

Percentage of people in selected operational categories and sub-categories classes drawn from the National Statistics Socio-economic Classification ([NS-SEC](https://www.ons.gov.uk/methodology/classificationsandstandards/otherclassifications/thenationalstatisticssocioeconomicclassificationnssecrebasedonsoc2010))


#### File download {-}
| File                                                 | Type           | Link |
| :------                                              | :------        | :------ |
| MSOA variables IUC input                             | `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/zip/msoa_iuc_input.csv) |

```{r 09-load-date-for-those-mean-ks, warnings=FALSE, message=FALSE}
# load data
iuc_input <- read_csv('raw/index/msoa_iuc_input.csv')

# inspect
head(iuc_input)
```

Before running our **k-means** clustering algorithmn, we need to extract the data which we want to use; i.e. we need to remove all the columns with data that we do not want to include in the clustering process.

```{r 09-select-the-columns, warnings=FALSE, message=FALSE}
# column names
names(iuc_input)

# extract columns
cluster_data <- iuc_input[,c(3:8,10:17,18:20)]

# inspect
head(cluster_data)
```

We also need to rescale the data so everything is on a similar scale, as the average download speed data (e.g. `avg_dwn_speed`) is very different to the other data.

```{r 09-rescale-those-values, warnings=FALSE, message=FALSE}
# rescale
cluster.data <- scale(cluster_data) 

# inspect
head(cluster_data)
```

#### Questions {-}
1. What does the function `scale()` do exactly?

Now our data are all on the same scale, we will start by creating an elbow plot. The [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use`) is a visual aid that can help in determining the number of clusters in a dataset. Remember: this is important because with a **k-means** clustering you need to specify the numbers of clusters *a priori*! 

The elbow method can help as it plots the explained variation ('Within Sum of Squares') in your data as a function of the number of cluster. The idea is that you pick the number of clusters at the 'elbow' of the curve as this is the point in which the additional variation that would be explained by an additional cluster is decreasing. Effectively this means you are actually running the **k-means** clustering multiple times before running the actual **k-means** clustering algorithmn.

```{r 09-settings-and-options, warnings=FALSE, message=FALSE, echo=FALSE}
# settings
options(max.print = 30)
```
```{r 09-elbow-and-plots, warnings=FALSE, message=FALSE}
# create empty list to store the within sum of square values 
wss_values <- list()

# execute a k-means clustering for k=1, k=2, ..., k=15
for (i in 1:15) {
  wss_values[i] <- sum(kmeans(cluster_data,centers=i)$withinss)
}

# inspect
wss_values

# vector to dataframe
wss_values <- as.data.frame(wss_values)

# transpose
wss_values <- as.data.frame(t(wss_values))

# add cluster numbers
wss_values$cluster <- seq.int(nrow(wss_values))
names(wss_values) <- c('wss','cluster')

# plot
ggplot(data=wss_values, aes(x=cluster,y=wss)) +
  geom_point() +
  geom_path() + 
  scale_x_continuous(breaks=seq(1,15)) +
  xlab('number of clusters') +
  ylab('within sum of squares')
```

Based on the elbow plot, we can now choose the number of clusters. Based on this example above, we are going to use **7** clusters.

<div class="note">
The interpretation of an elbow plot can be quite subjective and often mutliple options would be justified. You would need to try the different options and see what output you get to determine the 'optimal' solution. However, at very least, the elbow plot does give you an idea of what would potentially be a adequate number of clusters. 
</div>

Now we have decided on the number of clusters (i.e. 7 clusters), we can now run our cluster analysis. We will be running this analysis 10 times because there is an element ofrandomness within the clustering, and we want to make sure we get the best clustering output.

```{r 09-run-that-cluster, warnings=FALSE, message=FALSE}
# create empty list to store the results of the clustering
clusters <- list()

# create empty variable to store fit
fit <- NA
  
# run the k-means 10 times
for (i in 1:10){
  
  # keep track of the runs
  print(paste0('starting run: ', i))
  
  # run the k-means clustering algorithmn to extract 7 clusters
  clust7 <- kmeans(x=cluster.data, centers=7, iter.max=1000000, nstart=1)
  
  # get the total within sum of squares for the run and 
  fit[i] <- clust7$tot.withinss
  
  # update the results of the clustering if the total within sum of squares for the run
  # is lower than any of the runs that have been executed so far 
  if (fit[i] < min(fit[1:(i-1)])){
    clusters <- clust7}
}

# inspect
clusters

# inspect
fit
```

#### Questions {-}
1. Do you understand what is happening in each step of this code, especially within the loop?
2. Do you understand what the different parameters in the `kmeans()` function mean and what they do?



## Take home message

## Attributions {#attributions_w09}
This week's content and practical uses content and inspiration from:

- Bearman, Nick. 2020. Internet User Classification (IUC) and K-Means Clustering. [Consumer Data Research Centre](https://www.cdrc.ac.uk/)
- Garbade, Michael. 2018. Understanding K-means clustering in machine learning. https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1

## Feedback {#feedback_w09}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).

<!--chapter:end:09-week09.Rmd-->

# Reproducible research

In our final week of PSA, we will recap the main principles of spatial analysis that you have learnt over the last nine weeks. We will then look to tidy one of our projects from the previous week to use it within CodeOcean, an online platform that hosts code and data to create "reproducible runs" and "capsules" of research projects. The platform (alongside others!) enables you to create an anonymized version of your project, ready for its submission to journals as part of their increasing requirement to pass reproducibility and openness tests, e.g. the International Journal of Geographic Information Science. The live session of this week will be extended to present the two pieces of coursework you will assessed on as part of this module, as well as to answer any questions you may have on these assessments.

<center>**This week's content will be made on 14/12.**</center>

<!--chapter:end:10-week10.Rmd-->

