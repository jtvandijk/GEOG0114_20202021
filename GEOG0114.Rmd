--- 
title: "GEOG0114: Principles of Spatial Analysis"
author: Justin van Dijk^[Department of Geography, https://www.mappingdutchman.com/] and Joanna Wilkin^[Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/joanna-wilkin]
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
link-citations: yes
github-repo: "jtvandijk/GEOG0114"
description: "GEOG114: Principles of Spatial Analysis handbook."
url: 'https\://jtvandijk.github.io/GEOG0114/'
---

# Welcome {-}

Placeholder


## Get in touch {-}
## Noticed a mistake in this resource? {-}

<!--chapter:end:index.Rmd-->


# Course information {-}

Placeholder


## Module structure {-}
## Weekly topics {-}
## Learning objectives {-}
## Reading list {-}
## Module assessment details {-}
## Useful additional resources {-}

<!--chapter:end:00-course-info.Rmd-->

# Spatial analysis for data science

The first week of PSA will introduce how geography and Geographical Information Science (GIScience) fits within the wider data science discipline and why there is a need to specialise in spatial and social analysis for data science. 

To provide this understanding, you will first work your way through a short document written specifically for this week to provide an extensive overview to why a geographical understanding to data science is critical to accurate and valid data analysis. 
Next, through a recorded lecture, you'll be given a short introduction to Geographical Information Systems (GIS) tools for spatial data science and an explanation to how these tools have changed over the last decade, including a shift from traditional GIS software towards programming-based analysis in research applications. 

We’ll then show you examples of different types of GIS software through a recorded tutorial. To gain a practical basic understanding of the differences across these software, including their ease of use, the recorded tutorial will show you the steps and processing to create a simple choropleth map of population and population density in London. This week plays a formative role in providing everyone with baseline from which to not only pursue this module, but the other technical modules on the MSc. 

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>

<!--chapter:end:01-week01.Rmd-->

# Geographical representation

The first part of this week will look at spatial representation data models, i.e. how we transform geographic features and phenomena into geographic data for use within GIS. We will then explore the role of scale and geography within spatial analysis and provide you with a critical understanding of how both can impact and effect the analysis of data, particularly when looking at ‘event’ type data, i.e. the occurrence of a specific phenomenon over space. 

We will then introduce you to the role and usage of administrative geographies and discuss how they are subject to the Modifiable Area Unit Problem as well as its the consequences, including ecological fallacies. We will then discuss methods to account for these issues, including population standardization, as well as highlight alternative methods for representing data beyond traditional choropleth maps. The interactive lecture will also introduce the role of projections and what considerations you should make when choosing the projection for your analysis; projections are further discussed in Week 3 of CASA0005. 

The practical component of the week puts these issues into practice with an analysis of crime data from the UK and its various administrative geographies, as well as voting patterns in the USA. The practical component also introduces the two types of data joining primarily used in spatial analysis: attribute and spatial joins.

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>


<!--chapter:end:02-week02.Rmd-->

# Spatial properties and relationships

Understanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This week, we look to provide you with a thorough introduction into using spatial operations (and the properties and relationships associated with them) through an introductory lecture, a research-based analysis (with demo and practical) and then a research task which we will look at during this week's seminar.

Within the lecture, we will highlight the different ways of conceptualizing key spatial properties, such as distance, and the impact this may have on measurement.  We then focus on their application within spatial operations, and how they can be used for the selection, subset and validation of data. We then look at the core terminology used to define spatial relationships and how they can be used to process datasets, using the operations previously mentioned. 

The practical utilises these concepts to investigate the accessibility of greenspace for schools across London. Recent research (Bijnens et al, 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. Here we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed.

For the practical, we provide an introduction to the research problem and outline how we devise a research methodology to be able to investigate our research questions. We then look at the required processing steps to create the final dataset that can be used in our analysis. This is followed by a short demo in which Jo will demonstrate the analysis visually in QGIS. We then ask you to recreate the analysis by creating a script in R-Studio (code provided) - this will allow you to replicate the analysis for other cities within the U.K, or even further afield if you can extract the same data. Finally, in preparation for this week's seminar, we ask you to watch a five minute video from a local news channel in Jo's hometown - ready to discuss as a possible research task in Friday's seminar.

<center>**This week's content is [available on Moodle](https://moodle.ucl.ac.uk/).**</center>


<!--chapter:end:03-week03.Rmd-->

# Exploratory spatial data analysis

## Introduction {#introduction_w05}
This week, we focus on the second of the two key properties of spatial data: spatial heterogeneity. With the underlying process (or processes) that govern a spatial variable likely to vary across space, a single global relationship for an entire region of study may not adequately model the process that governs outcomes in any given location of the study region. As a result, multiple methods have been developed to incorporate ‘space’ into traditional regression models, including spatial lag models, spatial error models, and Geographical Weighted Regression. 

This week provides the building blocks to conducting a statistical and spatial investigation into the relationships between spatial variables, looking at the concept of **Exploratory Spatial Data Analysis (ESDA)**. We then look at the three types of spatial regression models in turn to understand their methodology and potential advantages and limitations in accounting for space when modelling relationships. However, before we move on to ESDA, we will look into some aspects of data preparation, data cleaning, and creating a **tidy dataset**.

In the first part of this week's session we will start by creating a **tidy dataset** using some data from the [UK 2011 Census of Population](https://www.ons.gov.uk/census/2011census). In the second part of this week's session we will explore potential factors that may contribute to the crime rate in London. We conduct an ESDA of these variables, followed by statistical and spatial regression analysis to determine whether our variables contribute to crime in our area of study. 

This week is structured by **three** short lecture videos, **two** [assignments](#assignment_w05) that you need to do in preparation for Friday's seminar, and the practical material. As always, this [week's reading list](https://rl.talis.com/3/ucl/lists/3FB0C66A-F603-A402-A31C-28F2DA091AB6.html?lang=en-gb&login=1) is availble on the UCL library reading list page for the course.

### Video: Overview {#overview_w05}
```{r 05-short-lecture-welcome, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
embed_msstream('9ba75497-60e1-40c2-99c4-ac6057acbd27') %>% use_align('left')
```
[Lecture slides] [[Watch on MS stream]](https://web.microsoftstream.com/video/9ba75497-60e1-40c2-99c4-ac6057acbd27?list=studio)

## Tidy data
Over the past weeks a lot of information has come your way, diving deep into the world of Spatial Data Science. However, whilst you are slowly becoming proficient in using R and Python to solve complex (spatial) problems, it is now a good moment to start thinking about data themselves and how they are organised. This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be **dirty data**. With dirty data we mean data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Exploratory Spatial Data Analysis very much starts with your data preparation. 

In the following, you will learn a consistent way to structure your data in R: **tidy data**. Tidy data, as formalised by [R Wizard Hadley Wickham](http://hadley.nz/) in his contribution to the [Journal of Statistical Software](http://www.jstatsoft.org/v59/i10/paper) is not only very much at the core of the `tidyverse` R package, but also of general importance when organising your data. In the words, of [the Wizard](https://r4ds.had.co.nz/tidy-data.html):

> Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand.

### Video: Tidy data
```{r 05-short-lecture-tidy-data, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
embed_msstream('1a534d23-24e2-4f4a-aa38-f85a029ba19b') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

### What do tidy data look like?

You can represent the same underlying data in multiple ways. The example below, taken from the the `tidyverse` package and described in the [R for Data Science](https://r4ds.had.co.nz/tidy-data.html) book, shows that the same data can organised in four different ways. 

```{r 05-loading-tidyverse, warnings=FALSE, message=FALSE}
# load the tidyverse 
library(tidyverse)
```

Table **1**:
```{r 05-tidyverse-table1, warnings=FALSE, message=FALSE}
table1
```

Table **2**:
```{r 05-tidyverse-table2, warnings=FALSE, message=FALSE}
table2
```

Table **3**:
```{r 05-tidyverse-table3, warnings=FALSE, message=FALSE}
table3
```

Table **4a**:
```{r 05-tidyverse-table4a, warnings=FALSE, message=FALSE}
table4a
```

Table **4b**:
```{r 05-tidyverse-table4b, warnings=FALSE, message=FALSE}
table4b
```

None of these representations are wrong per se, however, not are equally easy to use. Only Table **1** can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy:

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

```{r 05-figure-hadley-wickham, echo=FALSE, fig.align='center', fig.cap='A visual representation of tidy data by [Hadley Wickham](https://r4ds.had.co.nz/tidy-data.html).'}
knitr::include_graphics('images/week05/05_a_tidy_data.png')
```

Fortunately, there are some functions in the `tidyr` and `dplyr` packages, both part of the `tidyverse` that will help us cleaning and preparing our datasets to create a **tidy dataset**. The most important and useful functions are:

| Package   | Function          | Use to |
| :------   | :------           | :------ |
| dplyr	    | select            | select columns |
| dplyr	    | filter            | select rows |
| dplyr	    | mutate            | transform or recode variables |
| dplyr	    | summarise         | summarise data |
| dplyr	    | group by          | group data into subgropus for further processing |
| tidyr	    | pivot_longer      | convert data from wide format to long format |
| tidyr	    | pivot_wider       | convert long format dataset to wide format |

Keep in mind that if you encounter a function in a piece of R code that you have not seen before and you are wondering what it does that you can get access the documentation through `?name_of_function`, e.g. `?pivot_longer`. For any mature R package, the documentation contains a list of arguments that the function takes as well as a set of usage examples.

### Example: Creating tidy data
Now we know what consitute **tidy data**, we can put this into practice with an example using some data from the [Office for National Statistics](https://www.ons.gov.uk/). Let's say we are asked by our boss to analyse some data on the ethnic background of the UK population, for instance, because we want to get some insights into the relationship between [COVID-19 and ethnic background](https://www.theguardian.com/world/2020/oct/09/bame-groups-hit-hard-again-covid-second-wave-grips-uk-nations). Our assignment is to calculate the relative proportions of each ethnic group within the administrative geography of the [Middle layer Super Output Area (MSOA)](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/middlesuperoutputareamidyearpopulationestimates). In order to do this, we have been given a file that contains data on ethnicity by age group at the MSOA-level of every person in the 2011 UK Census who is 16 year or older. Download the file to your own computer and [set up your data directory](spatial-autocorrelation.html#setting-up-the-environment) in the same fashion as you did last week. You can also decide on setting up your own folder structure, no problem, just make sure that you update the file paths in the practical to match the file paths on your own computers. Also, make sure that you unzip the data.

#### File download {-}
| File                                                 | Type           | Link |
| :------                                              | :------        | :------ |
| Etnicity by age group 2011 Census of Population      | `csv`          | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/population/eth_pop_2011/msoa_eth2011_ew_16plus.zip) |

We start by making sure our `tidyverse` is loaded into R and using the `read_csv()` function to read our `csv` file.

```{r 05-reading-data, warnings=FALSE, message=FALSE}
# load the tidyverse 
library(tidyverse)

# read data into dataframe
df <- read_csv('raw/population/eth_pop_2011/msoa_eth2011_ew_16plus.csv')
  
# inspect the dataframe: number of columns
ncol(df)

# inspect the dataframe: number of rows
nrow(df)

# inspect the dataframe: sneak peak
print(df, n_extra=2)
```

It is clear that the data are not directly suitable to establish the proportion of each ethnic group within the population of each MSOA because the data are split out over multiple columns. Let's inspect the names of the columns to get a better idea of the structure of our data set.

```{r 05-setting-max-print, warnings=FALSE, message=FALSE, echo=FALSE}
# settings
options(max.print = 30)
```

```{r 05-inspecting-column-names, warnings=FALSE, message=FALSE}
# inspect the dataframe: column names
names(df)
```

The column names are all awfully long and it looks like that the data have been split out into age groups. Further to this, the data contain within group total counts: *all categories*, *white total*, *mixed/multiple ethnic group total*, and so on. You can also try using `View(df)` or use any other form of spreadsheet software (e.g. Microsoft Excel) to **browse** through the dataset to get a better idea of what is happening and get a better idea of the structure of the data.

Although the data is messy and we will need to reorganise our data set, it does look there is some form of structure present that we can exploit: the various columns with population counts for each ethnic group are repeated for each of the different age groups. This means that we can go through the data frame in steps of equal size to select the data we want: starting from column 2 (column 1 only contains the reference to the adminsitrative geography) we want to select all 24 columns of data for that particular age group. We can create a `for` loop that does exactly that:

```{r 05-creating-loop, warnings=FALSE, message=FALSE}
# loop through the columns of our data set
for (column in seq(2,ncol(df),24)) {
    
  # index number of start column of age group
  start <- column
  
  # index number of end column of age group
  stop <- column + 23 
  
  # print results
  print(c(start,stop))
}
```
For each age group in our data, the printed values should **(!)** correspond with the index number of the start column of the age group and the index number of the end column of the age group, respectively. Let's do a sanity check.

```{r 05-sanity-check-columns, warnings=FALSE, message=FALSE}
# sanity check: age group 16-17 (start column)
df[,2]

# sanity check: age group 16-17 (end column)
df[,25]

# sanity check: age group 18-19 (start column)
df[,26]

# sanity check: age group 18-19 (end column)
df[,49]
```

All seems to be correct and we have successfully identified our columns. This is great, however, we still cannot work with our data as everything is spread out over different columns. Let's fix this by manipulating the shape of our data by turning columns into rows.

```{r 05-columns-to-rows-function, warnings=FALSE, message=FALSE}
# create function
columns_to_rows <- function(df, start, stop) {
  
  # columns we are interested in
  col_sub <- c(1,start:stop)
  
  # subset the dataframe 
  df_sub <- select(df,col_sub)
  
  # pivot the columns in the dataframe, exclude the MSOA code column 
  df_piv <- pivot_longer(df_sub,-msoa11cd)
  
  # rename columns
  names(df_piv) <- c('msoa11cd','age_group','count')
  return(df_piv)
}

# test
columns_to_rows(df,2,25)
```

This looks much better. Now let's combine our loop with our newly created function to apply this to all of our data.

```{r 05-reformat-dataframe, warnings=FALSE, message=FALSE}
# create an empty list to store our result from the loop
df_lst <- list()

# loop through the columns of our data set
for (column in seq(2,ncol(df),24)) {
    
  # index number of start column of age group
  start <- column
  
  # index number of end column of age group
  stop <- column + 23 
  
  # call our function and assign it to the list
  df_lst[[length(df_lst)+1]] <- columns_to_rows(df,start=start,stop=stop)
}

# paste all elements from the list underneath one another
# do.call executes the function rbind for all elements in our list
df_reformatted <- as_tibble(do.call(rbind,df_lst))

# and the result
df_reformatted
```

Now the data is in a much more manageable format, we can move on with preparing the data further. We will start by filtering out the columns (now rows!) that contain *all categories* and the *within group totals*. We will do this by cleverly filtering our data set on only a part of the text string that is contained in the *age_group* column of our dataframe using a [regular expression](https://en.wikipedia.org/wiki/Regular_expression). We further truncate the information that is contained in the *age_group* column to make all a little more readable.

```{r 05-further-clean-dataframe, warnings=FALSE, message=FALSE}
# filter rows
# this can be a little slow because of the regular expression!
df_reformatted <- filter(df_reformatted,!grepl('*All categories*',age_group))
df_reformatted <- filter(df_reformatted,!grepl('*Total*',age_group))

# create variable that flags the 85 and over category
# this can be a little slow because of the regular expression!
df_reformatted$g <- ifelse(grepl('85',as.character(df_reformatted$age_group)),1,0)

# select information from character 41 (85 and over category) or from character 38
df_reformatted <- mutate(df_reformatted,group = ifelse(g==0,substr(as.character(age_group),38,500),
                                                            substr(as.character(age_group),41,500))) 

# remove unnecessary columns
df_reformatted <- select(df_reformatted, -age_group, -g)
```

We are now really getting somewhere, although in order for our data to be tidy each variable must have its own column. We also want, within each ethnic group, to have the sum of the values that used to belong to individual age groups.

```{r 05-pivot-table-aggregate, warnings=FALSE, message=FALSE}
# pivot table and aggregate values
df_clean <- pivot_wider(df_reformatted,names_from=group,values_from=count,values_fn=sum)

# rename columns
names(df_clean) <- c('msoa11cd','white_british','white_irish','white_traveller','white_other','mixed_white_black_caribbean',
                     'mixed_white_black_african','mixed_white_asian','mixed_other','asian_indian','asian_pakistani',
                     'asian_bangladeshi','asian_chinese','asian_other','black_african','black_caribbean','black_other',
                     'other_arab','other_other')

# tidy date
df_clean
```

Finally. We now have a **tidy dataset** that we can work with!

### Assignments {#assignment_w05}

#### Assignment 1 {-}
Since we went through all the trouble of cleaning and creating this file, the first task for Friday's seminar is to finalise the analysis: use the cleaned data set to create a table that, for each MSOA, contains the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population. It could look something like this:

| msoa11cd          | white_british          | white_irish       | etc.        |
| :------           | :------:               | :------:          | :------:    |
| E02002562         | 0.74                   | 0.03              | ...         |
| E02002560         | 0.32                   | 0.32              | ...         |

#### Tips {-} 
1. First think what you what steps you would need to take in order to get the group proportions. Write them down on a piece of paper if you like. Once you have identified the steps, then start coding.
2. Conduct sanity checks. Every time you have written a piece of code, check the results to see if the code did indeed give the result that you expected to get.
3. Google is your friend. Do not be afraid to search for specific solutions and suggestions, chances are that there have been other people who have faces similar problems and posted their questions on [stackoverflow](https://stackoverflow.com/).

#### Assignment 2 {-}
Further to calculating the proportions of the population belonging to each of the ethnic groups in the UK 2011 Census of Population, we also want to make a choropleth map at district level of the UK population that is older than 60 as a proportion of the total population. For this analysis we have available one dataset with the administrative boundaries of the UK 2020 Local Authorithy Districts administrative geography and we have a `csv` file that holds population estimates for the UK in 2019. Use everything you have learned over the past weeks to produces this map. Some tips:

#### Tips {-}
1. Inspect both the shapefile and the `csv` file to get an idea of how your data look like. Use any tool you like to do this inspection (ArcGIS, R, QGIS, Microsoft Excel, etc.).
2. The `csv` file does contain a mix of administrative geographies, and you will need to do some data cleaning by filtering out *Country*, *County*, *Metropolitan County*, and *Region* before you link the data to the shapefile.
3. You are in charge of deciding which tools you want to use to visualise the data (ArcGIS, R, QGIS, etc.).
4. You now have to make your own decisions on how to go about this problem. Although this practical has so far covered some of the functions and strategies you might need, the data cleaning and data preparation process is not the same.

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Local Authorithy District boundaries 2020   | `shp`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/administrative_boundaries/uk_lad_2020/LADs_uk_2020.zip) |
| Mid-Year Population Estimates 2019          | `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/population/mye_pop_2019/mye_pop_2019.zip) |

## Exploratory spatial data analysis in R
Exploratory Data Analysis got introduced in the the late 1970s by the American mathematician [John Tukey](https://en.wikipedia.org/wiki/John_Tukey). Tukey thought that in much data analysis there was much emphasis on statistical hypothesis testing and very little development of new hypotheses. He therefore made a distinction between **confirmatory data analysis** and **exploratory data analysis (EDA)**. EDA is a collection of descriptive techniques used to detect patterns, identify outliers, and form hypotheses from the data. An EDA typically involves descriptive statistics and data visualisation. Exploratory techniques generally stay 'close' to the original data. **Exploratory spatial data analysis (ESDA)** is the extension of exploratory data analysis (EDA) to the spatial realm.

### Video: Exploratory spatial data analysis
```{r short-lecture-esda, echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Exploring our dataset
After successfully creating the maps that we were requested to make, our boss got so happy that she directly gave us a new assignment. This time she wants us to look at crime data in London again, specifically she wants to know whether relative deprivation and factors from the built environment can explain the occurrence of anti-social behaviour in London in March 2019. We have access to two **open** data sets. The first one contains the English [Index of Multiple Deprivation (IMD)](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019) for 2019. The English indices of deprivation measure relative deprivation in small areas in England called Lower-layer Super Output Areas (LSOAs) The index of multiple deprivation is the most widely used of these indices. The second one contains some of the input data that have been used to create the [Access to Healthy Assets and Hazards (AHAH)](https://data.cdrc.ac.uk/dataset/access-healthy-assets-hazards-ahah). AHAH is a multi-dimensional index developed by the [Consumer Data Research Centre (CDRC)](https://www.cdrc.ac.uk/) for Great Britain measuring how ‘healthy’ neighbourhoods are. We also have access to the LSOAs for London.

#### File download {-}
| File                                        | Type         | Link |
| :------                                     | :------      | :------ |
| Lower-layer Super Output Areas London 2011  | `shp`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/administrative_boundaries/london_lsoa_2011/lsoa_london_2011.zip) |
| Anti-social behaviour crime data London 2019| `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/crime/march2019_london_anti_social.zip) |
| Index of Multiple Deprivation London 2019   | `csv`        | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/index/imd_ew_2019.zip) |
| Access to Healthy Assets and Hazards (AHAH) London | `csv` | [Download](https://github.com/jtvandijk/GEOG0114/tree/master/raw/index/ahah_input_gb_2016.zip) |

Download and the file to your own computer and again make sure your [your data directory](spatial-autocorrelation.html#setting-up-the-environment) is set up correctly and the data are unzipped. Then load the libraries and the data into R.

```{r 05-load-libraries, warnings=FALSE, message=FALSE}
# load libraries
library(tidyverse)
library(sf)
library(tmap)

# load spatial data
lsoa <- st_read('raw/administrative_boundaries/london_lsoa_2011/lsoa_london_2011.shp')
names(lsoa) <- c('lsoa11cd','geometry')

# load data
crime <- read_csv('raw/crime/march2019_london_anti_social.csv')
imd <- read_csv('raw/index/imd_ew_2019.csv')
ahah <- read_csv('raw/index/ahah_input_gb_2016.csv')

# inspect crime data
head(crime)

# inspect imd data
head(imd)

# inspect ahah data
head(ahah)

```

Looking at the data you will notice that the anti-social behaviour crime data exists of longitudes and latitudes. The IMD dataset has an *lsoa11cd* column, an IMD rank column, and an IMD decile column. The AHAH dataset has an *lsoa11cd* column, a distance to the nearest gamling outlet column, a distance to the nearest pub column, and a distance to the nearest off license column. The [technical report of the AHAH index](https://figshare.com/articles/Access_to_Healthy_Assets_and_Hazards_AHAH_-_Updated_version_2017/8295842/1) suggests that access is calculated as the mean distance (kilometres) by car travel route of postcodes within a LSOA to the nearest outlet. Good to know!

Let's start by linking the IMD dataset and the AHAH dataset to the spatial dataset. Since we are using `sf` for our spatial data, we can very easily joining them together using the common variable in these datasets: *lsoa11cd*.

```{r 05-join-data-to-shape, warnings=FALSE, message=FALSE}
# join imd data
lsoa_join <- left_join(lsoa,imd,by=c('lsoa11cd'))

# join ahah data
lsoa_join <- left_join(lsoa_join,ahah,by=c('lsoa11cd'))

# inspect the result
head(lsoa_join)
```

This is all looking fine. Let's now turn our turn our anti-social behaviour crime location dataset into a spatial dataset using the longitude and latitude values and get total anti-social behaviour crime counts within each of London's LSOAs.

```{r 05-points-to-lsoa, warnings=FALSE, message=FALSE}
# filter out points without a longitude and/or latitude value
crime <- filter(crime,!is.na(long) | !is.na(lat))
crime_points <- st_as_sf(crime, coords=c('long','lat'),crs=4326)

# project into british national grid (epsg 27700)
crime_points <- st_transform(crime_points,27700)

# count the number of points intersecting within each of London's LSOAs
lsoa_join$n_crime <- lengths(st_intersects(lsoa_join,crime_points))

# inspect the result
tm_shape(lsoa_join) +
  tm_fill(col='n_crime')

# cube transform the anti-social behaviour crime data 
# effect is more pronounced than squareroot transformation
lsoa_join$n_crime_cube <- abs(lsoa_join$n_crime)^(1/3)

# inspect the result
tm_shape(lsoa_join) +
  tm_fill(col='n_crime_cube')
```

This all looks good. We can see, however, that the the number of times anti-social behaviour was reported are highly skewed by Central London. It is now a good idea to explore the anti-social behaviour crime data a bit further to see how the data are distributed non-spatially by plotting them.

```{r 05-inspect-the-crime-data, warnings=FALSE, message=FALSE}
# summary
summary(lsoa_join$n_crime_cube)

# histogram
ggplot(lsoa_join, aes(x=n_crime_cube)) +
    geom_histogram(binwidth=.5, colour='black', fill='white')

# boxplot
ggplot(lsoa_join, aes(x=n_crime_cube))+
 geom_boxplot()
```

There are clearly several outliers in our data set with some areas that have a higher number of cases of anti-social behaviour in March 2019. This raises the questions: where are these differences coming from? Let's also inspect our input data before we start to see whether there is a relationship between our input data and the number of cases of anti-social behaviour. For the IMD dataset we will use the IMD deciles.

```{r 05-inspect-the-input-data, warnings=FALSE, message=FALSE}
# summary imd
summary(lsoa_join[,4])

# boxplot imd
ggplot(lsoa_join, aes(x=imd_dec))+
 geom_boxplot()

# summary ahah
summary(lsoa_join[,5:7])

# boplot ahah gambling outlet distance
ggplot(lsoa_join, aes(x=gamb_dist))+
 geom_boxplot()

# boplot ahah pub distance
ggplot(lsoa_join, aes(x=gamb_dist))+
 geom_boxplot()

# boplot ahah off license distance
ggplot(lsoa_join, aes(x=off_dist))+
 geom_boxplot()
```

The input data look good. There are no real strange and unexpected outliers or missing values and at the same time there is enough [heterogeneity](https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity) in our data. This is important because if the input data have very similar values for al LSOAs they would not be very useful in trying to explain the differences in anti-social behaviour. So, we can continue. Let's start by seeing if there is any relationship between our input (independent) variables and our output (dependent) variable by some plots.

```{r 05-lets-scatterplot, warnings=FALSE, message=FALSE}
# imd vs anti-social behaviour crime
ggplot(lsoa_join,aes(x=imd_dec, y=n_crime_cube)) +
  geom_point()

# gambling outlet distance vs anti-social behaviour crime
ggplot(lsoa_join,aes(x=gamb_dist, y=n_crime_cube)) +
  geom_point()

# pub distance vs anti-social behaviour crime
ggplot(lsoa_join,aes(x=pubs_dist, y=n_crime_cube)) +
  geom_point()

# off license distance vs anti-social behaviour crime
ggplot(lsoa_join,aes(x=off_dist, y=n_crime_cube)) +
  geom_point()
```

The IMD deciles, which are in fact categorical data, do not show any obvious pattern, however, the distance to the various outlets do show some pattern. Let's correlate.

```{r 05-lets-correlate, warnings=FALSE, message=FALSE}
# imd vs anti-social behaviour crime
cor(lsoa_join$imd_dec,lsoa_join$n_crime_cube)

# gambling outlet distance vs anti-social behaviour crime
cor(lsoa_join$gamb_dist,lsoa_join$n_crime_cube)

# pub distance vs anti-social behaviour crime
cor(lsoa_join$pubs_dist,lsoa_join$n_crime_cube)

# off license distance vs anti-social behaviour crime
cor(lsoa_join$off_dist,lsoa_join$n_crime_cube)
```
So, looking at these graphs there seems to be some relationship between our explanatory (independent) variables and our dependent variables. It is time to form a hypothesis: LSOAs that are in a more deprived IMD decile and LSOAs with a lower mean distance to one of our outlets of interest experience more anti-social behaviour. The null hypothesis then becomes: LSOAs that are in a more deprived IMD decile and LSOAs with a lower mean distance to one of our outlets of interest **do not** experience more anti-social behaviour, i.e. there is no relationship whatsoever.

Let's see if we can test this hypothesis using an Ordinary Least Squares (OLS) regression model. In R, models are typically fitted by calling a model-fitting function, in our case `lm()` (linear model). The `lm()` function returns a fitted model which we can inspect by calling `summary` on the object. 

```{r 05-we-are-regressing, warnings=FALSE, message=FALSE}
# linear model
crime_model <- lm(n_crime_cube ~
                  imd_dec+gamb_dist+pubs_dist+off_dist,
                  data=lsoa_join)

# get the results
summary(crime_model)
```

In running a regression model, we are effectively trying to test our null hypothesis. If our null hypothesis was true, then we would expect our coefficients to equal 0. Right now our null hypothesis was that there is no relationship between anti-social behaviour and our input variables. Without going in much further detail, as this will be in-depth covered in your [CASA005 module](https://andrewmaclachlan.github.io/CASA0005repo/gwr-and-spatially-lagged-regression.html), we can see that the null hypothesis is disproven and, in fact, there seems to be a negative relationship between our input variables and our dependent variable. Three out of our four variables are significant at a 0.05 level of significance. The R-squared, which is a measure of the goodness-of-fit of the model, suggest that **13.17** per cent of the variation in our dependent variable can be explained by our independent variables. This means that a relationship is present, but it is not very strong! Nonetheless, the model suggest that LSOAs that are in a more deprived IMD decile and LSOAs with a lower mean distance to one of our outlets of interest experience more anti-social behaviour. Consider that when moving house.

#### Important {-}
Normally, you would also check the underlying assumptions of the linear model (linearity, homoscedasticity, independence, and normality), however, this will be covered more in-depth in your [CASA005 module](https://andrewmaclachlan.github.io/CASA0005repo/gwr-and-spatially-lagged-regression.html) and for our current purposes we are going to be a little naughty and accept the results as is and assume that all our underlying assumptions are satisfied (even though we are actually violating some of these underlying assumptions!). 

If you do want to get into some more detail on regression analysis right here and now, the following video explains the process of fitting a line to your data very clearly. While you at it, do have a look at some of the other videos on the [StatsQuestwith Josh Stammer](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) Youtube Channel.

```{r 05-statquest-ols,echo=FALSE}
library(vembedr)
embed_youtube('nk2CQITm_eo',frameborder=1) %>% use_align('left)
```

## Spatial heterogeneity
Although we have established a relationship between IMD deciles, access to gambling outlets, access to pubs, access to off licenses, and the occurrence of anti-social behaviour, our data are very spatial.However, we did not incorporate this spatial distribution into our model. So if we want to develop a regression model for anti-social behaviour in London we may have to recognise this spatial component. On top of this, a regression model assumes indepence of observations: what happens in LSOA001 is not related to what happens in LSOA002 or any other LSOA. However, we know from last weeks content that this is not always the case because of spatial autocorrelation.

### Video: Spatial heterogeneity
```{r 05-short-lecture-spatial-heterogeneity,echo=FALSE}
library(vembedr)
# embed_msstream("52d0ac98-563e-46ac-b86b-efe5524d37bf") %>% use_align("center")
```
[[Lecture slides]]() [[Watch on MS stream]]()

### Example: Accounting for spatial heterogeneity
Let's start by exploring wether autocorrelation is an issue in our current analysis. Make sure the following libraries are (still) loaded:

```{r 05-load-libraries-again, warnings=FALSE, message=FALSE}
# load libraries
library(sf)
library(tmap)
library(sp)
library(spdep)
```

Now we are ready to go again, we are going to start by having a closer look at the residuals of our model. In other words, the residuals represent the distance between the observed values of the dependent variable and the predicted values of our dependent variables. Residuals should be randomly distributed over space, otherwise we may be dealing with spatial autocorrelation. Strange words coming from a geographe, but right now we do **not*** want to see any spatial patterning.

```{r 05-get-those-residuals, warnings=FALSE, message=FALSE}
# extract the residuals from the model and assign to our spatial dataset
lsoa_join$residuals <- residuals(crime_model)

# extract the predicted values from the model and assign to our spatial dataset
lsoa_join$predicted <- fitted(crime_model)

# example observed, residual, predicted of first LSOA in our data
lsoa_join[1,c(1,10)]
```

Now we have our residuals assigned to their spatial units, we can plot the residuals in number of standard deviations from the mean.

```{r 05-standardise-and-plot-yeah, warnings=FALSE, message=FALSE}
# standardise
lsoa_join$sd_mean <- (lsoa_join$predicted - mean(lsoa_join$predicted)) / sd(lsoa_join$predicted)

# inspect the result
breaks <- c(-14,-3,-2,-1,1,2,3,14)
tm_shape(lsoa_join) + 
  tm_fill('sd_mean', style='fixed', breaks=breaks, palette='-RdBu') +
  tm_borders(alpha = 0.1) 
```

You can see the spatial patterning of areas of over-prediction in the centre of Greater London with areas of under-prediction concentrating on the edges of area. We could be dealing with some spatial autocorrelation, so we need to test for it again. We will be using a Global Moran's I for this. Remember with a global Moran’s I we can test how 'random' the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a 'random' distribution, and 1 is a 'non-random' distribution of clearly defined clusters.

```{r 05-moran-preparation, message=FALSE, warnings=FALSE}
# force sf to sp
lsoa_join_sp <- as_Spatial(lsoa_join, IDs=lsoa_join$lsoa11cd)

# create a list of neighbours using the 'queen' criteria
lsoa_nb <- poly2nb(lsoa_join_sp, row.names=lsoa_join_sp$lsoa11cd)

# check connection
summary(lsoa_nb)

# generate the row standardise spatial weight matrix and the Moran Scatterplot
wm <- nb2mat(lsoa_nb, style='W')
rwm <- nb2listw(lsoa_nb, style='W')
```

Now we can execute a Moran's test **for the regression residuals**. We use `lm.morantest()` for this. This is important because this function takes into account that the input data are residuals, something which the 'normal' Moran's I test does not do.

```{r 05-moran-execution, warnings=FALSE, message=FALSE}
# Moran's I
lm.morantest(crime_model, rwm, alternative='two.sided')
```




## Take home message
Exploratory (Spatial) Data Analysis is at the core of any analysis. After you cleaned and prepared your data, you first want to inspect your data through simple and visual methods to get acquainted with the data. The you can start looking for relationships, and, if any relationships are found you are going to want to ask the question to what extent these relationships are real or to what extent other (spatial) processes, such as spatial autocorrelation, play a role in shaping these relationships. Next week we will move to point pattern analysis and go beyond simple point-to-polygon aggregation, but that is next week so for now we can [celebrate the successful completion](https://www.youtube.com/watch?v=d8Fmu3RLEOY) of this week's material.

## Attributions {#attributions_w05}
This week's content and practical uses content and inspiration from:

- Wickham, H., Grolemund, G. 2017. R for Data Science. https://r4ds.had.co.nz/index.html 
- Medina, J, Solymosi, R. 2019. Crime Mapping in R. https://maczokni.github.io/crimemapping_textbook_bookdown/

## Feedback {#feedback_w05}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).

<!--chapter:end:05-week05.Rmd-->

# Point pattern analysis

In our previous practicals, we have aggregated our event data into areal units, primarily using administrative geographies, to enable its easy comparison with other datasets provided at the same spatial scale, such as the census data used in the previous week, as well as to conduct spatial autocorrelation tests. However, when locations are precisely known, spatial point data can be used with a variety of spatial analytic techniques that go beyond the methods typically applied to areal data. The set of methods unique to point data are often referred to as point pattern analysis and geostatistics. 

This week, we focus on point pattern analysis, whilst next week we will look at geostatistics. Within point pattern analysis, we look to detect patterns across a set of locations, including measuring density, dispersion and homogeneity in our point structures. We will look first at basic forms of point pattern analysis, including mean centers and the standard deviational ellipse, and then more powerful analysis methods, including kernel density estimation and Ripley’s K function. These latter functions help determine and/or show whether points have a random, dispersed or clustered distribution pattern at a certain scale. 

To put these measures and methods into action, our practical component looks to test the hypothesis that "the majority of bicycle theft in London occurs with walking distance of a train or tube station", by assessing clusters and hotspots of bike theft in relation to a transportation locations.

<center>**This week's content will be made available on 16/10.**</center>

## Feedback {#feedback_w06}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).

<!--chapter:end:06-week06.Rmd-->

# Geostatistics

Following on from Week 3 and in conjunction with this week in your CASA005 module, we will also be focusing on rasters, exploring further the creation and application of rasters from vector datasets using geostatistics. During the lecture, we will provide a more in-depth and detailed explanation behind the geostatistics methods of interpolation, looking at various deterministic and geostatistical techniques. We then introduce methods to using vector datasets with rasters, utilizing a GIS method known as zonal statistics. 

Following on from Week 3’s practical, we look to study further the role of greenspace quality in relation to schools and how this varies across different geographies. This week, we look at greenspace quality from a personal health perspective, analyzing air quality across our greenspaces. By analyzing the air quality of greenspace, we can provide a more detailed understanding to how "accessible" the greenspaces are to the surrounding schools.

<center>**This week's content will be made available on 23/11.**</center>

## Feedback {#feedback_w07}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).

<!--chapter:end:07-week07.Rmd-->

# Cluster analysis

DBScan is a density-based clustering algorithm that is commonly used in data mining and machine learning. Across a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. Whilst the algorithm is nearly 25 years old, it still is incredibly relevant to many applications within data science today, including within spatial analysis. Novel use of the algorithm has seen researchers detect and delineate urban areas from building footprint data. This use presents significant opportunities for those working to provide spatial data within countries where until recently, significant reference data did not exist. 

To understand this potential application, we look at the issues of data scarcity and sparsity within LICs and MICs, which do not currently have extensive mapping data or national mapping agencies to provide this data. We look at current initiatives to address this data dearth, with a specific focus on those working on the automated extraction of building footprints from satellite imagery using machine learning. 

Within the practical component, we look to deploy a simple version of DBScan on building data subset from the Tanzanian building dataset created by Microsoft. We attempt to detect and delineate our own urban areas within the dataset and discuss the advantages of novel changes made by those working with the algorithm for this specific purpose.

<center>**This week's content will be made available on 30/10.**</center>

## Feedback {#feedback_w08}
Please take a moment to give us some [feedback on this week's content](https://forms.gle/BYbZySVSHeoUEkJh9).

<!--chapter:end:08-week08.Rmd-->

# Bayesian modelling 

Bayesian methodology is an approach to statistical inference that has existed for a long time. However, its applications have been limited until recent advancements in computation and simulation methods has made its deployment and use within big datasets viable. The concept underlying Bayesian spatial modeling is Bayes’ theorem, which considers both the distributions of the data and the unknown coefficient estimates (LeSage and Pace, 2009). Bayesian spatial modeling embraces most, if not all, spatial models in the literature, such as the spatial lag model, the spatial error model, and geographically weighted regression discussed in Week 5. As a result, recent decades have experienced a rapid growth in the application of Bayesian spatial modeling to epidemiology, demography, and environmental health research, in addition to other geography-related disciplines. Bayesian modelling has been used to estimate population distributions, disease spread and air pollution.

This week provides you with a general introduction to Bayesian modelling for geographical applications, with a focus on understanding the overall methodology as well as selection of variables for use within the model. 

The practical component looks at R-INLA package and how it can be used within health modelling. 

<center>**This week's content will be made [available on Moodle](https://moodle.ucl.ac.uk/) on 09/12.**</center>

<!--chapter:end:09-week09.Rmd-->

# Reproducible research

In our final week of PSA, we will recap the main principles of spatial analysis that you have learnt over the last nine weeks. We will then look to tidy one of our projects from the previous week to use it within CodeOcean, an online platform that hosts code and data to create "reproducible runs" and "capsules" of research projects. The platform (alongside others!) enables you to create an anonymized version of your project, ready for its submission to journals as part of their increasing requirement to pass reproducibility and openness tests, e.g. the International Journal of Geographic Information Science. The live session of this week will be extended to present the two pieces of coursework you will assessed on as part of this module, as well as to answer any questions you may have on these assessments.

<center>**This week's content will be made [available on Moodle](https://moodle.ucl.ac.uk/) on 09/12.**</center>

<!--chapter:end:10-week10.Rmd-->

